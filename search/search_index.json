{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is university-wide initiative that is to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to get access or become owners, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \nAmarel user guide\n as it has slurm tips and examples. \n\n\n\n\nFor users familiar with Linux but new to \nslurm\n, follow \nthis path\n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Home"
        },
        {
            "location": "/howto/",
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u <your net id>\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:",
            "title": "HowTo"
        },
        {
            "location": "/howto/#tunneling",
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:",
            "title": "Tunneling"
        },
        {
            "location": "/howto/#how-to-launch-jupyter-notebook-on-the-cluster",
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u <your net id>  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:",
            "title": "How to launch Jupyter notebook on the cluster"
        },
        {
            "location": "/workshop/",
            "text": "Genomic Software\n\n\nThis is a list of software to install for the workshop\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nSeqtk\n\n\nvery handy and fast for processing fastq/a files\n\n\nlink\n\n\n\n\n\n\nsratoolkit\n\n\ndownloading and processing data from GEO/SRA database\n\n\nlink\n\n\n\n\n\n\nhtseq-count\n\n\ncounting the  reads mapped on to genomics feature\n\n\nlink\n\n\n\n\n\n\nfastQC\n\n\nwidely used for sequencing read QC\n\n\nlink\n\n\n\n\n\n\nRSeQC-2.6.4\n\n\nAn RNA-seq quality control package, multiple functions\n\n\npython package\n\n\n\n\n\n\ntrimmomatic\n\n\nfastq quality trim and adaptor removal\n\n\nlink\n\n\n\n\n\n\n\n\nThis is a list of software already available on the cluster and the command you need to execute to load it in your environment: \n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nload it on the cluster\n\n\n\n\n\n\n\n\n\n\nSamtools\n\n\n\n\nmodule load samtools\n\n\n\n\n\n\nBedtools\n\n\n\n\nmodule load bedtools2./2.25.0\n\n\n\n\n\n\nbowtie2\n\n\nalignment software\n\n\nmodule load bowtie2\n\n\n\n\n\n\ntophat2\n\n\nalignment software\n\n\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n\n\n\n\n\n\nR\n\n\nlanguage for statistical analysis\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\n\n\n\n\nThis is a list of other software you might find useful\n               | GSEA     | genome set enrichment analysis| [link] (http://software.broadinstitute.org/gsea/index.jsp) |\n               | IGV      | Interactive Genome Viewer      | [link] (http://software.broadinstitute.org/software/igv/)|\n               | Cytoscape| Network visualization softwar   | [link] (http://www.cytoscape.org/)|\n\n\nSetup\n\n\n\n\nConnect to the cluster login node by one of the following methods: \n\n\nvia a terminal\n: if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as \nputty\n or \nmoba-xterm\n. Then from your terminal connect to the cluster by executing the following command: \n\nssh -X <your net id>@amarel.hpc.rutgers.edu\n\n   This is the preferred way, as your copy-pasting will most likely work best. \n\n\n\n\nvia FastX\n: in your browser, go to \nhttps://amarel.hpc.rutgers.edu:3443\n\n\n\n\n\n\nGet resources on the compute node of the cluster to execute your computations by running the following command in your terminal: \n\nsrun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash\n\nNotice that the name in your terminal will change from \namarel\n to node name like \nhal0025\n or \nslepner086\n. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. This explains the parts of this command: \n\n\n\n\n\n\n\n\n\n\n\n\ncommand part\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\nsrun\n\n\nslurm\n run, i.e. allocate resources and run via \nslurm\n scheduler\n\n\n\n\n\n\n-p main\n\n\non the main partition, one of several queues on the cluster\n\n\n\n\n\n\n--reservation=genomics\n\n\nwe reserved some compute nodes for this workshop to not wait long for resources\n\n\n\n\n\n\n-N 1\n\n\nask for one node\n\n\n\n\n\n\n-c 2\n\n\nask for two cores\n\n\n\n\n\n\n-n 1\n\n\nthis will be 1 most times\n\n\n\n\n\n\n-t 01:40:00\n\n\nrun this for a maximum time of 1 hour 40 minutes\n\n\n\n\n\n\n--pty /bin/bash\n\n\nrun the terminal shell in an interactive mode\n\n\n\n\n\n\n\n\n\n\nPrepare some directories for the data\n \n   You have two main spaces on the Amarel cluster. These are \n/home/netid/\n (e.g. \n/home/kp807/\n for my netid) and '/scratch/netid/'. They differ in how often they are backed up and by size (100Gb for \n/home\n and 500Gb for \n/scratch\n). So we will install programs in \n/home\n, while the data and output will be in \n/scratch\n. Execute these commands: \n\n\n\n\n                cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here\n\n\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\n\n\nInstall programs\n   Each program will have slightly different installation instructions. Here is a handy sequence of commands that will install them: \n\n\n\n\n##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                    \n\n\n\n\n\nMoreover, execute these commands to load system-installed sofware so the system knows where to find it (i.e. \nsamtools command will work\n): \n\n\nmodule load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\nDownload data",
            "title": "Workshops"
        },
        {
            "location": "/workshop/#genomic-software",
            "text": "This is a list of software to install for the workshop     software  description  link      Seqtk  very handy and fast for processing fastq/a files  link    sratoolkit  downloading and processing data from GEO/SRA database  link    htseq-count  counting the  reads mapped on to genomics feature  link    fastQC  widely used for sequencing read QC  link    RSeQC-2.6.4  An RNA-seq quality control package, multiple functions  python package    trimmomatic  fastq quality trim and adaptor removal  link     This is a list of software already available on the cluster and the command you need to execute to load it in your environment:      software  description  load it on the cluster      Samtools   module load samtools    Bedtools   module load bedtools2./2.25.0    bowtie2  alignment software  module load bowtie2    tophat2  alignment software  module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0    R  language for statistical analysis  module load intel/17.0.4 R-Project/3.4.1     This is a list of other software you might find useful\n               | GSEA     | genome set enrichment analysis| [link] (http://software.broadinstitute.org/gsea/index.jsp) |\n               | IGV      | Interactive Genome Viewer      | [link] (http://software.broadinstitute.org/software/igv/)|\n               | Cytoscape| Network visualization softwar   | [link] (http://www.cytoscape.org/)|",
            "title": "Genomic Software"
        },
        {
            "location": "/workshop/#setup",
            "text": "Connect to the cluster login node by one of the following methods:   via a terminal : if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as  putty  or  moba-xterm . Then from your terminal connect to the cluster by executing the following command:  ssh -X <your net id>@amarel.hpc.rutgers.edu \n   This is the preferred way, as your copy-pasting will most likely work best.    via FastX : in your browser, go to  https://amarel.hpc.rutgers.edu:3443    Get resources on the compute node of the cluster to execute your computations by running the following command in your terminal:  srun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash \nNotice that the name in your terminal will change from  amarel  to node name like  hal0025  or  slepner086 . This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. This explains the parts of this command:        command part  meaning      srun  slurm  run, i.e. allocate resources and run via  slurm  scheduler    -p main  on the main partition, one of several queues on the cluster    --reservation=genomics  we reserved some compute nodes for this workshop to not wait long for resources    -N 1  ask for one node    -c 2  ask for two cores    -n 1  this will be 1 most times    -t 01:40:00  run this for a maximum time of 1 hour 40 minutes    --pty /bin/bash  run the terminal shell in an interactive mode      Prepare some directories for the data  \n   You have two main spaces on the Amarel cluster. These are  /home/netid/  (e.g.  /home/kp807/  for my netid) and '/scratch/netid/'. They differ in how often they are backed up and by size (100Gb for  /home  and 500Gb for  /scratch ). So we will install programs in  /home , while the data and output will be in  /scratch . Execute these commands:                    cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated   Install programs\n   Each program will have slightly different installation instructions. Here is a handy sequence of commands that will install them:    ##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                      Moreover, execute these commands to load system-installed sofware so the system knows where to find it (i.e.  samtools command will work ):   module load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1",
            "title": "Setup"
        },
        {
            "location": "/workshop/#download-data",
            "text": "",
            "title": "Download data"
        },
        {
            "location": "/resources/",
            "text": "Here are some resources you might want to consult to learn more about how to use an HPC cluster\n\n\nSlurm\n\n\n\n\nOARC cluster user guide Amarel/Perceval\n - this is a must-read for any new users, even if you are an experienced Linux user \n\n\nOARC cluster community\n - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it\n\n\nintro videos by Kristina\n\n\nterse slurm tips\n\n\n\n\nLinux tutorials\n\n\n\n\nlinux tutorial by Galen\n\n\nto be continued\n\n\n\n\nGraphical user interface\n\n\n\n\nweb-based access to the cluster (still testing) - only from campus or VPN\n\n\n\n\nResources on the web\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources"
        },
        {
            "location": "/resources/#slurm",
            "text": "OARC cluster user guide Amarel/Perceval  - this is a must-read for any new users, even if you are an experienced Linux user   OARC cluster community  - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it  intro videos by Kristina  terse slurm tips",
            "title": "Slurm"
        },
        {
            "location": "/resources/#linux-tutorials",
            "text": "linux tutorial by Galen  to be continued",
            "title": "Linux tutorials"
        },
        {
            "location": "/resources/#graphical-user-interface",
            "text": "web-based access to the cluster (still testing) - only from campus or VPN",
            "title": "Graphical user interface"
        },
        {
            "location": "/resources/#resources-on-the-web",
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources on the web"
        }
    ]
}