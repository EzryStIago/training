{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is a university-wide initiative that aims to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to get access or become owners, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \nAmarel user guide\n as it has slurm tips and examples. \n\n\nWARNING - READ!\n\n\n\n\nDo not run large computational jobs on the \nlogin\n node. Use slurm to allocate resources on the compute node. \n\n\nTODO: add more complete list\n\n\n\n\nLearning paths\n\n\n\n\nFor users familiar with Linux but new to \nslurm\n, follow \nthis path\n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Home"
        },
        {
            "location": "/#warning-read",
            "text": "Do not run large computational jobs on the  login  node. Use slurm to allocate resources on the compute node.   TODO: add more complete list",
            "title": "WARNING - READ!"
        },
        {
            "location": "/#learning-paths",
            "text": "For users familiar with Linux but new to  slurm , follow  this path  For users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Learning paths"
        },
        {
            "location": "/Amarel_User_Guide/",
            "text": "General Information\n\n\n\n\nAmarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource.\n\n\nAmarel includes the following hardware (this list may already be outdated since the cluster is actively growing):\n\n\n52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM\n20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM\n4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard\n2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM\n53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM\n5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM\n26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM\n4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM\n3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard\n2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard\n1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard\n\n\n\n\nDefault run time = 2 hours in the 'main' partition\nMaximum run time = 3 days in the 'main' partition\n\n\nConnecting to Amarel\n\n\n\n\nAmarel is currently accessed using a single hostname, amarel.rutgers.edu\n\n\nWhen you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc.\n\n\nSo, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using.\n\n\nssh [your NetID]@amarel.rutgers.edu\n\n\nIf you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See \nhere\n for details.\n\n\nMoving files\n\n\n\n\nThere are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory,\n\n\nscp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID]\n\n\nTo pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command),\n\n\nscp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt  .\n\n\nIf you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it:\n\n\ntar -czf my-directory.tar.gz my-directory\n\n\nAfter moving it, you can unpack that .tar.gz file to get your original directory and contents:\n\n\ntar -xzf my-directory.tar.gz\n\n\nA handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel:\n\n\nrsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir\n\n\nIn this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data)\n\n\nTo sync a local directory with updated data from Amarel:\n\n\nrsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir\n\n\nHere, we've simply reversed the order of the local and remote locations.\n\n\nFor added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case):\n\n\nrsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir\n\n\nListing available resources\n\n\n\n\nBefore requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources.\n\n\nExample of using the \nsinfo\n command:\n\n\nsinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*        up 3-00:00:00      4 drain* hal[0050-0051,0055,0093]\nmain*        up 3-00:00:00      5  down* slepner[084-088]\nmain*        up 3-00:00:00      4  drain hal[0023,0025-0027]\nmain*        up 3-00:00:00     86    mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082]\nmain*        up 3-00:00:00     84  alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083]\nmain*        up 3-00:00:00      2   down gpu[001-002]\ngpu          up 3-00:00:00      8    mix gpu[003,006],pascal[001-006]\ngpu          up 3-00:00:00      1  alloc gpu005\ngpu          up 3-00:00:00      2   down gpu[001-002]\nphi          up 3-00:00:00      1    mix gpu004\nmem          up 3-00:00:00      1    mix mem002\nmem          up 3-00:00:00      1  alloc mem001\n\n\n\n\nUnderstanding this output:\n\n\nThere are 4 basic partitions, main (traditional compute nodes, CPUs only), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors.\n\n\nThe upper limit for a job\u2019s run time is 3 days (72 hours).\n\n\n\n\n\n\n\n\nTerm\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAllocated (alloc)\n\n\nnodes are currently running jobs.\n\n\n\n\n\n\nMixed (mix)\n\n\nnodes have jobs using some, but not all, CPU cores onboard.\n\n\n\n\n\n\nIdle\n\n\nnodes are currently available for new jobs.\n\n\n\n\n\n\nDrained (drain, drng)\n\n\nnodes are not available for use and may be offline for maintenance.\n\n\n\n\n\n\nSlepner, Norse Mythology\n\n\n\"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores).\n\n\n\n\n\n\nHal\n\n\nHal is a dependable member of the Discovery One crew who does an excellent job of following instructions.\n\n\n\n\n\n\nPascal\n\n\nFrench mathematician and the name of one of NVIDIA's GPU architectures.\n\n\n\n\n\n\nCUDA\n\n\nThis is the name of a parallel computing platform and application programming interface (API) model created by Nvidia\n\n\n\n\n\n\n\n\nLoading software modules\n\n\n\n\nWhen you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.\n\n\nThe module avail command will show a list of the core (primary) modules available:\n\n\nmodule avail\n\n-------------------------------------------------- /opt/sw/modulefiles/Core --------------------------------------------------\n   ARACNE/20110228         blat/35                  gcc/4.9.3               intel_mkl/16.0.3 (D)    mvapich2/2.2        (D)\n   HISAT2/2.0.4            bowtie2/2.2.6            gcc/4.9.4               intel_mkl/17.0.0        openmpi/2.1.1\n   HISAT2/2.1.0     (D)    bowtie2/2.2.9     (D)    gcc/5.3                 intel_mkl/17.0.1        pgi/16.9\n   MATLAB/R2017a           bwa/0.7.12               gcc/5.4          (D)    intel_mkl/17.0.2        pgi/16.10           (D)\n   MATLAB/R2017b    (D)    bwa/0.7.13        (D)    hdf5/1.8.16             java/1.7.0_79           python/2.7.11\n   Mathematica/11.1        cuda/7.5                 intel/16.0.1            java/1.8.0_66           python/2.7.12\n   OpenCV/2.3.1            cuda/8.0                 intel/16.0.3     (D)    java/1.8.0_73           python/3.5.0\n   STAR/2.5.2a             cuda/9.0          (D)    intel/16.0.4            java/1.8.0_121          python/3.5.2        (D)\n   Trinotate/2.0.2         cudnn/7.0.3              intel/17.0.0            java/1.8.0_141          samtools/0.1.19\n   bamtools/2.4.0          cufflinks/2.2.1          intel/17.0.1            java/1.8.0_152   (D)    samtools/1.2\n   bcftools/1.2            delly/0.7.6              intel/17.0.2            modeller/9.16           samtools/1.3.1      (D)\n   bedtools2/2.25.0        gaussian/03revE01        intel/17.0.4            moe/2016.0802           trinityrnaseq/2.1.1\n   blast/2.6.0             gaussian/09revD01 (D)    intel_mkl/16.0.1        mvapich2/2.1\n\n\n\n\nUnderstanding this output:\n\n\nThe packages with a (D) are the default versions for packages where multiple versions are available.\n\n\nTo see a comprehensive list of all available modules (not just the core modules) use the \nmodule spider\n command.\n\n\nmodule spider\n\n---------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------------------------------------------------\n  ARACNE: ARACNE/20110228\n    ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context\n\n  HISAT2: HISAT2/2.0.4, HISAT2/2.1.0\n    HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes\n\n  HMMER: HMMER/3.1b2\n    HMMER: biosequence analysis using profile hidden Markov models\n\n  MATLAB: MATLAB/R2017a, MATLAB/R2017b\n    MATLAB: The Language of Technical Computing\n\n  Mathematica: Mathematica/11.1\n    Wolfram Mathematica: Modern Technical Computing\n\n  NAMD: NAMD/2.10\n    NAMD: Scalable Molecular Dynamics\n\n  ORCA: ORCA/3.0.3\n    ORCA: An ab initio, DFT and semiempirical SCF-MO package\n\n  OpenCV: OpenCV/2.3.1\n    OpenCV: Open Source Computer Vision\n\n\n\n\nLoading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use.\n\nTo load a software module, use the module load command, followed by the name and version desired.\n\nTo remove select modules, use the module unload command. To remove all loaded software modules, use the module purge command.\n\nTo load the default version of any software package, use the module load command but only specify the name of the package, not the version number.  \n\n\nBelow are some examples.\n\n\nmodule load intel/16.0.3 mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3   2) mvapich2/2.1\n\nmodule unload mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\nmodule purge\nmodule list\nNo modules loaded\n\nmodule load intel\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\n\n\n\nIf you always use the same software modules, your \n~/.bashrc\n (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your ~/.bashrc file to change or remove those commands later.\n\n\nPLEASE NOTE:\n Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.\n\n\nRunning a serial (single-core) job\n\n\n\n\nHere\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance.\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=2000                   # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1\n\nsrun /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in\n\n\n\n\nUnderstanding this job script:\n\n\nA job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job.\n\n\nThe lines beginning with #SBATCH contain commands intended only for the workload manager.\n\n\nMy job will be assigned to the \u201cmain\u201d partition (job queue).\n\n\nIf this job is preempted, it will be returned to the job queue and will start again when required resources are available\n\n\nThis job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.\n\n\nMy job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running.\n\n\nAny output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number.\n\n\nBe sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable.\n\n\nHere\u2019s how to run a serial batch job, loading modules and using the \nsbatch\n command:\n\n\nsbatch my-job-script.sh\n. \nThe \nsbatch\n command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.\n\n\nRunning a parallel (multicore MPI) job\n\n\n\n\nHere\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here.\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=16                  # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=124000                 # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1 mvapich2/2.1\n\nsrun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in\n\n\n\n\nUnderstanding this job script:\n\n\nThe srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request.\n\n\nThis job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.\n\n\nNote here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.\n\n\nHere\u2019s how to run a parallel batch job, loading modules and using the \nsbatch\n command:\n\n\nsbatch my-job-script.sh\n\n\nRunning an interactive job\n\n\nAn interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications.\n\n\nWhen submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job:\n\n\n[NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\nsrun: job 1365471 queued and waiting for resources\nsrun: job 1365471 has been allocated resources\n\n[NetID@slepner045 ~]$\n\n\n\n\nNotice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command.\n\n\nMonitoring the status of jobs\n\n\n\n\nThe simplest way to quickly check on the status of active jobs is by using the \nsqueue\n command:\n\n\nsqueue -u [your NetID]\n\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n1633383      main   zipper    xx345   R       1:15      1 slepner36\n\n\n\n\nHere, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS).\n\n\nFor summary accounting information (including jobs that have already completed), you can use the \nsacct\n command:\n\n\nsacct\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n1633383          zipper       main      statx         16    RUNNING      0:0\n\n\n\n\nHere, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED.\n\n\nFor complete and detailed job info, you can use the \nscontrol show job [JobID]\n command:\n\n\nscontrol show job 244348\n\nJobId=244348 JobName=XIoT22\n   UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A\n   Priority=5050 Nice=0 Account=oarc QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A\n   SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19\n   StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   Partition=main AllocNode:Sid=amarel1:22391\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=hal0053\n   BatchHost=hal0053\n   NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=28,mem=124000M,node=1\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0\n   Features=(null) Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/gc563/run.STMV.CPU.slurm\n   WorkDir=/scratch/gc563\n   StdErr=/scratch/gc563/slurm.%N.244348.out\n   StdIn=/dev/null\n   StdOut=/scratch/gc563/slurm.%N.244348.out\n   Power=\n\n\n\n\nKilling/ cancelling/ terminating jobs\n\n\n\n\nTo terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate:\n\nscancel 1633383\n\nA job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this:\n\n\nslurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***\n\n\nInstalling your own software\n\n\n\n\nPackage management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]).\n\n\nAs an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist:\n\n\n(1) Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the \nwget\n utility:\n\nwget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz\n\n\n(2) Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files:\n\n\ntar -zxf zipper-4.1.5.tar.gz\n\n\n3) Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software.\n\n\ncd zipper-4.1.5\n\n\nless README\n\n\n4) Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER:\n\nmodule load intel/16.0.3 mvapich2/2.1\n\n\n(5) Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the \n./configure -\u2013help\n command. What you see below is just a typical example of special options that might be specified.\n\n\n./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared\nmake -j 4\nmake install\n\n\n\n\nSeveral packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation:\n\n\n/home/[NetID]/zipper/4.1.5/bin\n where executables will be placed\n\n\n/home/[NetID]/zipper/4.1.5/lib\n where library files will be placed\n\n\n/home/[NetID]/zipper/4.1.5/include\n where header files will be placed\n\n\n/home/[NetID]/zipper/4.1.5/share/man\n where documentation will be placed\n\n\n(6) Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment:\n\n\nexport PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH\nexport C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH\nexport MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH\n\n\n\n\nThese \nexport\n commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.\n\n\nSingularity\n\n\n\n\nSingularity\n is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers.\n\n\nDocker\n is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.\n\n\nImportant differences between Docker and Singularity:\n\n\n\n\nDocker and Singularity have their own container formats.\n\n\nDocker containers can be imported and run using Singularity.\n\n\nDocker containers usually run as root, which means you cannot run Docker on a  shared computing system (cluster).\n\n\nSingularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.\n\n\n\n\nImporting a Docker image:\n\n\nIf you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container.\n\n\nHere's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel.\n\n\nIf you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories.\n\n\nStart your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel.\n\nsudo docker run -it ubuntu:latest bash  \nroot@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc\n\n\nExporting your Docker image\n\n\nFind the name of your Docker image using the 'docker ps' command,\n\n\nsudo docker ps\nCONTAINER ID  IMAGE          COMMAND  CREATED        STATUS        NAMES\n11a87dkw8748  ubuntu:latest  \"bash\"   2 minutes ago  Up 2 minutes  bendakaya_pakodi\n\n\n\n\nIn this example the name of the images is bendakaya_pakodi. Export this image to a tarball,\n\n\nsudo docker export bendakaya_pakodi > ubuntu.tar\n\n\nConverting to a Singularity image\n\n\nYou will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel.\n\n\nCreate an empty singularity image, and then import the exported docker image into it,\n\n\nsudo singularity create ubuntu.img\nCreating a sparse image with a maximum size of 1024MiB...\nUsing given image size of 1024\nFormatting image (/sbin/mkfs.ext3)\nDone. Image can be found at: ubuntu.img\n$ sudo singularity import ubuntu.img ubuntu.tar\n\n\n\n\nUsing Singularity containers inside a SLURM job\n\n\nTransfer\n your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel.\n\n\nYou can run any task/program inside the container by prefacing it with 'singularity exec [your image name]'\n\n\nHere is a simple example job script that executes commands inside a container,\n\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=sing2me           # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                   # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n\nmodule purge\nmodule load singularity/.2.5.1\n\n## Where am I running?\nsrun singularity exec ubuntu.img hostname\n\n## What is the current time and date?\nsrun singularity exec ubuntu.img date\n\n\n\n\nIf you created directories for any Amarel filesystems, you should find they are mounted inside your container,\n\n\nmount | grep gpfs\n/dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime)\n/dev/projects/oarc on /projects/oarc type gpfs (rw,relatime)\n\n\n\n\nNOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.\n\n\nTroubleshooting/ Common Problems\n\n\n\n\nFailure to load module dependencies/prerequisites:\n\n\n module load R-Project/3.4.1\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"R-Project/3.4.1\"\nTry: \"module spider R-Project/3.4.1\" to see how to load the module(s).\n\n\n\n\n\nThis software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load:\n\n\nmodule spider R-Project/3.4.1\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help: \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.\n\n\n\n\nAh-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1\n\n\nAcknowledging Amarel\n\n\n\n\nPlease reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include:\n\n\nOffice of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey\n\n\nA suggested acknowledgement is:\n\n\nThe authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu",
            "title": "Amarel Guide"
        },
        {
            "location": "/Amarel_User_Guide/#general-information",
            "text": "Amarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource.  Amarel includes the following hardware (this list may already be outdated since the cluster is actively growing):  52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM\n20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM\n4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard\n2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM\n53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM\n5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM\n26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM\n4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM\n3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard\n2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard\n1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard  Default run time = 2 hours in the 'main' partition\nMaximum run time = 3 days in the 'main' partition",
            "title": "General Information"
        },
        {
            "location": "/Amarel_User_Guide/#connecting-to-amarel",
            "text": "Amarel is currently accessed using a single hostname, amarel.rutgers.edu  When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc.  So, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using.  ssh [your NetID]@amarel.rutgers.edu  If you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See  here  for details.",
            "title": "Connecting to Amarel"
        },
        {
            "location": "/Amarel_User_Guide/#moving-files",
            "text": "There are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory,  scp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID]  To pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command),  scp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt  .  If you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it:  tar -czf my-directory.tar.gz my-directory  After moving it, you can unpack that .tar.gz file to get your original directory and contents:  tar -xzf my-directory.tar.gz  A handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel:  rsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir  In this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data)  To sync a local directory with updated data from Amarel:  rsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir  Here, we've simply reversed the order of the local and remote locations.  For added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case):  rsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir",
            "title": "Moving files"
        },
        {
            "location": "/Amarel_User_Guide/#listing-available-resources",
            "text": "Before requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources.  Example of using the  sinfo  command:  sinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*        up 3-00:00:00      4 drain* hal[0050-0051,0055,0093]\nmain*        up 3-00:00:00      5  down* slepner[084-088]\nmain*        up 3-00:00:00      4  drain hal[0023,0025-0027]\nmain*        up 3-00:00:00     86    mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082]\nmain*        up 3-00:00:00     84  alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083]\nmain*        up 3-00:00:00      2   down gpu[001-002]\ngpu          up 3-00:00:00      8    mix gpu[003,006],pascal[001-006]\ngpu          up 3-00:00:00      1  alloc gpu005\ngpu          up 3-00:00:00      2   down gpu[001-002]\nphi          up 3-00:00:00      1    mix gpu004\nmem          up 3-00:00:00      1    mix mem002\nmem          up 3-00:00:00      1  alloc mem001  Understanding this output:  There are 4 basic partitions, main (traditional compute nodes, CPUs only), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors.  The upper limit for a job\u2019s run time is 3 days (72 hours).     Term  Meaning      Allocated (alloc)  nodes are currently running jobs.    Mixed (mix)  nodes have jobs using some, but not all, CPU cores onboard.    Idle  nodes are currently available for new jobs.    Drained (drain, drng)  nodes are not available for use and may be offline for maintenance.    Slepner, Norse Mythology  \"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores).    Hal  Hal is a dependable member of the Discovery One crew who does an excellent job of following instructions.    Pascal  French mathematician and the name of one of NVIDIA's GPU architectures.    CUDA  This is the name of a parallel computing platform and application programming interface (API) model created by Nvidia",
            "title": "Listing available resources"
        },
        {
            "location": "/Amarel_User_Guide/#loading-software-modules",
            "text": "When you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.  The module avail command will show a list of the core (primary) modules available:  module avail\n\n-------------------------------------------------- /opt/sw/modulefiles/Core --------------------------------------------------\n   ARACNE/20110228         blat/35                  gcc/4.9.3               intel_mkl/16.0.3 (D)    mvapich2/2.2        (D)\n   HISAT2/2.0.4            bowtie2/2.2.6            gcc/4.9.4               intel_mkl/17.0.0        openmpi/2.1.1\n   HISAT2/2.1.0     (D)    bowtie2/2.2.9     (D)    gcc/5.3                 intel_mkl/17.0.1        pgi/16.9\n   MATLAB/R2017a           bwa/0.7.12               gcc/5.4          (D)    intel_mkl/17.0.2        pgi/16.10           (D)\n   MATLAB/R2017b    (D)    bwa/0.7.13        (D)    hdf5/1.8.16             java/1.7.0_79           python/2.7.11\n   Mathematica/11.1        cuda/7.5                 intel/16.0.1            java/1.8.0_66           python/2.7.12\n   OpenCV/2.3.1            cuda/8.0                 intel/16.0.3     (D)    java/1.8.0_73           python/3.5.0\n   STAR/2.5.2a             cuda/9.0          (D)    intel/16.0.4            java/1.8.0_121          python/3.5.2        (D)\n   Trinotate/2.0.2         cudnn/7.0.3              intel/17.0.0            java/1.8.0_141          samtools/0.1.19\n   bamtools/2.4.0          cufflinks/2.2.1          intel/17.0.1            java/1.8.0_152   (D)    samtools/1.2\n   bcftools/1.2            delly/0.7.6              intel/17.0.2            modeller/9.16           samtools/1.3.1      (D)\n   bedtools2/2.25.0        gaussian/03revE01        intel/17.0.4            moe/2016.0802           trinityrnaseq/2.1.1\n   blast/2.6.0             gaussian/09revD01 (D)    intel_mkl/16.0.1        mvapich2/2.1  Understanding this output:  The packages with a (D) are the default versions for packages where multiple versions are available.  To see a comprehensive list of all available modules (not just the core modules) use the  module spider  command.  module spider\n\n---------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------------------------------------------------\n  ARACNE: ARACNE/20110228\n    ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context\n\n  HISAT2: HISAT2/2.0.4, HISAT2/2.1.0\n    HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes\n\n  HMMER: HMMER/3.1b2\n    HMMER: biosequence analysis using profile hidden Markov models\n\n  MATLAB: MATLAB/R2017a, MATLAB/R2017b\n    MATLAB: The Language of Technical Computing\n\n  Mathematica: Mathematica/11.1\n    Wolfram Mathematica: Modern Technical Computing\n\n  NAMD: NAMD/2.10\n    NAMD: Scalable Molecular Dynamics\n\n  ORCA: ORCA/3.0.3\n    ORCA: An ab initio, DFT and semiempirical SCF-MO package\n\n  OpenCV: OpenCV/2.3.1\n    OpenCV: Open Source Computer Vision  Loading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use. \nTo load a software module, use the module load command, followed by the name and version desired. \nTo remove select modules, use the module unload command. To remove all loaded software modules, use the module purge command. \nTo load the default version of any software package, use the module load command but only specify the name of the package, not the version number.    Below are some examples.  module load intel/16.0.3 mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3   2) mvapich2/2.1\n\nmodule unload mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\nmodule purge\nmodule list\nNo modules loaded\n\nmodule load intel\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3  If you always use the same software modules, your  ~/.bashrc  (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your ~/.bashrc file to change or remove those commands later.  PLEASE NOTE:  Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.",
            "title": "Loading software modules"
        },
        {
            "location": "/Amarel_User_Guide/#running-a-serial-single-core-job",
            "text": "Here\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance.  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=2000                   # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1\n\nsrun /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in  Understanding this job script:  A job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job.  The lines beginning with #SBATCH contain commands intended only for the workload manager.  My job will be assigned to the \u201cmain\u201d partition (job queue).  If this job is preempted, it will be returned to the job queue and will start again when required resources are available  This job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.  My job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running.  Any output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number.  Be sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable.  Here\u2019s how to run a serial batch job, loading modules and using the  sbatch  command:  sbatch my-job-script.sh . \nThe  sbatch  command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.",
            "title": "Running a serial (single-core) job"
        },
        {
            "location": "/Amarel_User_Guide/#running-a-parallel-multicore-mpi-job",
            "text": "Here\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here.  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=16                  # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=124000                 # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1 mvapich2/2.1\n\nsrun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in  Understanding this job script:  The srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request.  This job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.  Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.  Here\u2019s how to run a parallel batch job, loading modules and using the  sbatch  command:  sbatch my-job-script.sh",
            "title": "Running a parallel (multicore MPI) job"
        },
        {
            "location": "/Amarel_User_Guide/#running-an-interactive-job",
            "text": "An interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications.  When submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job:  [NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\nsrun: job 1365471 queued and waiting for resources\nsrun: job 1365471 has been allocated resources\n\n[NetID@slepner045 ~]$  Notice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command.",
            "title": "Running an interactive job"
        },
        {
            "location": "/Amarel_User_Guide/#monitoring-the-status-of-jobs",
            "text": "The simplest way to quickly check on the status of active jobs is by using the  squeue  command:  squeue -u [your NetID]\n\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n1633383      main   zipper    xx345   R       1:15      1 slepner36  Here, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS).  For summary accounting information (including jobs that have already completed), you can use the  sacct  command:  sacct\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n1633383          zipper       main      statx         16    RUNNING      0:0  Here, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED.  For complete and detailed job info, you can use the  scontrol show job [JobID]  command:  scontrol show job 244348\n\nJobId=244348 JobName=XIoT22\n   UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A\n   Priority=5050 Nice=0 Account=oarc QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A\n   SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19\n   StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   Partition=main AllocNode:Sid=amarel1:22391\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=hal0053\n   BatchHost=hal0053\n   NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=28,mem=124000M,node=1\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0\n   Features=(null) Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/gc563/run.STMV.CPU.slurm\n   WorkDir=/scratch/gc563\n   StdErr=/scratch/gc563/slurm.%N.244348.out\n   StdIn=/dev/null\n   StdOut=/scratch/gc563/slurm.%N.244348.out\n   Power=",
            "title": "Monitoring the status of jobs"
        },
        {
            "location": "/Amarel_User_Guide/#killing-cancelling-terminating-jobs",
            "text": "To terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate: scancel 1633383 \nA job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this:  slurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***",
            "title": "Killing/ cancelling/ terminating jobs"
        },
        {
            "location": "/Amarel_User_Guide/#installing-your-own-software",
            "text": "Package management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]).  As an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist:  (1) Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the  wget  utility: wget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz  (2) Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files:  tar -zxf zipper-4.1.5.tar.gz  3) Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software.  cd zipper-4.1.5  less README  4) Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER: module load intel/16.0.3 mvapich2/2.1  (5) Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the  ./configure -\u2013help  command. What you see below is just a typical example of special options that might be specified.  ./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared\nmake -j 4\nmake install  Several packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation:  /home/[NetID]/zipper/4.1.5/bin  where executables will be placed  /home/[NetID]/zipper/4.1.5/lib  where library files will be placed  /home/[NetID]/zipper/4.1.5/include  where header files will be placed  /home/[NetID]/zipper/4.1.5/share/man  where documentation will be placed  (6) Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment:  export PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH\nexport C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH\nexport MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH  These  export  commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.",
            "title": "Installing your own software"
        },
        {
            "location": "/Amarel_User_Guide/#singularity",
            "text": "Singularity  is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers.  Docker  is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.",
            "title": "Singularity"
        },
        {
            "location": "/Amarel_User_Guide/#important-differences-between-docker-and-singularity",
            "text": "Docker and Singularity have their own container formats.  Docker containers can be imported and run using Singularity.  Docker containers usually run as root, which means you cannot run Docker on a  shared computing system (cluster).  Singularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.",
            "title": "Important differences between Docker and Singularity:"
        },
        {
            "location": "/Amarel_User_Guide/#importing-a-docker-image",
            "text": "If you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container.  Here's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel.  If you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories.  Start your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel. sudo docker run -it ubuntu:latest bash  \nroot@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc",
            "title": "Importing a Docker image:"
        },
        {
            "location": "/Amarel_User_Guide/#exporting-your-docker-image",
            "text": "Find the name of your Docker image using the 'docker ps' command,  sudo docker ps\nCONTAINER ID  IMAGE          COMMAND  CREATED        STATUS        NAMES\n11a87dkw8748  ubuntu:latest  \"bash\"   2 minutes ago  Up 2 minutes  bendakaya_pakodi  In this example the name of the images is bendakaya_pakodi. Export this image to a tarball,  sudo docker export bendakaya_pakodi > ubuntu.tar",
            "title": "Exporting your Docker image"
        },
        {
            "location": "/Amarel_User_Guide/#converting-to-a-singularity-image",
            "text": "You will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel.  Create an empty singularity image, and then import the exported docker image into it,  sudo singularity create ubuntu.img\nCreating a sparse image with a maximum size of 1024MiB...\nUsing given image size of 1024\nFormatting image (/sbin/mkfs.ext3)\nDone. Image can be found at: ubuntu.img\n$ sudo singularity import ubuntu.img ubuntu.tar",
            "title": "Converting to a Singularity image"
        },
        {
            "location": "/Amarel_User_Guide/#using-singularity-containers-inside-a-slurm-job",
            "text": "Transfer  your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel.  You can run any task/program inside the container by prefacing it with 'singularity exec [your image name]'  Here is a simple example job script that executes commands inside a container,  #SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=sing2me           # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                   # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n\nmodule purge\nmodule load singularity/.2.5.1\n\n## Where am I running?\nsrun singularity exec ubuntu.img hostname\n\n## What is the current time and date?\nsrun singularity exec ubuntu.img date  If you created directories for any Amarel filesystems, you should find they are mounted inside your container,  mount | grep gpfs\n/dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime)\n/dev/projects/oarc on /projects/oarc type gpfs (rw,relatime)  NOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.",
            "title": "Using Singularity containers inside a SLURM job"
        },
        {
            "location": "/Amarel_User_Guide/#troubleshooting-common-problems",
            "text": "Failure to load module dependencies/prerequisites:   module load R-Project/3.4.1\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"R-Project/3.4.1\"\nTry: \"module spider R-Project/3.4.1\" to see how to load the module(s).  This software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load:  module spider R-Project/3.4.1\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help: \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.  Ah-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1",
            "title": "Troubleshooting/ Common Problems"
        },
        {
            "location": "/Amarel_User_Guide/#acknowledging-amarel",
            "text": "Please reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include:  Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey  A suggested acknowledgement is:  The authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu",
            "title": "Acknowledging Amarel"
        },
        {
            "location": "/Cluster_Examples/",
            "text": "Using R\n\n\n\n\nGenerally, there are 2 approaches for accessing R on Amarel:\n\n(1) use one of the pre-installed R modules named R-Project/\nversion\n (these versions come bundled with a very broad range of common and useful tools). \n\n(2) install your own custom build of R in your /home directory or in a shared directory (e.g. \n/projects/[group]\n or \n/projects/community\n).\n\n\nUsing pre-installed R modules\n\n\nStart by finding which module you wish to use with the 'module spider R-Project' command:\n\n\nmodule spider R-Project\n--------------------------------------------------\n  R-Project:\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n     Versions:\n        R-Project/3.2.2\n        R-Project/3.2.5\n        R-Project/3.3.3\n        R-Project/3.4.1\n--------------------------------------------------\n  To find detailed information about R-Project please enter the full name.\n  For example:\n     $ module spider R-Project/3.4.1\n--------------------------------------------------\n\n\n\n\nNext, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?):\n\n\n module spider R-Project/3.4.1\n--------------------------------------------------\n  R-Project: R-Project/3.4.1\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help:    \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.\n\n\n\n\nLoad the R-Project module of your choice:\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\nwhich R\n/opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R\n\n\n\n\nWhat R packages are already installed?\n\n\npkgs <- installed.packages ()\npkgs[,c(\"Package\", \"Version\")]\n\n                     Package                Version    \nbase                 \"base\"                 \"3.4.4\"    \nBH                   \"BH\"                   \"1.66.0-1\" \nBiobase              \"Biobase\"              \"2.38.0\"   \nBiocGenerics         \"BiocGenerics\"         \"0.24.0\"   \nBiocInstaller        \"BiocInstaller\"        \"1.28.0\"   \nBiocParallel         \"BiocParallel\"         \"1.12.0\"   \nBiostrings           \"Biostrings\"           \"2.46.0\"   \nbitops               \"bitops\"               \"1.0-6\"    \nboot                 \"boot\"                 \"1.3-20\"   \nBSgenome             \"BSgenome\"             \"1.46.0\"\n...\n\n\n\n\nIt's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account.\n\n\nFirst, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access:\n\n\nmkdir ~/my.R.libs\n\n\nNext, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file:\n\n\nexport R_LIBS=~/my.R.libs\n\nNow, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library.\n\n\n Some important notes about installing packages:\n\n\nThere are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball (\n.tar.gz or \n.tgz file) containing the source code for that version.\n\n\nFor example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the \nwebsite\n. At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command:  \n\n\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz\", lib=\"~/my.R.libs\")\n\n\n\n\nThe other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows:\n\n\ninstall.packages(\"gplots\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(c(\"mvtnorm\",\"modeltools\",\"strucchange\",\"coin\",\"zoo\",\"sandwich\"), lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz\", lib=\"~/my.R.libs\")\n\n\n\n\nOnce all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.\n\n\nInstalling your own build of R\n\n\nFor some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below.\n\n\nHere are the commands to use for installing R-3.4.4 from source:\n\n\nwget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz\ntar -zxf R-3.4.4.tar.gz\ncd R-3.4.4\nmodule load gcc/5.4 java/1.8.0_161\n./configure --prefix=/home/gc563/R/3.4.4 --enable-java\nmake -j 4\nmake install\ncd ..\nrm -rf R-3.4.4*\n\n\n\n\nHere, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages.\n\n\nWhen I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference.\n\n\nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.\n\n\nSince I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory.\n\n\nBefore using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):\n\n\nmodule load gcc/5.4 java/1.8.0_161\nexport PATH=/home/gc563/R/3.4.4/bin:$PATH\nexport LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport MANPATH=/home/gc563/R/3.4.4/share/man\n\n\n\n\nIf you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:\n\n\nmodule list\nCurrently Loaded Modules:\n  1) gcc/5.4   2) java/1.8.0_161\nwhich R\n~/R/3.4.4/bin/R\n\n\n\n\nNow that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library\n\n\n.libPaths()  \n[1] \"/home/gc563/R/3.4.4/lib64/R/library\"\n\n\nInstall a package:\n\n\ninstall.packages(\"rJava\")\n   library(rJava)\n\n\n\n\nSaving figures/ plots from R (without a display):\n\n\nNeed to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example:\n\n\nR --no-save\npng('my-figure.png', type='cairo')\nplot(rnorm(10),rnorm(10))\ndev.off()\nq()\n\n\n\n\nUsing Python\n\n\n\n\nGenerally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).\n\n\nUsing pre-installed Python modules\n\n\nWith the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package:\n\n\nmodule load python/3.5.2\npip install --user Django\n\n\n\n\nNote: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly:\n\n\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\npip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5)\n$ pip install -U --user pip\nSuccessfully installed pip-10.0.1\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\nTraceback (most recent call last):\n  File \"/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\", line 7, in \n    from pip import main\nImportError: cannot import name 'main'\n\n$ .local/bin/pip --version\npip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5)\n$ .local/bin/pip install --user Django\n\n\n\n\nBuilding your own Python installation\n\n\nUsing this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference).\n\nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.\n\n\nwget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz\ntar -zxf Python-3.6.5.tgz\ncd Python-3.6.5\n./configure --prefix=/home/gc563/python/3.6.5\nmake -j 4\nmake install\ncd ..\nrm -rf Python-3.6.5*\n\n\n\n\nBefore using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):\n\n\nexport PATH=/home/gc563/python/3.6.5/bin:$PATH\nexport LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib\nexport MANPATH=/home/gc563/python/3.6.5/share/man\n\n\n\n\nIf you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:\n\n\nwhich python3  \n~/python/3.6.5/bin\n\n\nRunning GROMACS\n\n\n\n\nHere is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel.\n\n(1) Download a PDB file.\n\n\nwget https://files.rcsb.org/view/5EWT.pdb\n\n(2) Load the GROMACS software module plus any needed prerequisites.\n\n\nmodule purge  \nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\n\n(3) Import the PDB into GROMACS, while defining the force field and water model to be used for this system.\n\n\ngmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp\n\n(4) Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein.  \n\n\ngmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5\n\n(5) Now add water molecules into the empty space in the unit cell to solvate the protein.\n\n\ngmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro\n\n(6) Prepare your SLURM job script(s). The 2 \nmdrun\n commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2 \nmdrun\n commands below, simply replace the example \nmdrun\n command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate \ngrompp\n step.\n\n\n#!/bin/bash\n#SBATCH --partition=main                # Partition (job queue)\n#SBATCH --job-name=gmdrun               # Assign an 8-character name to your job\n#SBATCH --nodes=1                       # Number of nodes\n#SBATCH --ntasks=16                     # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1               # Threads per process (or per core)\n#SBATCH --mem=124000                    # Memory per node (MB)\n#SBATCH --time=00:20:00                 # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out        # combined STDOUT and STDERR output file\n#SBATCH --export=ALL                    # Export you current env to the job env\n\nmodule purge\nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\nsrun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\\n                -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\\n                -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log\n\n\n\n\n(7) Perform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions:\n\n\nintegrator     = steep\nnsteps         = 200\ncutoff-scheme  = Verlet\ncoulombtype    = PME\npbc            = xyz\nemtol          = 100\n\n\n\n\nThese are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization:\n\n\ngmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log\n\n\n\n\n(8) Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions:\n\n\nintegrator               = md\ndt                       = 0.002\nnsteps                   = 5000\nnstlog                   = 50\nnstenergy                = 50\nnstxout                  = 50\ncontinuation             = yes\nconstraints              = all-bonds\nconstraint-algorithm     = lincs\ncutoff-scheme            = Verlet\ncoulombtype              = PME\nrcoulomb                 = 1.0\nvdwtype                  = Cut-off\nrvdw                     = 1.0\nDispCorr                 = EnerPres\ntcoupl                   = V-rescale\ntc-grps                  = Protein  SOL\ntau-t                    = 0.1      0.1\nref-t                    = 300      300\npcoupl                   = Parrinello-Rahman\ntau-p                    = 2.0\ncompressibility          = 4.5e-5\nref-p                    = 1.0\n\n\n\n\nThese are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration:\n\n\ngmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log\n\n\n\n\nRunning TensorFlow with a GPU\n\n\n\n\nTo do this, you can use the \nSingularity\n container manager and a Docker image containing the TensorFlow software.\n\nRunning Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script TF_gpu.sh)\n\n\n#!/bin/bash\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=TF_gpu            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=2            # Cores per task (>1 if multithread tasks)\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --mem=16000                  # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\nmodule load singularity/.2.5.1\n\nsrun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python \n\n\n\n\nOnce your job script is ready, submit it using the sbatch command:\n\n\nsbatch TF_gpu.sh\n\nAlternatively, you can run Singularity interactively:\n\n\nsrun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu\n\nDocker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu\nCache folder set to /home/user/.singularity/docker\nCreating container runtime...\nImporting: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz\nImporting: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz\nImporting: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz\nImporting: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz\nImporting: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz\nImporting: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz\nImporting: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz\nImporting: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz\nImporting: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz\nImporting: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz\nSingularity: Invoking an interactive shell within container...\n\nSingularity tensorflow:latest-gpu:~> \n\n\n\n\nNow, you're ready to execute commands:\n\n\n\nSingularity tensorflow:latest-gpu:~> python -V\nPython 2.7.12\nSingularity tensorflow:latest-gpu:~> python3 -V\nPython 3.5.2\n\n\n\n\nPlease remember to exit from your interactive job after you are finished with your calculations.\n\nThere are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be 'python3' instead of 'python' in your script.",
            "title": "Examples"
        },
        {
            "location": "/Cluster_Examples/#using-r",
            "text": "Generally, there are 2 approaches for accessing R on Amarel: \n(1) use one of the pre-installed R modules named R-Project/ version  (these versions come bundled with a very broad range of common and useful tools).  \n(2) install your own custom build of R in your /home directory or in a shared directory (e.g.  /projects/[group]  or  /projects/community ).",
            "title": "Using R"
        },
        {
            "location": "/Cluster_Examples/#using-pre-installed-r-modules",
            "text": "Start by finding which module you wish to use with the 'module spider R-Project' command:  module spider R-Project\n--------------------------------------------------\n  R-Project:\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n     Versions:\n        R-Project/3.2.2\n        R-Project/3.2.5\n        R-Project/3.3.3\n        R-Project/3.4.1\n--------------------------------------------------\n  To find detailed information about R-Project please enter the full name.\n  For example:\n     $ module spider R-Project/3.4.1\n--------------------------------------------------  Next, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?):   module spider R-Project/3.4.1\n--------------------------------------------------\n  R-Project: R-Project/3.4.1\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help:    \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.  Load the R-Project module of your choice:  module load intel/17.0.4 R-Project/3.4.1\nwhich R\n/opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R  What R packages are already installed?  pkgs <- installed.packages ()\npkgs[,c(\"Package\", \"Version\")]\n\n                     Package                Version    \nbase                 \"base\"                 \"3.4.4\"    \nBH                   \"BH\"                   \"1.66.0-1\" \nBiobase              \"Biobase\"              \"2.38.0\"   \nBiocGenerics         \"BiocGenerics\"         \"0.24.0\"   \nBiocInstaller        \"BiocInstaller\"        \"1.28.0\"   \nBiocParallel         \"BiocParallel\"         \"1.12.0\"   \nBiostrings           \"Biostrings\"           \"2.46.0\"   \nbitops               \"bitops\"               \"1.0-6\"    \nboot                 \"boot\"                 \"1.3-20\"   \nBSgenome             \"BSgenome\"             \"1.46.0\"\n...  It's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account.  First, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access:  mkdir ~/my.R.libs  Next, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file:  export R_LIBS=~/my.R.libs \nNow, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library.   Some important notes about installing packages:  There are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball ( .tar.gz or  .tgz file) containing the source code for that version.  For example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the  website . At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command:    install.packages(\"https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz\", lib=\"~/my.R.libs\")  The other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows:  install.packages(\"gplots\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(c(\"mvtnorm\",\"modeltools\",\"strucchange\",\"coin\",\"zoo\",\"sandwich\"), lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz\", lib=\"~/my.R.libs\")\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz\", lib=\"~/my.R.libs\")  Once all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.",
            "title": "Using pre-installed R modules"
        },
        {
            "location": "/Cluster_Examples/#installing-your-own-build-of-r",
            "text": "For some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below.  Here are the commands to use for installing R-3.4.4 from source:  wget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz\ntar -zxf R-3.4.4.tar.gz\ncd R-3.4.4\nmodule load gcc/5.4 java/1.8.0_161\n./configure --prefix=/home/gc563/R/3.4.4 --enable-java\nmake -j 4\nmake install\ncd ..\nrm -rf R-3.4.4*  Here, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages.  When I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference.  At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.  Since I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory.  Before using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):  module load gcc/5.4 java/1.8.0_161\nexport PATH=/home/gc563/R/3.4.4/bin:$PATH\nexport LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport MANPATH=/home/gc563/R/3.4.4/share/man  If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:  module list\nCurrently Loaded Modules:\n  1) gcc/5.4   2) java/1.8.0_161\nwhich R\n~/R/3.4.4/bin/R  Now that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library  .libPaths()  \n[1] \"/home/gc563/R/3.4.4/lib64/R/library\"  Install a package:  install.packages(\"rJava\")\n   library(rJava)  Saving figures/ plots from R (without a display):  Need to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example:  R --no-save\npng('my-figure.png', type='cairo')\nplot(rnorm(10),rnorm(10))\ndev.off()\nq()",
            "title": "Installing your own build of R"
        },
        {
            "location": "/Cluster_Examples/#using-python",
            "text": "Generally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).",
            "title": "Using Python"
        },
        {
            "location": "/Cluster_Examples/#using-pre-installed-python-modules",
            "text": "With the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package:  module load python/3.5.2\npip install --user Django  Note: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly:  $ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\npip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5)\n$ pip install -U --user pip\nSuccessfully installed pip-10.0.1\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\nTraceback (most recent call last):\n  File \"/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\", line 7, in \n    from pip import main\nImportError: cannot import name 'main'\n\n$ .local/bin/pip --version\npip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5)\n$ .local/bin/pip install --user Django",
            "title": "Using pre-installed Python modules"
        },
        {
            "location": "/Cluster_Examples/#building-your-own-python-installation",
            "text": "Using this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference). \nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.  wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz\ntar -zxf Python-3.6.5.tgz\ncd Python-3.6.5\n./configure --prefix=/home/gc563/python/3.6.5\nmake -j 4\nmake install\ncd ..\nrm -rf Python-3.6.5*  Before using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):  export PATH=/home/gc563/python/3.6.5/bin:$PATH\nexport LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib\nexport MANPATH=/home/gc563/python/3.6.5/share/man  If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:  which python3  \n~/python/3.6.5/bin",
            "title": "Building your own Python installation"
        },
        {
            "location": "/Cluster_Examples/#running-gromacs",
            "text": "Here is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel. \n(1) Download a PDB file.  wget https://files.rcsb.org/view/5EWT.pdb \n(2) Load the GROMACS software module plus any needed prerequisites.  module purge  \nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1 \n(3) Import the PDB into GROMACS, while defining the force field and water model to be used for this system.  gmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp \n(4) Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein.    gmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5 \n(5) Now add water molecules into the empty space in the unit cell to solvate the protein.  gmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro \n(6) Prepare your SLURM job script(s). The 2  mdrun  commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2  mdrun  commands below, simply replace the example  mdrun  command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate  grompp  step.  #!/bin/bash\n#SBATCH --partition=main                # Partition (job queue)\n#SBATCH --job-name=gmdrun               # Assign an 8-character name to your job\n#SBATCH --nodes=1                       # Number of nodes\n#SBATCH --ntasks=16                     # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1               # Threads per process (or per core)\n#SBATCH --mem=124000                    # Memory per node (MB)\n#SBATCH --time=00:20:00                 # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out        # combined STDOUT and STDERR output file\n#SBATCH --export=ALL                    # Export you current env to the job env\n\nmodule purge\nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\nsrun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\\n                -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\\n                -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log  (7) Perform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions:  integrator     = steep\nnsteps         = 200\ncutoff-scheme  = Verlet\ncoulombtype    = PME\npbc            = xyz\nemtol          = 100  These are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization:  gmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log  (8) Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions:  integrator               = md\ndt                       = 0.002\nnsteps                   = 5000\nnstlog                   = 50\nnstenergy                = 50\nnstxout                  = 50\ncontinuation             = yes\nconstraints              = all-bonds\nconstraint-algorithm     = lincs\ncutoff-scheme            = Verlet\ncoulombtype              = PME\nrcoulomb                 = 1.0\nvdwtype                  = Cut-off\nrvdw                     = 1.0\nDispCorr                 = EnerPres\ntcoupl                   = V-rescale\ntc-grps                  = Protein  SOL\ntau-t                    = 0.1      0.1\nref-t                    = 300      300\npcoupl                   = Parrinello-Rahman\ntau-p                    = 2.0\ncompressibility          = 4.5e-5\nref-p                    = 1.0  These are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration:  gmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log",
            "title": "Running GROMACS"
        },
        {
            "location": "/Cluster_Examples/#running-tensorflow-with-a-gpu",
            "text": "To do this, you can use the  Singularity  container manager and a Docker image containing the TensorFlow software. \nRunning Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script TF_gpu.sh)  #!/bin/bash\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=TF_gpu            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=2            # Cores per task (>1 if multithread tasks)\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --mem=16000                  # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\nmodule load singularity/.2.5.1\n\nsrun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python   Once your job script is ready, submit it using the sbatch command:  sbatch TF_gpu.sh \nAlternatively, you can run Singularity interactively:  srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu\n\nDocker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu\nCache folder set to /home/user/.singularity/docker\nCreating container runtime...\nImporting: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz\nImporting: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz\nImporting: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz\nImporting: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz\nImporting: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz\nImporting: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz\nImporting: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz\nImporting: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz\nImporting: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz\nImporting: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz\nSingularity: Invoking an interactive shell within container...\n\nSingularity tensorflow:latest-gpu:~>   Now, you're ready to execute commands:  \nSingularity tensorflow:latest-gpu:~> python -V\nPython 2.7.12\nSingularity tensorflow:latest-gpu:~> python3 -V\nPython 3.5.2  Please remember to exit from your interactive job after you are finished with your calculations. \nThere are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be 'python3' instead of 'python' in your script.",
            "title": "Running TensorFlow with a GPU"
        },
        {
            "location": "/howtos/jupyter/",
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u <your net id>\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:",
            "title": "Jupyter"
        },
        {
            "location": "/howtos/jupyter/#tunneling",
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:",
            "title": "Tunneling"
        },
        {
            "location": "/howtos/jupyter/#how-to-launch-jupyter-notebook-on-the-cluster",
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u <your net id>  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:",
            "title": "How to launch Jupyter notebook on the cluster"
        },
        {
            "location": "/howtos/fastx/",
            "text": "Connecting to the remote Linux cluster makes running graphical programs remotely tricky, because the graphical program runs on the remote computer, yet, it must be displayed on the local machine such as laptop. In general, command line interaction with the cluster is normally preferred and even more efficient than a GUI. However, there are times when running a graphical program cannot be avoided, for example, running a debugger for a code running on the cluster. \n\n\nThere is a convenient way to run a graphical program remotely, for example, using FastX. Here is the procedure: \n\n\n\n\nGo to htts://amarel.hpc.rutgers.edu:3443  and log in (you must be either on campus, or connected through VPN)\n\n\nClick on Launch session\n\n\nClick on xterm\n\n\nRun this command that will ask the resource scheduler to put you on a compute node, rather than a login node (where you shouldn't be running intensive computations)\n\n\nsrun -p main -N 1 -c 2 --mem=4Gb -t 1:00:00 --pty /bin/bash\n\nThis command puts you on main partition, asks for 2 cores on 1 node, asks for 4 Gb or memory and time of 1 hour and runs interactive shell. After executing this, you should notice that the name of the node changed from \namarel\n to \nslepner036\n or some such. You can request whatever resources you deem necessary for your work, but keep in mind that bigger requests are typically placed further down in the queue. \n\n\nStart the program with graphical interface from the terminal window.\n\n\n\n\nHere is the video walking through these steps:",
            "title": "FastX"
        },
        {
            "location": "/workshops/workshop/",
            "text": "Overview\n\n\nThese are the notes from the workshop on genomics. What's covered in these notes:  \n\n1.vk1 Setup\n\n\nGenomic Software\n\n\nThis is a list of software to install for the workshop\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nSeqtk\n\n\nvery handy and fast for processing fastq/a files\n\n\nlink\n\n\n\n\n\n\nsratoolkit\n\n\ndownloading and processing data from GEO/SRA database\n\n\nlink\n\n\n\n\n\n\nhtseq-count\n\n\ncounting the  reads mapped on to genomics feature\n\n\nlink\n\n\n\n\n\n\nfastQC\n\n\nwidely used for sequencing read QC\n\n\nlink\n\n\n\n\n\n\nRSeQC-2.6.4\n\n\nAn RNA-seq quality control package, multiple functions\n\n\npython package\n\n\n\n\n\n\ntrimmomatic\n\n\nfastq quality trim and adaptor removal\n\n\nlink\n\n\n\n\n\n\n\n\nThis is a list of software already available on the cluster and the command you need to execute to load it in your environment: \n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nload it on the cluster\n\n\n\n\n\n\n\n\n\n\nSamtools\n\n\n\n\nmodule load samtools\n\n\n\n\n\n\nBedtools\n\n\n\n\nmodule load bedtools2./2.25.0\n\n\n\n\n\n\nbowtie2\n\n\nalignment software\n\n\nmodule load bowtie2\n\n\n\n\n\n\ntophat2\n\n\nalignment software\n\n\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n\n\n\n\n\n\nR\n\n\nlanguage for statistical analysis\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\n\n\n\n\nThis is a list of other software you might find useful:\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nGSEA\n\n\ngenome set enrichment analysis\n\n\nlink\n\n\n\n\n\n\nIGV\n\n\nInteractive Genome Viewer\n\n\nlink\n\n\n\n\n\n\nCytoscape\n\n\nNetwork visualization softwar\n\n\nlink\n\n\n\n\n\n\n\n\nSetup\n\n\nConnect to the cluster login node\n\n\nThe preferred method to connect to the cluster is through a web browser and fastX client\n - \nvia FastX\n: in your browser, go to `https://amarel.hpc.rutgers.edu:3443\nIf the above method doesn't work you may still connect \n\n\n\n\nvia a terminal\n: if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as \nmobaXterm\n [link] (https://mobaxterm.mobatek.net/). Then from your terminal connect to the cluster by executing the following command: \n\n\nssh -X <your net id>@amarel.hpc.rutgers.edu\n   \n\n\n\n\nGet resources on the compute node\n\n\nYou get to the cluster to execute your computations by running the following command in your terminal:  \n\n\nsrun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash\n \n\nNotice that the name in your terminal will change from \namarel\n to node name like \nhal0025\n or \nslepner086\n. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command: \n\n\n\n\n\n\n\n\ncommand part\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\nsrun\n\n\nslurm\n run, i.e. allocate resources and run via \nslurm\n scheduler\n\n\n\n\n\n\n-p main\n\n\non the main partition, one of several queues on the cluster\n\n\n\n\n\n\n--reservation=genomics\n\n\nwe reserved some compute nodes for this workshop to not wait long for resources\n\n\n\n\n\n\n-N 1\n\n\nask for one node\n\n\n\n\n\n\n-c 2\n\n\nask for two cores\n\n\n\n\n\n\n-n 1\n\n\nthis will be 1 most times\n\n\n\n\n\n\n-t 01:40:00\n\n\nrun this for a maximum time of 1 hour 40 minutes\n\n\n\n\n\n\n--pty /bin/bash\n\n\nrun the terminal shell in an interactive mode\n\n\n\n\n\n\n\n\nPrepare some directories for the data\n\n\nYou have two main spaces on the Amarel cluster. These are: \n\n\n\n\nyour home directory - \n/home/netid/\n  (e.g. \n/home/kp807/\n for my netid) -\n\n\nyour scratch directory - \n/scratch/netid/\n\n\n\n\nThey differ in how often they are backed up and by size (100Gb for \n/home\n and 500Gb for \n/scratch\n). So we will install programs in \n/home\n, while the data and output will be in \n/scratch\n. Execute these commands: \n\n\n                cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here\n\n\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nInstall programs\n\n\nEach program will have slightly different installation instructions. Here is a handy sequence of commands that will install them: \n\n\n##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                    \n\n\n\n\n\nMoreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e. \nsamtools\n command will work if you execute \nmodule load samtools\n): \n\n\nmodule load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\nDownload data\n\n\nWe will download human RNA-seq data with \nGEO accession GSE52778\n. The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nWe will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored. \nsratoolkit\n will be in your home directory, under \nPrograms\n, and the \nvdb-config\n might be under the \nbin\n directory. You will enter \n/scratch/your_netID/Genomics_Workshop/download\n for the path - NOTE you have to replace \nyour_netID\n with your true netId, e.g. \nkp807\n. Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them. \n\n\n                vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y\n\n\n\n\nThen execute the following commands to get the data. Both \nprefetch\n and \nfastq-dump\n are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]\n\n\nprefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ???? \n\n\n\n\nYou have to pay attention to where you are putting your data. So these two commands will actually be several: \n\n\n                cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516\n\n\n\n\nThe commands above showed how to do it for one sample. You need to do it for 6 samples total. \n\n\n                SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)\n\n\n\n\nRunning bioinformatics jobs\n\n\nFastQC - raw data QC\n\n\nExplain what is fastqc is doing here - TODO\n\n\n        cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq\n\n\n\n\nFastQC produces an html page as output, \nfastqc/SRR1039508_1_fastqc.html\n, with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf\n\n\nTrimmomatic - quality trim/adaptor removal\n\n\n    ##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n\n\n\nNOTE:\n  the above is a one line command, illustrated as the following:\n\n\n        java -jar trimmomatic-0.36.jar PE \\\n        -phred33 -trimlog trim.log \\\n        input_1.fq  input_2.fq \\\n        output_1_paired.fq  output_1_unpaired.fq \\\n        output_2_paired.fq  output_2_unpaired.fq \\\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 / \n\n        ## Once it started run, you shall see the following:\n        TrimmomaticPE: Started with arguments:\n        -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n        Multiple cores found: Using 2 threads\n        Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n        ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n        Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%)\n        TrimmomaticPE: Completed successfully\n\n        ##view the output, the trim.log file, .e.g.  length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained)\n\n        ##you may also try fastx_quality_stats from the FASTX\u2014toolkit\n\n\n\n\n\nFastQC - Run on cleaned reads, compare result\n\n\n     module load java\n     fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n\n     ## /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf , helpful in viewing and interpreting the output\n\n\n\n\nDownload reference and reference indexing\n\n\nHuman genome indexing will take hours, we have the reference pre-prepared. Stored at  \n/projects/oarc/Genomics_Workshop/Reference/\n\nFor in class practice, we will do this on E.coli genome\n\n\n        cd /scratch/$USER/Genomics_Workshop/\n        mkdir Reference\n        cd Reference\n\n        wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz\n\n        gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz\n        module load bowtie2\n        bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic\n\n        ##if download from ENSEMBLE\n        wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-38/fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/dna/Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.dna.toplevel.fa.gz\n\n\n\n\nMapping with tophat2, (STAR, HISAT2)\n\n\nNow, go to your data folder\n        \ncd  /scratch/$USER/Genomics_Workshop/untreated\n\n\n        cd  /scratch/$USER/Genomics_Workshop/untreated\n        module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n        module load samtools   #bowtie2 is loaded already\n        mkdir tophat_out\n        tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039508_1\n        0k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/  hg20_transciptome/GR\n        Ch38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel\n        SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n        ## you shall modify the -p value to be consistent with the -c value you requested in the beginning\n\n\n\n\nYou shall see something like:\n\n\n   [2018-03-30 11:48:57] Beginning TopHat run (v2.1.0)\n-----------------------------------------------\n[2018-03-30 11:48:57] Checking for Bowtie\n                  Bowtie version:        2.2.9.0\n[2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome)..\n[2018-03-30 11:48:58] Checking for Bowtie index files (genome)..\n[2018-03-30 11:48:58] Checking for reference FASTA file\n[2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen\nce/hg20/Homo_sapiens.GRCh38.dna.toplevel\n[2018-03-30 11:49:09] Reading known junctions from GTF file\n[2018-03-30 11:49:27] Preparing reads\n         left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded)\n        right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded)\n[2018-03-30 11:49:29] Using pre-built transcriptome data..\n[2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads\n[2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t\noplevel with Bowtie2\n[2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n[2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2\n[2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\n\n\n\n\nThe transcriptome index was built  by pointing to gtf file first,  here we have it prepared already, just so we can save time.  The following would be the command to generate the transcriptome index while running tophat alignment.\n\n\n   tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039516 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/ Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039516_1.fastq.pairedOut.fastq SRR1039516_2.fastq.pairedOut.fastq\n\n\n\n\n\nThe output folder \ntophat_out/untreated_SRR1039508/\n shall contain the following files/folder (in blue): \n\n\n     cd  /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508\n        $ ll\n   total 2183632\n -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam\n -rw-rw-r-- 1 yc759 oarc        565 Jan 16 21:57 align_summary.txt\n -rw-rw-r-- 1 yc759 oarc    1921529 Jan 16 21:57 deletions.bed\n -rw-rw-r-- 1 yc759 oarc    2239884 Jan 16 21:57 insertions.bed\n -rw-rw-r-- 1 yc759 oarc   14181618 Jan 16 21:57 junctions.bed\n drwxrwsr-x 2 yc759 oarc       4096 Jan 16 21:57 logs\n -rw-rw-r-- 1 yc759 oarc        184 Jan 16 21:57 prep_reads.info\n -rw-rw-r-- 1 yc759 oarc   42846571 Jan 16 21:57 unmapped.bam\n\n\n\n\nRead counts using htseq-count\n\n\nGO TO WHERE YOUR ALIGNMENT OUTPUT FOLDER IS, FOR EXAMPLE: \n\n\n     cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n     ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/ untreated_SRR1039508/accepted_hits.bam accepted_hits.bam   \n\n     ##make a soft link to the full bam file we already prepared, if you didn\u2019t have the bam ready yet\n\n     module load samtools intel/17.0.2 python/2.7.12 \n     samtools sort -n  accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name  -/projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf > untreated08.txt\n\n\n\n\nUse samtools to sort the bam file by name:  because htseq-count accepts bam file sorted by name as default, but tophat generates bam sorted by coordinates by default\nThe same way to generate the counts file \nuntreated12.txt\n, \nuntreated16.txt\n,\ndex09.txt\n, \ndex13.txt\n, \ndex17.txt\n\n\nPerform Mapping QC using RSeQC\n\n\nNow,  quality control using RSeQC \u2013a few examples here, please go to the website for more functions \nhttp://rseqc.sourceforge.net/\n\n\ncd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \nmodule load python/2.7.12\nmodule load intel/17.0.4\n\nread_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/\nReference/Homo_sapiens.GRCh38.79.bed\nprocessing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n        processing accepted_hits.bam ... Finished\n\n        Total Reads                   43474036\n        Total Tags                    54438789\n        Total Assigned Tags           53991382\n        =====================================================================\n        Group               Total_bases         Tag_count           Tags/Kb\n        CDS_Exons           103371993           43264842            418.54\n        5'UTR_Exons         5217678             583447              111.82\n        3'UTR_Exons         29324747            8145122             277.76\n        Introns             1500197093          1805034             1.20\n        TSS_up_1kb          33306654            18893               0.57\n        TSS_up_5kb          148463534           41165               0.28\n        TSS_up_10kb         265823549           55644               0.21\n        TES_down_1kb        35215293            50954               1.45\n        TES_down_5kb        152556214           113325              0.74\n        TES_down_10kb       268614580           137293              0.51\n        =====================================================================\n\nbam_stat.py -i accepted_hits.bam\n        Load BAM file ...\n\n        Done\n\n        #==================================================\n        #All numbers are READ count\n        #==================================================\n\n        Total records:                          52528699\n\n        QC failed:                              0\n        Optical/PCR duplicate:                  0\n        Non primary hits                        9054663\n        Unmapped reads:                         0\n        mapq < mapq_cut (non-unique):           2684801\n\n        mapq >= mapq_cut (unique):              40789235\n        Read-1:                                 20414530\n        Read-2:                                 20374705\n        Reads map to '+':                       20393901\n        Reads map to '-':                       20395334\n        Non-splice reads:                       30860931\n        Splice reads:                           9928304\n        Reads mapped in proper pairs:           32386536\n        Proper-paired reads map to different chrom:312\n\n\n\n\n\nThe script does genebody coverage calculation requires the input bam files to be sorted and indexed (we will do it using samtools). The calculation and plot will require R\nGo to one of the tophat_out sample folder\n\n\nmodule load intel/17.0.4  R-Project/3.4.1    \nmodule load samtools\nsamtools sort accepted_hits.bam \u2013o accepted_hits.sorted.bam \n##this may take a while, you may use the one already prepared for you by making a soft link\n\nln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam\n\n\nsamtools index accepted_hits.sorted.bam\n\ngeneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test\n@ 2018-01-14 13:17:33: Read BED file (reference gene model) ...\n@ 2018-01-14 13:17:33: Total 3802 transcripts loaded\n@ 2018-01-14 13:17:33: Get BAM file(s) ...\n        accepted_hits.sorted.bam\n@ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ...\n        3800 transcripts finished\n\n\n        Sample  Skewness\n        accepted_hits.sorted    -3.61577607436\n@ 2018-01-14 13:28:59: Running R script ...\nnull device\n          1\n\n\n\n\n\noutput files:        test.geneBodyCoverage.r\n                                test.geneBodyCoverage.txt\n                log.txt\n                test.geneBodyCoverage.curves.pdf\n\n\ndownload the gene.bed files :  \nhttps://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/\n\nBe careful that the genome version, be consistent between reference genome used in mapping and now. For now, you may use what\u2019s provided \n\n\nif you want to find out whether the sequencing read is strand specific or not, do:\n\n\n        $ infer_experiment.py -r  /projects/oarc/Genomics_Workshop/Reference/ Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam\n\n    Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n    Loading SAM/BAM file ...  Total 200000 usable reads were sampled\n\n\nThis is PairEnd Data\nFraction of reads failed to determine: 0.1406\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292",
            "title": "Genomics"
        },
        {
            "location": "/workshops/workshop/#overview",
            "text": "These are the notes from the workshop on genomics. What's covered in these notes:   \n1.vk1 Setup",
            "title": "Overview"
        },
        {
            "location": "/workshops/workshop/#genomic-software",
            "text": "This is a list of software to install for the workshop     software  description  link      Seqtk  very handy and fast for processing fastq/a files  link    sratoolkit  downloading and processing data from GEO/SRA database  link    htseq-count  counting the  reads mapped on to genomics feature  link    fastQC  widely used for sequencing read QC  link    RSeQC-2.6.4  An RNA-seq quality control package, multiple functions  python package    trimmomatic  fastq quality trim and adaptor removal  link     This is a list of software already available on the cluster and the command you need to execute to load it in your environment:      software  description  load it on the cluster      Samtools   module load samtools    Bedtools   module load bedtools2./2.25.0    bowtie2  alignment software  module load bowtie2    tophat2  alignment software  module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0    R  language for statistical analysis  module load intel/17.0.4 R-Project/3.4.1     This is a list of other software you might find useful:     software  description  link      GSEA  genome set enrichment analysis  link    IGV  Interactive Genome Viewer  link    Cytoscape  Network visualization softwar  link",
            "title": "Genomic Software"
        },
        {
            "location": "/workshops/workshop/#setup",
            "text": "",
            "title": "Setup"
        },
        {
            "location": "/workshops/workshop/#connect-to-the-cluster-login-node",
            "text": "The preferred method to connect to the cluster is through a web browser and fastX client\n -  via FastX : in your browser, go to `https://amarel.hpc.rutgers.edu:3443\nIf the above method doesn't work you may still connect    via a terminal : if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as  mobaXterm  [link] (https://mobaxterm.mobatek.net/). Then from your terminal connect to the cluster by executing the following command:   ssh -X <your net id>@amarel.hpc.rutgers.edu",
            "title": "Connect to the cluster login node"
        },
        {
            "location": "/workshops/workshop/#get-resources-on-the-compute-node",
            "text": "You get to the cluster to execute your computations by running the following command in your terminal:    srun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash   \nNotice that the name in your terminal will change from  amarel  to node name like  hal0025  or  slepner086 . This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command:      command part  meaning      srun  slurm  run, i.e. allocate resources and run via  slurm  scheduler    -p main  on the main partition, one of several queues on the cluster    --reservation=genomics  we reserved some compute nodes for this workshop to not wait long for resources    -N 1  ask for one node    -c 2  ask for two cores    -n 1  this will be 1 most times    -t 01:40:00  run this for a maximum time of 1 hour 40 minutes    --pty /bin/bash  run the terminal shell in an interactive mode",
            "title": "Get resources on the compute node"
        },
        {
            "location": "/workshops/workshop/#prepare-some-directories-for-the-data",
            "text": "You have two main spaces on the Amarel cluster. These are:    your home directory -  /home/netid/   (e.g.  /home/kp807/  for my netid) -  your scratch directory -  /scratch/netid/   They differ in how often they are backed up and by size (100Gb for  /home  and 500Gb for  /scratch ). So we will install programs in  /home , while the data and output will be in  /scratch . Execute these commands:                   cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated",
            "title": "Prepare some directories for the data"
        },
        {
            "location": "/workshops/workshop/#install-programs",
            "text": "Each program will have slightly different installation instructions. Here is a handy sequence of commands that will install them:   ##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                      Moreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e.  samtools  command will work if you execute  module load samtools ):   module load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1",
            "title": "Install programs"
        },
        {
            "location": "/workshops/workshop/#download-data",
            "text": "We will download human RNA-seq data with  GEO accession GSE52778 . The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated  We will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored.  sratoolkit  will be in your home directory, under  Programs , and the  vdb-config  might be under the  bin  directory. You will enter  /scratch/your_netID/Genomics_Workshop/download  for the path - NOTE you have to replace  your_netID  with your true netId, e.g.  kp807 . Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them.                   vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y  Then execute the following commands to get the data. Both  prefetch  and  fastq-dump  are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]  prefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ????   You have to pay attention to where you are putting your data. So these two commands will actually be several:                   cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516  The commands above showed how to do it for one sample. You need to do it for 6 samples total.                   SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)",
            "title": "Download data"
        },
        {
            "location": "/workshops/workshop/#running-bioinformatics-jobs",
            "text": "",
            "title": "Running bioinformatics jobs"
        },
        {
            "location": "/workshops/workshop/#fastqc-raw-data-qc",
            "text": "Explain what is fastqc is doing here - TODO          cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq  FastQC produces an html page as output,  fastqc/SRR1039508_1_fastqc.html , with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf",
            "title": "FastQC - raw data QC"
        },
        {
            "location": "/workshops/workshop/#trimmomatic-quality-trimadaptor-removal",
            "text": "##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35  NOTE:   the above is a one line command, illustrated as the following:          java -jar trimmomatic-0.36.jar PE \\\n        -phred33 -trimlog trim.log \\\n        input_1.fq  input_2.fq \\\n        output_1_paired.fq  output_1_unpaired.fq \\\n        output_2_paired.fq  output_2_unpaired.fq \\\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 / \n\n        ## Once it started run, you shall see the following:\n        TrimmomaticPE: Started with arguments:\n        -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n        Multiple cores found: Using 2 threads\n        Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n        ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n        Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%)\n        TrimmomaticPE: Completed successfully\n\n        ##view the output, the trim.log file, .e.g.  length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained)\n\n        ##you may also try fastx_quality_stats from the FASTX\u2014toolkit",
            "title": "Trimmomatic - quality trim/adaptor removal"
        },
        {
            "location": "/workshops/workshop/#fastqc-run-on-cleaned-reads-compare-result",
            "text": "module load java\n     fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n\n     ## /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf , helpful in viewing and interpreting the output",
            "title": "FastQC - Run on cleaned reads, compare result"
        },
        {
            "location": "/workshops/workshop/#download-reference-and-reference-indexing",
            "text": "Human genome indexing will take hours, we have the reference pre-prepared. Stored at   /projects/oarc/Genomics_Workshop/Reference/ \nFor in class practice, we will do this on E.coli genome          cd /scratch/$USER/Genomics_Workshop/\n        mkdir Reference\n        cd Reference\n\n        wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz\n\n        gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz\n        module load bowtie2\n        bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic\n\n        ##if download from ENSEMBLE\n        wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-38/fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/dna/Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.dna.toplevel.fa.gz",
            "title": "Download reference and reference indexing"
        },
        {
            "location": "/workshops/workshop/#mapping-with-tophat2-star-hisat2",
            "text": "Now, go to your data folder\n         cd  /scratch/$USER/Genomics_Workshop/untreated          cd  /scratch/$USER/Genomics_Workshop/untreated\n        module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n        module load samtools   #bowtie2 is loaded already\n        mkdir tophat_out\n        tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039508_1\n        0k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/  hg20_transciptome/GR\n        Ch38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel\n        SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n        ## you shall modify the -p value to be consistent with the -c value you requested in the beginning  You shall see something like:     [2018-03-30 11:48:57] Beginning TopHat run (v2.1.0)\n-----------------------------------------------\n[2018-03-30 11:48:57] Checking for Bowtie\n                  Bowtie version:        2.2.9.0\n[2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome)..\n[2018-03-30 11:48:58] Checking for Bowtie index files (genome)..\n[2018-03-30 11:48:58] Checking for reference FASTA file\n[2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen\nce/hg20/Homo_sapiens.GRCh38.dna.toplevel\n[2018-03-30 11:49:09] Reading known junctions from GTF file\n[2018-03-30 11:49:27] Preparing reads\n         left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded)\n        right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded)\n[2018-03-30 11:49:29] Using pre-built transcriptome data..\n[2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads\n[2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t\noplevel with Bowtie2\n[2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n[2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2\n[2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.  The transcriptome index was built  by pointing to gtf file first,  here we have it prepared already, just so we can save time.  The following would be the command to generate the transcriptome index while running tophat alignment.     tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039516 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/ Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039516_1.fastq.pairedOut.fastq SRR1039516_2.fastq.pairedOut.fastq  The output folder  tophat_out/untreated_SRR1039508/  shall contain the following files/folder (in blue):        cd  /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508\n        $ ll\n   total 2183632\n -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam\n -rw-rw-r-- 1 yc759 oarc        565 Jan 16 21:57 align_summary.txt\n -rw-rw-r-- 1 yc759 oarc    1921529 Jan 16 21:57 deletions.bed\n -rw-rw-r-- 1 yc759 oarc    2239884 Jan 16 21:57 insertions.bed\n -rw-rw-r-- 1 yc759 oarc   14181618 Jan 16 21:57 junctions.bed\n drwxrwsr-x 2 yc759 oarc       4096 Jan 16 21:57 logs\n -rw-rw-r-- 1 yc759 oarc        184 Jan 16 21:57 prep_reads.info\n -rw-rw-r-- 1 yc759 oarc   42846571 Jan 16 21:57 unmapped.bam",
            "title": "Mapping with tophat2, (STAR, HISAT2)"
        },
        {
            "location": "/workshops/workshop/#read-counts-using-htseq-count",
            "text": "GO TO WHERE YOUR ALIGNMENT OUTPUT FOLDER IS, FOR EXAMPLE:        cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n     ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/ untreated_SRR1039508/accepted_hits.bam accepted_hits.bam   \n\n     ##make a soft link to the full bam file we already prepared, if you didn\u2019t have the bam ready yet\n\n     module load samtools intel/17.0.2 python/2.7.12 \n     samtools sort -n  accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name  -/projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf > untreated08.txt  Use samtools to sort the bam file by name:  because htseq-count accepts bam file sorted by name as default, but tophat generates bam sorted by coordinates by default\nThe same way to generate the counts file  untreated12.txt ,  untreated16.txt , dex09.txt ,  dex13.txt ,  dex17.txt",
            "title": "Read counts using htseq-count"
        },
        {
            "location": "/workshops/workshop/#perform-mapping-qc-using-rseqc",
            "text": "Now,  quality control using RSeQC \u2013a few examples here, please go to the website for more functions  http://rseqc.sourceforge.net/  cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \nmodule load python/2.7.12\nmodule load intel/17.0.4\n\nread_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/\nReference/Homo_sapiens.GRCh38.79.bed\nprocessing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n        processing accepted_hits.bam ... Finished\n\n        Total Reads                   43474036\n        Total Tags                    54438789\n        Total Assigned Tags           53991382\n        =====================================================================\n        Group               Total_bases         Tag_count           Tags/Kb\n        CDS_Exons           103371993           43264842            418.54\n        5'UTR_Exons         5217678             583447              111.82\n        3'UTR_Exons         29324747            8145122             277.76\n        Introns             1500197093          1805034             1.20\n        TSS_up_1kb          33306654            18893               0.57\n        TSS_up_5kb          148463534           41165               0.28\n        TSS_up_10kb         265823549           55644               0.21\n        TES_down_1kb        35215293            50954               1.45\n        TES_down_5kb        152556214           113325              0.74\n        TES_down_10kb       268614580           137293              0.51\n        =====================================================================\n\nbam_stat.py -i accepted_hits.bam\n        Load BAM file ...\n\n        Done\n\n        #==================================================\n        #All numbers are READ count\n        #==================================================\n\n        Total records:                          52528699\n\n        QC failed:                              0\n        Optical/PCR duplicate:                  0\n        Non primary hits                        9054663\n        Unmapped reads:                         0\n        mapq < mapq_cut (non-unique):           2684801\n\n        mapq >= mapq_cut (unique):              40789235\n        Read-1:                                 20414530\n        Read-2:                                 20374705\n        Reads map to '+':                       20393901\n        Reads map to '-':                       20395334\n        Non-splice reads:                       30860931\n        Splice reads:                           9928304\n        Reads mapped in proper pairs:           32386536\n        Proper-paired reads map to different chrom:312  The script does genebody coverage calculation requires the input bam files to be sorted and indexed (we will do it using samtools). The calculation and plot will require R\nGo to one of the tophat_out sample folder  module load intel/17.0.4  R-Project/3.4.1    \nmodule load samtools\nsamtools sort accepted_hits.bam \u2013o accepted_hits.sorted.bam \n##this may take a while, you may use the one already prepared for you by making a soft link\n\nln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam\n\n\nsamtools index accepted_hits.sorted.bam\n\ngeneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test\n@ 2018-01-14 13:17:33: Read BED file (reference gene model) ...\n@ 2018-01-14 13:17:33: Total 3802 transcripts loaded\n@ 2018-01-14 13:17:33: Get BAM file(s) ...\n        accepted_hits.sorted.bam\n@ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ...\n        3800 transcripts finished\n\n\n        Sample  Skewness\n        accepted_hits.sorted    -3.61577607436\n@ 2018-01-14 13:28:59: Running R script ...\nnull device\n          1  output files:        test.geneBodyCoverage.r\n                                test.geneBodyCoverage.txt\n                log.txt\n                test.geneBodyCoverage.curves.pdf  download the gene.bed files :   https://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/ \nBe careful that the genome version, be consistent between reference genome used in mapping and now. For now, you may use what\u2019s provided   if you want to find out whether the sequencing read is strand specific or not, do:          $ infer_experiment.py -r  /projects/oarc/Genomics_Workshop/Reference/ Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam\n\n    Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n    Loading SAM/BAM file ...  Total 200000 usable reads were sampled\n\n\nThis is PairEnd Data\nFraction of reads failed to determine: 0.1406\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292",
            "title": "Perform Mapping QC using RSeQC"
        },
        {
            "location": "/workshops/Labs/",
            "text": "LAB 1: Visualization using IGV\n\n\n\n\nIGV\n  --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis\n The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command\n\n\n  cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508\n  module load samtools                                                             \n  samtools sort accepted_hits.bam -o accepted_hits.sorted.bam  ##this step  takes about 10 minutes to complete \n  samtools index accepted_hits.sorted.bam ## it takes about 30 seconds\n\n\n\n\nThe resulting files: accepted_hits.sorted.bam  \n\n                     accepted_hits.sorted.bam.bai\n\n                     are the files to be uploaded to IGV\n\n\nYou need to repeat these steps for every sample \n\n\nShortcut Lab 1\n\n\nWe have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at: \n/projects/oarc/Genomics_Workshop/Bam_for_IGV/\n . Make a soft link (see the following command), or copy them into your scratch  folder, then we use IGV to analyze them. \n\n\ncd /scratch/$USER/Genomics_Workshop/\nln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV  Bam_for_IGV  ## this step was  done when you ran lab_PartII.sh\n#start IGV \nmodule load java\n/projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh \n\n\n\n\nPractice and get familiar with:  \n\n\n\n\nHow to Load genome and data track\n\n\nHow to navigate\n\n\nHow and what to visualize:\n\n\nExamine coverage\n\n\nLow mapping quality\n\n\nMis-alignment\n\n\nTranslocation\n\n\nNovel genes/transcript\n\n\nAlternative splicing\n\n\nInversion\n\n\nLook for SNPs\n\n\nCNV, ChipSeq, RNASeq, WGS alignmentSNP\n\n\n\n\nMore detailed explanation \nhere\n\n\nThe following is a \nsample\n snap shot of the above two files loaded to IGV.\n\n  CLOSE your interactive session on a node  when done with IGV by typing exit\n\n\nLAB 2:  Data processing and expression analysis using edgeR\n\n\n\n\nAll the htseq-count output files should be present in one folder. Here we created the folder read_counts.\n\n\n            read_counts/dex09.txt\n                        dex13.txt\n                        dex17.txt\n                        untreated08.txt\n                        untreated12.txt\n                        untreated16.txt\n\n\n\n\nGo to this folder containing all above six counts output files\n\n\ncd /scratch/$USER/Genomics_Workshop/read_counts\n\n\nWe need to combine all counts into one file, which will be imported into R for further analysis.\n The following shows how this can be done with bash commands on linux ( you may do it in excel too)\n\n\n paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt > merged_counts.txt    \n ## merge files by columns\n cut -f1,2,4,6,8,10,13 merged_counts.txt  > merged_counts_clean.txt     ## extract the relevant columns  \n head -n -5 merged_counts_clean.txt > merged_counts_clean2.txt      ##remove the last 5 line stats summary\n\n\n\n\nWe also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file. You may construct in excel. \n \nThe file should contain the following info\n\n\n\n\n\n\n\n\nlabel\n\n\nsample\n\n\ngroup\n\n\ntreatment\n\n\n\n\n\n\n\n\n\n\n1\n\n\ndex09\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n2\n\n\ndex13\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n3\n\n\ndex17\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n4\n\n\nuntreated08\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n5\n\n\nuntreated12\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n6\n\n\nuntreated16\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n\n\nThis file has been prepared for you. You will need to copy it into your folder, see later Lab2.2:\n\n\nLab2.2: Pre-processing the data in R\n\n\ncd /scratch/$USER/Genomics_Workshop/\nmkdir  DE_analysis  ##set working directory to run differential expression analysis\ncd DE_analysis \n\n## Copy the needed files here\n\ncp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD                           ##This file denotes the experimental group\ncp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD              ##This file contains read counts on genes for all samples\ncp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD     ##annotation file needed to calculate exonic gene length -- needed for FPKM calculation\n\n\n\n\nHOMEWORK CATCH-UP\n\n\nFrom your homework assignment you should have the following packages to be installed already :).\nIf you didn't, install them now.\n\n\n##Open a terminal for  amarel2 login node.\nssh -X amarel2.hpc.rutgers.edu\n\n##On the login node start R\nmodule load intel/17.0.4\nmodule load R-Project/3.4.1\nR\n\n##then in the R workspace do the following:\nsource(\"https://bioconductor.org/biocLite.R\")\nbiocLite(\"MKmisc\")\nWould you like to use a personal library instead?  (y/n)  y\nWould you like to create a personal library\n~/R/x86_64-pc-linux-gnu-library/3.4 to install packages into?  (y/n) y\n\n##Wait till it finishes.\n\nbiocLite(\"Heatplus\")\nbiocLite(\"affycoretools\")\nbiocLite(\"flashClust\")\nbiocLite(\"affy\")\nbiocLite(\"GenomicFeatures\")\nquit()   ###quit R, no save\nSave workspace image? [y/n/c]: n\n\n\n\n\nStarting the Job\n\n\nNow,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running\n\n\nsrun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i\n   \n\nGo to your working directory\n\n\ncd /scratch/$USER/Genomics_Workshop/DE_analysis/\n\n Then, start R\n\n\nmodule load intel/17.0.4\n module load R-Project/3.4.1\n\n\ngetwd()\n Check which directory you are in\n\n You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory :\n \nsetwd(\"/scratch/<your_netID>/Genomics_Workshop/DE_analysis/\")\n. Set the working directory\n\n\nNow load up the libraries needed for the analysis\n\n\n    library(edgeR)\n    library(MKmisc)\n    library(affy)\n    library(flashClust)\n    library(affycoretools)\n    library(Heatplus)\n    library(GenomicFeatures)\n\n\n\nraw.data <- read.delim(\"merged_counts_clean2.txt\", header=F, row.names=1)  \n##import count data to R\nhead(raw.data)  ## check the beginning portion of the imported file, now an object\ndim(raw.data)   ##check the dimention of this object\nclass(raw.data)  ## check what class of this object\napply(raw.data, 2, summary) ## check the range of counts per sample\nrange(rowSums(raw.data))      ## check the range of counts per gene\ncolnames(raw.data) <- c(\"dex09\",\"dex13\",\"dex17\",\"untreated08\",\"untreated12\",\"untreated16\")\n##add column header\n\nraw.data2 <- raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples\ndim(raw.data2)\n\ncpm.values <- cpm(raw.data2) #calculate counts per million mapped reads without any other normalization\n\nabove1cpm <- apply(cpm.values, 1, function(x) sum(x >=1)) ##How many samples/genes had at least 1 cpm \ncounts.use <- raw.data2[above1cpm >= 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group)\ndim(counts.use)\ncolSums(counts.use) / colSums(raw.data)  ##the % of total counts kept after filtering\nnrow(counts.use) / nrow(raw.data)  ##the % of genes kept after filtering\n\ntargets <- readTargets()      ##import targets file that contains experiment group info.\ntargets$GpF <- factor(targets$group)   ##change character to factor\ntargets$GpN <- as.numeric(targets$GpF) ##change factor to numeric (optional)\n\nls()  #see that objects have been loaded\nsave.image(\"RNASeqDemo.RData\")   ##save the work workspace\nsavehistory(\"RNASeqDemo.Rhistory\")  ##save command history\n\n\n\n\n\nLab 2.3: To calculate expression values as fpkm\n\n\nFirst, compute the gene length as described in Lab6--partI\n\n\n library(GenomicFeatures)  ##may skip, because we already loaded at start\n gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n class(exonic.gene.sizes)\n Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n colnames(Hg20_geneLength) <- paste('geneLength')    \n\n Hg20_geneLength2 <- data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain \n the same number genes in the same order as in the filtered read counts file\n colnames(Hg20_geneLength2) <- paste('geneLength')  ## to change column name, make    it neat\n fpkm.data <- cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm\n min.count <- apply(fpkm.data,1,min)\n write.csv(fpkm.data,file=\"fpkm_values.csv\")  #### To output FPKM data  \n\n\n\n\n\nLab 2.4:  Analysis QC ---or sample diagnosis\n\n\n## density distribution\nplotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2)\nlegend(\"topright\", legend=levels(targets$GpF),fill=1:4)\n\n\n\n\nChange data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization\n\n\nClustering\n\n\nhc.raw <- flashClust(dist(t(log2(raw.data2+0.1))),method=\"average\")\nplot(hc.raw,hang = -1, main = \"RNASeqDemo, raw values\", sub = \"\", xlab = \"\",cex=0.9, labels=targets$sample)\n##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization\n\n#####PCA#######\nplotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on FPKM\")\n\n###do it after edgeR analysis, otherwise some values not existing yet###\n\nplotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on logCPM\")\n\n\n#####heatmap####\ntest <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"PValue\")$table ####sort gene list according to P values\ntest2 <- test[1:500,]  ###Take the top 500 significant genes\nlogCPM2 <- logCPM[rownames(test2),]  ###Extract logCPM values of these 500 genes\nmeanFC <- logCPM2 - rowMeans(logCPM2)\ncolor.meanFC <- heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c(\"blue\",\"white\",\"red\"))(128))\nheatmap_2(meanFC, col=color.meanFC, legend=3, scale=\"none\")\n\n\n\n #####Volcano plot####\nwith(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2),ylim=c(0,25)))\nwith(subset(eR.dex_Ctl.detailed,FDR<0.05), points(logFC, -log10(PValue), pch=20, col=\"red\"))\nwith(subset(eR.dex_Ctl.detailed, abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"orange\"))\nwith(subset(eR.dex_Ctl.detailed, FDR<0.05&abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"green\"))\n\n\n\n\n\nLab 2.5: Differential expression analysis  with edgeR\n\n\nCreate the design matrix\n\n\n  groups <- factor(targets$group)\n  design <- model.matrix(~0+groups)\n  colnames(design) <- levels(groups)\n  rownames(design) <- targets$sample   \n  design\n\n\n\n\n                                        control dex\ndex09             0   1\ndex13             0   1\ndex17             0   1\nuntreated08       1   0\nuntreated12       1   0\nuntreated16       1   0\nattr(,\"assign\")\n[1] 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$groups\n[1] \"contr.treatment\"\n\n\n\n\nCreate contrast matrix\n\n\ncont.matrix <- makeContrasts(dex_Ctl= dex - control, levels=design)\n\n\nCreate DGEList object\n\n\n d <- DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF)\n class(d)\n names(d)    ## the names of the items in the list\n d$counts[1:5,]    ## The counts are stored in the $counts:\n d$samples   ## The group info and library sizes stored in $samples\n d <- calcNormFactors(d)  ## an additional normalization factor using a TMM method\n d$samples   ## now the norm.factors are no longer 1\n\n ###this can be useful when diagnose problem)###\n apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100\n ##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. \n\n logCPM <-  cpm(d$counts, log = TRUE)   ## modified logCPM values in edgeR, can be used for clustering, heatmap\n\n\n\n\n\nNow,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among \nreplicates.\n\n\n  d <- estimateGLMCommonDisp(d, design, verbose=TRUE)   \n  Disp = 0.06037 , BCV = 0.2457 \n  d <- estimateGLMTrendedDisp(d, design)\n  d <- estimateGLMTagwiseDisp(d, design)\n  plotBCV(d)   ## the relationship between the overall abundance and the tagwise dispersion estimates\n\n\n  fit.edgeR <- glmFit(d, design)  ## Estimate model coefficients from count data and design matrix\n  names(fit.edgeR)\n\n\n\n\nSpecify contrasts of interest, do empirical Bayes \"shrinkage\" of  variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models)\n\n\n eR.dex_Ctl <- glmLRT(fit.edgeR, contrast=cont.matrix[,1])\n\n summary(decideTestsDGE(eR.dex_Ctl)) [,1]  ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS)\n\n eR.dex_Ctl.detailed <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"none\")$table ## Get detailed output for a single contrast\n\n eR.dex_Ctl.detailed[1:5,]\n                      logFC   logCPM         LR    PValue       FDR\n ENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740\n ENSG00000000419  0.18626588 4.674100 0.79016266 0.3740509 0.8571700\n ENSG00000000457  0.04304803 3.838702 0.03317218 0.8554789 0.9905954\n ENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274\n ENSG00000000971  0.35401120 8.007586 1.55551391 0.2123233 0.7070242\n\n ## Fold change is simply group A-B (if on the log scale), or A/B (if on   raw scale).\n ## logCPM = the average log2-counts-per-million \n ## LR = likelihood ratio statistics \n ##PValue = the two-sided p-value \n ## FDR = adjusted p-value \n\n #### To back-translate logFC to regular FC with down-reg as -FC\n eR.dex_Ctl.detailed$FC <- 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC)\n\n write.csv(eR.dex_Ctl.detailed,file=\"Demo_eR.dex_Ctl_results.csv\")  ##output the file\n\n\n\n\n\nLab 3  Running enrichment analysis using GSEA\n\n\n\n\nThe command to start the gsea:\n\n\nsrun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i  ##get onto a reserved compute node\nmodule load java\njava -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar      \n\n\n\n\n\nPrepare files required to run GSEA\n\nFor detailed file format, see \nhere\n\n\n\n\n\n\n\n\nFiles\n\n\nFormat\n\n\n\n\n\n\n\n\n\n\nExpression data files\n\n\nGene cluster text file format (.gct)\n\n\n\n\n\n\nGene set files\n\n\ngene matrix file format(.gmx), gene matrix transposed format(.gmt)\n\n\n\n\n\n\nPhenotype data files\n\n\nCategorical class file (.cls) (defining experimental group)\n\n\n\n\n\n\n\n\nA set of following sample files are prepared for GSEA analysis practice, which are located at  \n/projects/oarc/Genomics_Workshop/GSEA/\n\n\nfpkm_values.ready.gct    gct file (expression fpkm values)\nfpkm_values.ready.cls     cls file  (defining experimental group)\nMouse_Human_NCI_Nature_November_01_2017_symbol.gmt     gmt file (gene set file biological function set)\n\n\n\n\n\nIn practice analysis, use online broad C2 geneset instead of the above .gmt file\n\n Results are located at \n/home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/\n  \n\n\nTo view your result:\n\n\n  cd ~/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/  \n  ## (same as: cd /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnn/)\n  firefox index.html\n\n\n\n\nAdditional gene set database downloading source:\n\n\n\n\nhttp://software.broadinstitute.org/gsea/msigdb/index.jsp\n\n\nhttp://download.baderlab.org/EM_Genesets/\n\n\nhttp://www.go2msig.org/cgi-bin/prebuilt.cgi\n\n\nor build your \nown\n\n\n\n\nLab 4 Running the GO term analysis\n\n\n\n\nOpen \nthis\n link\n\nGene list from de-analysis of our downloaded data (selected based on FDR and FC):\n\n\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927\nENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B \nENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904\nENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244\nENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218\nENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513\nENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498\nENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175\nENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924\nENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139\nENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812\nENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813\nENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383\nENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869\nENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626\nENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591\nENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386\nENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442\nENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490\nENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571\nENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675\nENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793\nENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437\nENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171\nENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260\nENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647\nENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265\nENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878\nENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431\nENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959\nENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493\nENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772\nENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675\nENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930\nENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569\nENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244\nENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738\nENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448\nENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697\nENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214\nENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194\nENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637\nENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715\nENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138\nENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218\nENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950\nENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672\nENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829\nENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741\nENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661\nENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368\nENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480\nENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072\nENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899\nENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928\nENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798\nENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614\nENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869\nENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251\nENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017\nENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907\nENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381\nENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364\nENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549\nENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718\nENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554\nENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401\nENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167\nENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390\nENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285\n\nFDR<0.05, FC < -2.5             FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000164761         ENSG00000177283 ENSG00000268913\nENSG00000149633         ENSG00000230018 ENSG00000103196\nENSG00000012048         ENSG00000261468 ENSG00000162630\nENSG00000126860         ENSG00000197301 ENSG00000247311\nENSG00000092621         ENSG00000154734 ENSG00000046653\nENSG00000154263         ENSG00000169271 ENSG00000167641\nENSG00000079462         ENSG00000124440 ENSG00000135821\nENSG00000182580         ENSG00000099998 ENSG00000136237\nENSG00000167771         ENSG00000120162 ENSG00000099337\nENSG00000205208         ENSG00000126803 ENSG00000120129\nENSG00000172986         ENSG00000068831 ENSG00000004799\nENSG00000272341         ENSG00000123685 ENSG00000221866\n                ENSG00000128045 ENSG00000157150\n                ENSG00000101342 ENSG00000102760\n                ENSG00000096060 ENSG00000198624\n                ENSG00000128917 ENSG00000179094\n                ENSG00000163083 ENSG00000179300\n                ENSG00000173838 ENSG00000136383\n                ENSG00000143127 ENSG00000189221\n                ENSG00000163884 ENSG00000174697\n                ENSG00000168309 ENSG00000112936\n                ENSG00000152583 ENSG00000165995\n                ENSG00000127954 ENSG00000157514\n                ENSG00000250978 ENSG00000233117\n                ENSG00000109906 ENSG00000157152\n                ENSG00000179593 ENSG00000187193\n                ENSG00000101347 ENSG00000152779\n                ENSG00000211445 ENSG00000170214\n\n\n\n\nLab 5 ID mapping and conversion\n\n\n\n\nLearn about gene identifiers, g:profiler, Synergizer and BioMart\n\n\nUse the above gene list:  \n\n1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler \n  Explore more functions, what the site can do for you \n\n2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart\n\n3. Do it again with your own gene list",
            "title": "Labs"
        },
        {
            "location": "/workshops/Labs/#lab-1-visualization-using-igv",
            "text": "IGV   --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis\n The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command    cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508\n  module load samtools                                                             \n  samtools sort accepted_hits.bam -o accepted_hits.sorted.bam  ##this step  takes about 10 minutes to complete \n  samtools index accepted_hits.sorted.bam ## it takes about 30 seconds  The resulting files: accepted_hits.sorted.bam   \n                     accepted_hits.sorted.bam.bai \n                     are the files to be uploaded to IGV  You need to repeat these steps for every sample",
            "title": "LAB 1: Visualization using IGV"
        },
        {
            "location": "/workshops/Labs/#shortcut-lab-1",
            "text": "We have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at:  /projects/oarc/Genomics_Workshop/Bam_for_IGV/  . Make a soft link (see the following command), or copy them into your scratch  folder, then we use IGV to analyze them.   cd /scratch/$USER/Genomics_Workshop/\nln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV  Bam_for_IGV  ## this step was  done when you ran lab_PartII.sh\n#start IGV \nmodule load java\n/projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh   Practice and get familiar with:     How to Load genome and data track  How to navigate  How and what to visualize:  Examine coverage  Low mapping quality  Mis-alignment  Translocation  Novel genes/transcript  Alternative splicing  Inversion  Look for SNPs  CNV, ChipSeq, RNASeq, WGS alignmentSNP   More detailed explanation  here  The following is a  sample  snap shot of the above two files loaded to IGV. \n  CLOSE your interactive session on a node  when done with IGV by typing exit",
            "title": "Shortcut Lab 1"
        },
        {
            "location": "/workshops/Labs/#lab-2-data-processing-and-expression-analysis-using-edger",
            "text": "All the htseq-count output files should be present in one folder. Here we created the folder read_counts.              read_counts/dex09.txt\n                        dex13.txt\n                        dex17.txt\n                        untreated08.txt\n                        untreated12.txt\n                        untreated16.txt  Go to this folder containing all above six counts output files  cd /scratch/$USER/Genomics_Workshop/read_counts  We need to combine all counts into one file, which will be imported into R for further analysis.\n The following shows how this can be done with bash commands on linux ( you may do it in excel too)   paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt > merged_counts.txt    \n ## merge files by columns\n cut -f1,2,4,6,8,10,13 merged_counts.txt  > merged_counts_clean.txt     ## extract the relevant columns  \n head -n -5 merged_counts_clean.txt > merged_counts_clean2.txt      ##remove the last 5 line stats summary  We also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file. You may construct in excel. \n  The file should contain the following info     label  sample  group  treatment      1  dex09  dex  dex_treated    2  dex13  dex  dex_treated    3  dex17  dex  dex_treated    4  untreated08   control untreated    5  untreated12   control untreated    6  untreated16   control untreated     This file has been prepared for you. You will need to copy it into your folder, see later Lab2.2:",
            "title": "LAB 2:  Data processing and expression analysis using edgeR"
        },
        {
            "location": "/workshops/Labs/#lab22-pre-processing-the-data-in-r",
            "text": "cd /scratch/$USER/Genomics_Workshop/\nmkdir  DE_analysis  ##set working directory to run differential expression analysis\ncd DE_analysis \n\n## Copy the needed files here\n\ncp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD                           ##This file denotes the experimental group\ncp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD              ##This file contains read counts on genes for all samples\ncp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD     ##annotation file needed to calculate exonic gene length -- needed for FPKM calculation",
            "title": "Lab2.2: Pre-processing the data in R"
        },
        {
            "location": "/workshops/Labs/#homework-catch-up",
            "text": "From your homework assignment you should have the following packages to be installed already :).\nIf you didn't, install them now.  ##Open a terminal for  amarel2 login node.\nssh -X amarel2.hpc.rutgers.edu\n\n##On the login node start R\nmodule load intel/17.0.4\nmodule load R-Project/3.4.1\nR\n\n##then in the R workspace do the following:\nsource(\"https://bioconductor.org/biocLite.R\")\nbiocLite(\"MKmisc\")\nWould you like to use a personal library instead?  (y/n)  y\nWould you like to create a personal library\n~/R/x86_64-pc-linux-gnu-library/3.4 to install packages into?  (y/n) y\n\n##Wait till it finishes.\n\nbiocLite(\"Heatplus\")\nbiocLite(\"affycoretools\")\nbiocLite(\"flashClust\")\nbiocLite(\"affy\")\nbiocLite(\"GenomicFeatures\")\nquit()   ###quit R, no save\nSave workspace image? [y/n/c]: n",
            "title": "HOMEWORK CATCH-UP"
        },
        {
            "location": "/workshops/Labs/#starting-the-job",
            "text": "Now,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running  srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i     \nGo to your working directory  cd /scratch/$USER/Genomics_Workshop/DE_analysis/ \n Then, start R  module load intel/17.0.4\n module load R-Project/3.4.1  getwd()  Check which directory you are in \n You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory :\n  setwd(\"/scratch/<your_netID>/Genomics_Workshop/DE_analysis/\") . Set the working directory  Now load up the libraries needed for the analysis      library(edgeR)\n    library(MKmisc)\n    library(affy)\n    library(flashClust)\n    library(affycoretools)\n    library(Heatplus)\n    library(GenomicFeatures)  raw.data <- read.delim(\"merged_counts_clean2.txt\", header=F, row.names=1)  \n##import count data to R\nhead(raw.data)  ## check the beginning portion of the imported file, now an object\ndim(raw.data)   ##check the dimention of this object\nclass(raw.data)  ## check what class of this object\napply(raw.data, 2, summary) ## check the range of counts per sample\nrange(rowSums(raw.data))      ## check the range of counts per gene\ncolnames(raw.data) <- c(\"dex09\",\"dex13\",\"dex17\",\"untreated08\",\"untreated12\",\"untreated16\")\n##add column header\n\nraw.data2 <- raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples\ndim(raw.data2)\n\ncpm.values <- cpm(raw.data2) #calculate counts per million mapped reads without any other normalization\n\nabove1cpm <- apply(cpm.values, 1, function(x) sum(x >=1)) ##How many samples/genes had at least 1 cpm \ncounts.use <- raw.data2[above1cpm >= 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group)\ndim(counts.use)\ncolSums(counts.use) / colSums(raw.data)  ##the % of total counts kept after filtering\nnrow(counts.use) / nrow(raw.data)  ##the % of genes kept after filtering\n\ntargets <- readTargets()      ##import targets file that contains experiment group info.\ntargets$GpF <- factor(targets$group)   ##change character to factor\ntargets$GpN <- as.numeric(targets$GpF) ##change factor to numeric (optional)\n\nls()  #see that objects have been loaded\nsave.image(\"RNASeqDemo.RData\")   ##save the work workspace\nsavehistory(\"RNASeqDemo.Rhistory\")  ##save command history",
            "title": "Starting the Job"
        },
        {
            "location": "/workshops/Labs/#lab-23-to-calculate-expression-values-as-fpkm",
            "text": "First, compute the gene length as described in Lab6--partI   library(GenomicFeatures)  ##may skip, because we already loaded at start\n gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n class(exonic.gene.sizes)\n Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n colnames(Hg20_geneLength) <- paste('geneLength')    \n\n Hg20_geneLength2 <- data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain \n the same number genes in the same order as in the filtered read counts file\n colnames(Hg20_geneLength2) <- paste('geneLength')  ## to change column name, make    it neat\n fpkm.data <- cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm\n min.count <- apply(fpkm.data,1,min)\n write.csv(fpkm.data,file=\"fpkm_values.csv\")  #### To output FPKM data",
            "title": "Lab 2.3: To calculate expression values as fpkm"
        },
        {
            "location": "/workshops/Labs/#lab-24-analysis-qc-or-sample-diagnosis",
            "text": "## density distribution\nplotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2)\nlegend(\"topright\", legend=levels(targets$GpF),fill=1:4)  Change data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization",
            "title": "Lab 2.4:  Analysis QC ---or sample diagnosis"
        },
        {
            "location": "/workshops/Labs/#clustering",
            "text": "hc.raw <- flashClust(dist(t(log2(raw.data2+0.1))),method=\"average\")\nplot(hc.raw,hang = -1, main = \"RNASeqDemo, raw values\", sub = \"\", xlab = \"\",cex=0.9, labels=targets$sample)\n##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization\n\n#####PCA#######\nplotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on FPKM\")\n\n###do it after edgeR analysis, otherwise some values not existing yet###\n\nplotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on logCPM\")\n\n\n#####heatmap####\ntest <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"PValue\")$table ####sort gene list according to P values\ntest2 <- test[1:500,]  ###Take the top 500 significant genes\nlogCPM2 <- logCPM[rownames(test2),]  ###Extract logCPM values of these 500 genes\nmeanFC <- logCPM2 - rowMeans(logCPM2)\ncolor.meanFC <- heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c(\"blue\",\"white\",\"red\"))(128))\nheatmap_2(meanFC, col=color.meanFC, legend=3, scale=\"none\")\n\n\n\n #####Volcano plot####\nwith(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2),ylim=c(0,25)))\nwith(subset(eR.dex_Ctl.detailed,FDR<0.05), points(logFC, -log10(PValue), pch=20, col=\"red\"))\nwith(subset(eR.dex_Ctl.detailed, abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"orange\"))\nwith(subset(eR.dex_Ctl.detailed, FDR<0.05&abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"green\"))",
            "title": "Clustering"
        },
        {
            "location": "/workshops/Labs/#lab-25-differential-expression-analysis-with-edger",
            "text": "Create the design matrix    groups <- factor(targets$group)\n  design <- model.matrix(~0+groups)\n  colnames(design) <- levels(groups)\n  rownames(design) <- targets$sample   \n  design                                          control dex\ndex09             0   1\ndex13             0   1\ndex17             0   1\nuntreated08       1   0\nuntreated12       1   0\nuntreated16       1   0\nattr(,\"assign\")\n[1] 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$groups\n[1] \"contr.treatment\"",
            "title": "Lab 2.5: Differential expression analysis  with edgeR"
        },
        {
            "location": "/workshops/Labs/#create-contrast-matrix",
            "text": "cont.matrix <- makeContrasts(dex_Ctl= dex - control, levels=design)",
            "title": "Create contrast matrix"
        },
        {
            "location": "/workshops/Labs/#create-dgelist-object",
            "text": "d <- DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF)\n class(d)\n names(d)    ## the names of the items in the list\n d$counts[1:5,]    ## The counts are stored in the $counts:\n d$samples   ## The group info and library sizes stored in $samples\n d <- calcNormFactors(d)  ## an additional normalization factor using a TMM method\n d$samples   ## now the norm.factors are no longer 1\n\n ###this can be useful when diagnose problem)###\n apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100\n ##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. \n\n logCPM <-  cpm(d$counts, log = TRUE)   ## modified logCPM values in edgeR, can be used for clustering, heatmap  Now,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among \nreplicates.    d <- estimateGLMCommonDisp(d, design, verbose=TRUE)   \n  Disp = 0.06037 , BCV = 0.2457 \n  d <- estimateGLMTrendedDisp(d, design)\n  d <- estimateGLMTagwiseDisp(d, design)\n  plotBCV(d)   ## the relationship between the overall abundance and the tagwise dispersion estimates\n\n\n  fit.edgeR <- glmFit(d, design)  ## Estimate model coefficients from count data and design matrix\n  names(fit.edgeR)  Specify contrasts of interest, do empirical Bayes \"shrinkage\" of  variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models)   eR.dex_Ctl <- glmLRT(fit.edgeR, contrast=cont.matrix[,1])\n\n summary(decideTestsDGE(eR.dex_Ctl)) [,1]  ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS)\n\n eR.dex_Ctl.detailed <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"none\")$table ## Get detailed output for a single contrast\n\n eR.dex_Ctl.detailed[1:5,]\n                      logFC   logCPM         LR    PValue       FDR\n ENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740\n ENSG00000000419  0.18626588 4.674100 0.79016266 0.3740509 0.8571700\n ENSG00000000457  0.04304803 3.838702 0.03317218 0.8554789 0.9905954\n ENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274\n ENSG00000000971  0.35401120 8.007586 1.55551391 0.2123233 0.7070242\n\n ## Fold change is simply group A-B (if on the log scale), or A/B (if on   raw scale).\n ## logCPM = the average log2-counts-per-million \n ## LR = likelihood ratio statistics \n ##PValue = the two-sided p-value \n ## FDR = adjusted p-value \n\n #### To back-translate logFC to regular FC with down-reg as -FC\n eR.dex_Ctl.detailed$FC <- 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC)\n\n write.csv(eR.dex_Ctl.detailed,file=\"Demo_eR.dex_Ctl_results.csv\")  ##output the file",
            "title": "Create DGEList object"
        },
        {
            "location": "/workshops/Labs/#lab-3-running-enrichment-analysis-using-gsea",
            "text": "The command to start the gsea:  srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i  ##get onto a reserved compute node\nmodule load java\njava -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar        Prepare files required to run GSEA \nFor detailed file format, see  here     Files  Format      Expression data files  Gene cluster text file format (.gct)    Gene set files  gene matrix file format(.gmx), gene matrix transposed format(.gmt)    Phenotype data files  Categorical class file (.cls) (defining experimental group)     A set of following sample files are prepared for GSEA analysis practice, which are located at   /projects/oarc/Genomics_Workshop/GSEA/  fpkm_values.ready.gct    gct file (expression fpkm values)\nfpkm_values.ready.cls     cls file  (defining experimental group)\nMouse_Human_NCI_Nature_November_01_2017_symbol.gmt     gmt file (gene set file biological function set)  In practice analysis, use online broad C2 geneset instead of the above .gmt file \n Results are located at  /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/     To view your result:    cd ~/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/  \n  ## (same as: cd /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnn/)\n  firefox index.html",
            "title": "Lab 3  Running enrichment analysis using GSEA"
        },
        {
            "location": "/workshops/Labs/#additional-gene-set-database-downloading-source",
            "text": "http://software.broadinstitute.org/gsea/msigdb/index.jsp  http://download.baderlab.org/EM_Genesets/  http://www.go2msig.org/cgi-bin/prebuilt.cgi  or build your  own",
            "title": "Additional gene set database downloading source:"
        },
        {
            "location": "/workshops/Labs/#lab-4-running-the-go-term-analysis",
            "text": "Open  this  link \nGene list from de-analysis of our downloaded data (selected based on FDR and FC):  FDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927\nENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B \nENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904\nENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244\nENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218\nENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513\nENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498\nENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175\nENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924\nENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139\nENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812\nENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813\nENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383\nENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869\nENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626\nENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591\nENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386\nENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442\nENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490\nENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571\nENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675\nENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793\nENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437\nENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171\nENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260\nENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647\nENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265\nENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878\nENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431\nENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959\nENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493\nENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772\nENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675\nENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930\nENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569\nENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244\nENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738\nENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448\nENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697\nENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214\nENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194\nENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637\nENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715\nENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138\nENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218\nENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950\nENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672\nENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829\nENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741\nENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661\nENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368\nENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480\nENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072\nENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899\nENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928\nENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798\nENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614\nENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869\nENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251\nENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017\nENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907\nENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381\nENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364\nENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549\nENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718\nENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554\nENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401\nENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167\nENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390\nENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285\n\nFDR<0.05, FC < -2.5             FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000164761         ENSG00000177283 ENSG00000268913\nENSG00000149633         ENSG00000230018 ENSG00000103196\nENSG00000012048         ENSG00000261468 ENSG00000162630\nENSG00000126860         ENSG00000197301 ENSG00000247311\nENSG00000092621         ENSG00000154734 ENSG00000046653\nENSG00000154263         ENSG00000169271 ENSG00000167641\nENSG00000079462         ENSG00000124440 ENSG00000135821\nENSG00000182580         ENSG00000099998 ENSG00000136237\nENSG00000167771         ENSG00000120162 ENSG00000099337\nENSG00000205208         ENSG00000126803 ENSG00000120129\nENSG00000172986         ENSG00000068831 ENSG00000004799\nENSG00000272341         ENSG00000123685 ENSG00000221866\n                ENSG00000128045 ENSG00000157150\n                ENSG00000101342 ENSG00000102760\n                ENSG00000096060 ENSG00000198624\n                ENSG00000128917 ENSG00000179094\n                ENSG00000163083 ENSG00000179300\n                ENSG00000173838 ENSG00000136383\n                ENSG00000143127 ENSG00000189221\n                ENSG00000163884 ENSG00000174697\n                ENSG00000168309 ENSG00000112936\n                ENSG00000152583 ENSG00000165995\n                ENSG00000127954 ENSG00000157514\n                ENSG00000250978 ENSG00000233117\n                ENSG00000109906 ENSG00000157152\n                ENSG00000179593 ENSG00000187193\n                ENSG00000101347 ENSG00000152779\n                ENSG00000211445 ENSG00000170214",
            "title": "Lab 4 Running the GO term analysis"
        },
        {
            "location": "/workshops/Labs/#lab-5-id-mapping-and-conversion",
            "text": "Learn about gene identifiers, g:profiler, Synergizer and BioMart  Use the above gene list:   \n1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler \n  Explore more functions, what the site can do for you  \n2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart \n3. Do it again with your own gene list",
            "title": "Lab 5 ID mapping and conversion"
        },
        {
            "location": "/ressentials/",
            "text": "To start R shell on the cluster\n\n\nSee workshop for explanation of the various options of \nsrun\n\n\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R\n\n\n\n\nPackages used from BioConductor\n\n\nIf these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: \n\n\n        source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")\n\n\n\n\nExample: Calculate gene length\n\n\nGet some data from ENSEMBLE\n\n\nwget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz\n\n\nIn R shell, you can execute these commands to compute gene lengths: \n\n\n\n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')    \n\n\n\n\nSome R essentials\n\n\nArithmetic functions\n\n\n        2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)\n\n\n\n\nCreating objects\n\n\n        ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()\n\n\n\n\nVector and Matrix\n\n\n        x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x\n\n\n\n\nData Frames\n\n\n        z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C\n\n\n\n\nMore ways to subset dataframes\n\n\n        z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]\n\n\n\n\nLists\n\n\n        mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)\n\n\n\n\nFunctions\n\n\n        my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)\n\n\n\n\nVarious directory/file/library manipulations\n\n\n        library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")\n\n\n\n\nSave all the objects you have created to your workspace\n\n\n        save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file\n\n\n\n\nRemove objects from your workspace\n\n\n        ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()\n\n\n\n\nSave a history of all the commands entered\n\n\n        savehistory(\"introhistory.Rhistory\")",
            "title": "R tutorial"
        },
        {
            "location": "/ressentials/#to-start-r-shell-on-the-cluster",
            "text": "See workshop for explanation of the various options of  srun          module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R",
            "title": "To start R shell on the cluster"
        },
        {
            "location": "/ressentials/#packages-used-from-bioconductor",
            "text": "If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands:           source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")",
            "title": "Packages used from BioConductor"
        },
        {
            "location": "/ressentials/#example-calculate-gene-length",
            "text": "Get some data from ENSEMBLE  wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz  In R shell, you can execute these commands to compute gene lengths:   \n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')",
            "title": "Example: Calculate gene length"
        },
        {
            "location": "/ressentials/#some-r-essentials",
            "text": "",
            "title": "Some R essentials"
        },
        {
            "location": "/ressentials/#arithmetic-functions",
            "text": "2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)",
            "title": "Arithmetic functions"
        },
        {
            "location": "/ressentials/#creating-objects",
            "text": "ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()",
            "title": "Creating objects"
        },
        {
            "location": "/ressentials/#vector-and-matrix",
            "text": "x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x",
            "title": "Vector and Matrix"
        },
        {
            "location": "/ressentials/#data-frames",
            "text": "z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C",
            "title": "Data Frames"
        },
        {
            "location": "/ressentials/#more-ways-to-subset-dataframes",
            "text": "z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]",
            "title": "More ways to subset dataframes"
        },
        {
            "location": "/ressentials/#lists",
            "text": "mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)",
            "title": "Lists"
        },
        {
            "location": "/ressentials/#functions",
            "text": "my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)",
            "title": "Functions"
        },
        {
            "location": "/ressentials/#various-directoryfilelibrary-manipulations",
            "text": "library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")",
            "title": "Various directory/file/library manipulations"
        },
        {
            "location": "/ressentials/#save-all-the-objects-you-have-created-to-your-workspace",
            "text": "save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file",
            "title": "Save all the objects you have created to your workspace"
        },
        {
            "location": "/ressentials/#remove-objects-from-your-workspace",
            "text": "ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()",
            "title": "Remove objects from your workspace"
        },
        {
            "location": "/ressentials/#save-a-history-of-all-the-commands-entered",
            "text": "savehistory(\"introhistory.Rhistory\")",
            "title": "Save a history of all the commands entered"
        },
        {
            "location": "/resources/",
            "text": "Here are some resources you might want to consult to learn more about how to use an HPC cluster\n\n\nSlurm\n\n\n\n\nOARC cluster user guide Amarel/Perceval\n - this is a must-read for any new users, even if you are an experienced Linux user \n\n\nOARC cluster community\n - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it\n\n\nintro videos by Kristina\n\n\nterse slurm tips\n\n\n\n\nLinux tutorials\n\n\n\n\nLinux tutorial by Galen\n\n\nUNIX Tutorial for Beginners\n\n\nRutgers Linux course\n Rutgers course teaching Linux for engineering students, runs every semester\n\n\nCornell virtual workshop stuff\n - interactive if you have xsede login (?)\n\n\nSoftware Carpentry\n - Software Carpentry has a number of lectures and workshops on many computing subjects\n\n\n\n\nGraphical user interface\n\n\n\n\nweb-based access to the cluster (still testing) - only from campus or VPN\n\n\n\n\nResources on the web\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources"
        },
        {
            "location": "/resources/#slurm",
            "text": "OARC cluster user guide Amarel/Perceval  - this is a must-read for any new users, even if you are an experienced Linux user   OARC cluster community  - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it  intro videos by Kristina  terse slurm tips",
            "title": "Slurm"
        },
        {
            "location": "/resources/#linux-tutorials",
            "text": "Linux tutorial by Galen  UNIX Tutorial for Beginners  Rutgers Linux course  Rutgers course teaching Linux for engineering students, runs every semester  Cornell virtual workshop stuff  - interactive if you have xsede login (?)  Software Carpentry  - Software Carpentry has a number of lectures and workshops on many computing subjects",
            "title": "Linux tutorials"
        },
        {
            "location": "/resources/#graphical-user-interface",
            "text": "web-based access to the cluster (still testing) - only from campus or VPN",
            "title": "Graphical user interface"
        },
        {
            "location": "/resources/#resources-on-the-web",
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources on the web"
        },
        {
            "location": "/CheatSheet/",
            "text": "Navigating file tree\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nls\n\n\nlist files and directories\n\n\n\n\n\n\nls -a\n\n\nlist all files and directories\n\n\n\n\n\n\nmkdir\n\n\nmake a directory\n\n\n\n\n\n\ncd directory\n\n\nchange to named directory\n\n\n\n\n\n\ncd\n\n\nchange to home-directory\n\n\n\n\n\n\ncd ~\n\n\nchange to home-directory\n\n\n\n\n\n\ncd ..\n\n\nchange to parent directory\n\n\n\n\n\n\npwd\n\n\ndisplay the path of the current directory\n\n\n\n\n\n\nrm\n\n\nremoves files only\n\n\n\n\n\n\nrmdir\n\n\nremoves directory(make sure its empty)\n\n\n\n\n\n\n\n\nViewing files\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\ncp file1 file2\n\n\ncopy file1 and call it file2\n\n\n\n\n\n\nmv file1 file2\n\n\nmove or rename file1 to file2\n\n\n\n\n\n\nrm file\n\n\nremove a file\n\n\n\n\n\n\nrmdir directory\n\n\nremove a directory\n\n\n\n\n\n\ncat file\n\n\ndisplay a file\n\n\n\n\n\n\nless file\n\n\ndisplay a file a page at a time\n\n\n\n\n\n\nhead file\n\n\ndisplay the first few lines of a file\n\n\n\n\n\n\ntail file\n\n\ndisplay the last few lines of a file\n\n\n\n\n\n\ngrep 'keyword'\n\n\nfile   search a file for keywords\n\n\n\n\n\n\nwc file -l/w\n\n\ncount number of lines/words/characters in file\n\n\n\n\n\n\n\n\nUseful commands:\n\n\n\n\n\n\n\n\ncommand\n\n\ndescription\n\n\nusage example\n\n\n\n\n\n\n\n\n\n\nwhich \n\n\nsee where command is installed\n\n\nwhich python\n\n\n\n\n\n\npwd\n\n\nwhich directory I'm in\n\n\npwd\n\n\n\n\n\n\nman \n\n\nmanual page for command\n\n\nman cut\n\n\n\n\n\n\ngrep \n\n\nfilter for lines which fit pattern\n\n\ncat myfile | grep GATK\n\n\n\n\n\n\ncut -d\n -f\n\n\nsplit line by delimiter and get field number 3\n\n\ncat myfile | cut -d'_' -f3\n\n\n\n\n\n\nsort \n\n\nsort lines, often used with \nuniq\n\n\nsort myfile | uniq\n\n\n\n\n\n\nuniq\n\n\nsuppress repeated lines, works only if sorted\n\n\nsee above example\n\n\n\n\n\n\nless\n\n\npaginated output\n\n\nless myfile\n\n\n\n\n\n\n>\n\n\nredirect output (e.g. list files and save filenames in aaa.txt)\n\n\nls > aaa.txt\n\n\n\n\n\n\n>>\n\n\nappend output to existing file\n\n\necho \"blah\" >> aaa.txt\n\n\n\n\n\n\nfind\n\n\nfind files with some properties e.g. display all files recursively from current directory\n\n\nfind .\n\n\n\n\n\n\nchmod\n\n\nchange permissions on a file or directory\n\n\nchmod u+x myscript.sh\n\n\n\n\n\n\ntop\n\n\ndisplay most intensive processes\n\n\ntop\n\n\n\n\n\n\nps auxw\n\n\nlist processes\n\n\nps auxw\n\n\n\n\n\n\n\n\nRedirecting\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\ncommand > file\n\n\nredirect standard output to a file\n\n\n\n\n\n\ncommand >> file\n\n\nappend standard output to a file\n\n\n\n\n\n\ncommand < file\n\n\nredirect standard input from a file\n\n\n\n\n\n\ncommand1 | command2\n\n\npipe the output of command1 to the input of command2\n\n\n\n\n\n\ncat file1 file2 > file0\n\n\nconcatenate file1 and file2 to file0\n\n\n\n\n\n\nsort\n\n\nsort data\n\n\n\n\n\n\nwho\n\n\nlist users currently logged in\n\n\n\n\n\n\n*\n\n\nmatch any number of characters\n\n\n\n\n\n\n?\n\n\nmatch one character\n\n\n\n\n\n\nman command\n\n\nread the online manual page for a command\n\n\n\n\n\n\nwhatis command\n\n\nbrief description of a command\n\n\n\n\n\n\napropos keyword\n\n\nmatch commands with keyword in their man pages\n\n\n\n\n\n\n\n\nPermissions\n\n\n\n\n\n\n\n\nSymbol\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nu\n\n\nuser\n\n\n\n\n\n\ng\n\n\ngroup\n\n\n\n\n\n\no\n\n\nother\n\n\n\n\n\n\na\n\n\nall\n\n\n\n\n\n\nr\n\n\nread\n\n\n\n\n\n\nw\n\n\nwrite (and delete)\n\n\n\n\n\n\nx\n\n\nexecute (and access directory)\n\n\n\n\n\n\nu+x\n\n\nadd executing permission for user\n\n\n\n\n\n\nu-x\n\n\ntake away executing permission\n\n\n\n\n\n\n\n\nModules\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nmodule avail\n\n\nshow a list of the core (primary) modules available\n\n\n\n\n\n\nmodule load \nmodule_name\n\n\nloads the named software module\n\n\n\n\n\n\nmodule use \n/projects/community/projectsmodule use /projects/community/modulefiles/\n\n\nNow if you do module avail, it also shows modules uploaded by other users\n\n\n\n\n\n\nmodule spider\n\n\nshows a comprehensive list of all available modules or type name after spider to show details about specific module\n\n\n\n\n\n\nmodule purge\n\n\nremoves all loaded module\n\n\n\n\n\n\nml\n\n\nlists modules loaded",
            "title": "CheatSheet"
        },
        {
            "location": "/CheatSheet/#navigating-file-tree",
            "text": "Command  Meaning      ls  list files and directories    ls -a  list all files and directories    mkdir  make a directory    cd directory  change to named directory    cd  change to home-directory    cd ~  change to home-directory    cd ..  change to parent directory    pwd  display the path of the current directory    rm  removes files only    rmdir  removes directory(make sure its empty)",
            "title": "Navigating file tree"
        },
        {
            "location": "/CheatSheet/#viewing-files",
            "text": "Command  Meaning      cp file1 file2  copy file1 and call it file2    mv file1 file2  move or rename file1 to file2    rm file  remove a file    rmdir directory  remove a directory    cat file  display a file    less file  display a file a page at a time    head file  display the first few lines of a file    tail file  display the last few lines of a file    grep 'keyword'  file   search a file for keywords    wc file -l/w  count number of lines/words/characters in file",
            "title": "Viewing files"
        },
        {
            "location": "/CheatSheet/#useful-commands",
            "text": "command  description  usage example      which   see where command is installed  which python    pwd  which directory I'm in  pwd    man   manual page for command  man cut    grep   filter for lines which fit pattern  cat myfile | grep GATK    cut -d  -f  split line by delimiter and get field number 3  cat myfile | cut -d'_' -f3    sort   sort lines, often used with  uniq  sort myfile | uniq    uniq  suppress repeated lines, works only if sorted  see above example    less  paginated output  less myfile    >  redirect output (e.g. list files and save filenames in aaa.txt)  ls > aaa.txt    >>  append output to existing file  echo \"blah\" >> aaa.txt    find  find files with some properties e.g. display all files recursively from current directory  find .    chmod  change permissions on a file or directory  chmod u+x myscript.sh    top  display most intensive processes  top    ps auxw  list processes  ps auxw",
            "title": "Useful commands:"
        },
        {
            "location": "/CheatSheet/#redirecting",
            "text": "Command  Meaning      command > file  redirect standard output to a file    command >> file  append standard output to a file    command < file  redirect standard input from a file    command1 | command2  pipe the output of command1 to the input of command2    cat file1 file2 > file0  concatenate file1 and file2 to file0    sort  sort data    who  list users currently logged in    *  match any number of characters    ?  match one character    man command  read the online manual page for a command    whatis command  brief description of a command    apropos keyword  match commands with keyword in their man pages",
            "title": "Redirecting"
        },
        {
            "location": "/CheatSheet/#permissions",
            "text": "Symbol  Meaning      u  user    g  group    o  other    a  all    r  read    w  write (and delete)    x  execute (and access directory)    u+x  add executing permission for user    u-x  take away executing permission",
            "title": "Permissions"
        },
        {
            "location": "/CheatSheet/#modules",
            "text": "Command  Meaning      module avail  show a list of the core (primary) modules available    module load  module_name  loads the named software module    module use  /projects/community/projectsmodule use /projects/community/modulefiles/  Now if you do module avail, it also shows modules uploaded by other users    module spider  shows a comprehensive list of all available modules or type name after spider to show details about specific module    module purge  removes all loaded module    ml  lists modules loaded",
            "title": "Modules"
        }
    ]
}