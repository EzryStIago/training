{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is university-wide initiative that is to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to get access or become owners, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \nAmarel user guide\n as it has slurm tips and examples. \n\n\n\n\nFor users familiar with Linux but new to \nslurm\n, follow \nthis path\n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Home"
        },
        {
            "location": "/howto/",
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u <your net id>\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:",
            "title": "HowTo"
        },
        {
            "location": "/howto/#tunneling",
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:",
            "title": "Tunneling"
        },
        {
            "location": "/howto/#how-to-launch-jupyter-notebook-on-the-cluster",
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u <your net id>  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:",
            "title": "How to launch Jupyter notebook on the cluster"
        },
        {
            "location": "/workshop/",
            "text": "Overview\n\n\nThese are notes from the workshop on genomics. What's covered in these notes: \n1. Setup\n\n\nGenomic Software\n\n\nThis is a list of software to install for the workshop\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nSeqtk\n\n\nvery handy and fast for processing fastq/a files\n\n\nlink\n\n\n\n\n\n\nsratoolkit\n\n\ndownloading and processing data from GEO/SRA database\n\n\nlink\n\n\n\n\n\n\nhtseq-count\n\n\ncounting the  reads mapped on to genomics feature\n\n\nlink\n\n\n\n\n\n\nfastQC\n\n\nwidely used for sequencing read QC\n\n\nlink\n\n\n\n\n\n\nRSeQC-2.6.4\n\n\nAn RNA-seq quality control package, multiple functions\n\n\npython package\n\n\n\n\n\n\ntrimmomatic\n\n\nfastq quality trim and adaptor removal\n\n\nlink\n\n\n\n\n\n\n\n\nThis is a list of software already available on the cluster and the command you need to execute to load it in your environment: \n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nload it on the cluster\n\n\n\n\n\n\n\n\n\n\nSamtools\n\n\n\n\nmodule load samtools\n\n\n\n\n\n\nBedtools\n\n\n\n\nmodule load bedtools2./2.25.0\n\n\n\n\n\n\nbowtie2\n\n\nalignment software\n\n\nmodule load bowtie2\n\n\n\n\n\n\ntophat2\n\n\nalignment software\n\n\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n\n\n\n\n\n\nR\n\n\nlanguage for statistical analysis\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\n\n\n\n\nThis is a list of other software you might find useful:\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nGSEA\n\n\ngenome set enrichment analysis\n\n\nlink\n\n\n\n\n\n\nIGV\n\n\nInteractive Genome Viewer\n\n\nlink\n\n\n\n\n\n\nCytoscape\n\n\nNetwork visualization softwar\n\n\nlink\n\n\n\n\n\n\n\n\nSetup\n\n\nConnect to the cluster login node\n\n\nDo this by one of the following methods: \n\n\n\n\nvia a terminal\n: if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as \nputty\n or \nmoba-xterm\n. Then from your terminal connect to the cluster by executing the following command: \n\n\nssh -X <your net id>@amarel.hpc.rutgers.edu\n \n\n   This is the preferred way, as your copy-pasting will most likely work best. \n\n\nvia FastX\n: in your browser, go to \nhttps://amarel.hpc.rutgers.edu:3443\n\n\n\n\nGet resources on the compute node\n\n\nYou get to the cluster to execute your computations by running the following command in your terminal:  \n\n\nsrun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash\n \n\nNotice that the name in your terminal will change from \namarel\n to node name like \nhal0025\n or \nslepner086\n. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command: \n\n\n\n\n\n\n\n\ncommand part\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\nsrun\n\n\nslurm\n run, i.e. allocate resources and run via \nslurm\n scheduler\n\n\n\n\n\n\n-p main\n\n\non the main partition, one of several queues on the cluster\n\n\n\n\n\n\n--reservation=genomics\n\n\nwe reserved some compute nodes for this workshop to not wait long for resources\n\n\n\n\n\n\n-N 1\n\n\nask for one node\n\n\n\n\n\n\n-c 2\n\n\nask for two cores\n\n\n\n\n\n\n-n 1\n\n\nthis will be 1 most times\n\n\n\n\n\n\n-t 01:40:00\n\n\nrun this for a maximum time of 1 hour 40 minutes\n\n\n\n\n\n\n--pty /bin/bash\n\n\nrun the terminal shell in an interactive mode\n\n\n\n\n\n\n\n\nPrepare some directories for the data\n\n\nYou have two main spaces on the Amarel cluster. These are: \n\n\n\n\nyour home directory - \n/home/netid/\n  (e.g. \n/home/kp807/\n for my netid) -\n\n\nyour scratch directory - \n/scratch/netid/\n\n\n\n\nThey differ in how often they are backed up and by size (100Gb for \n/home\n and 500Gb for \n/scratch\n). So we will install programs in \n/home\n, while the data and output will be in \n/scratch\n. Execute these commands: \n\n\n                cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here\n\n\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nInstall programs\n\n\nEach program will have slightly different installation instructions. Here is a handy sequence of commands that will install them: \n\n\n##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                    \n\n\n\n\n\nMoreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e. \nsamtools\n command will work if you execute \nmodule load samtools\n): \n\n\nmodule load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\nDownload data\n\n\nWe will download human RNA-seq data with \nGEO accession GSE52778\n. The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nWe will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored. \nsratoolkit\n will be in your home directory, under \nPrograms\n, and the \nvdb-config\n might be under the \nbin\n directory. You will enter \n/scratch/your_netID/Genomics_Workshop/download\n for the path - NOTE you have to replace \nyour_netID\n with your true netId, e.g. \nkp807\n. Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them. \n\n\n                vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y\n\n\n\n\nThen execute the following commands to get the data. Both \nprefetch\n and \nfastq-dump\n are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]\n\n\nprefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ???? \n\n\n\n\nYou have to pay attention to where you are putting your data. So these two commands will actually be several: \n\n\n                cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516\n\n\n\n\nThe commands above showed how to do it for one sample. You need to do it for 6 samples total. \n\n\n                SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)\n\n\n\n\nRunning bioinformatics jobs\n\n\nFastQC - raw data QC\n\n\nExplain what is fastqc is doing here - TODO\n\n\n        cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq\n\n\n\n\nFastQC produces an html page as output, \nfastqc/SRR1039508_1_fastqc.html\n, with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf\n\n\nTrimmomatic - quality trim/adaptor removal\n\n\n    ##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35",
            "title": "Workshops"
        },
        {
            "location": "/workshop/#overview",
            "text": "These are notes from the workshop on genomics. What's covered in these notes: \n1. Setup",
            "title": "Overview"
        },
        {
            "location": "/workshop/#genomic-software",
            "text": "This is a list of software to install for the workshop     software  description  link      Seqtk  very handy and fast for processing fastq/a files  link    sratoolkit  downloading and processing data from GEO/SRA database  link    htseq-count  counting the  reads mapped on to genomics feature  link    fastQC  widely used for sequencing read QC  link    RSeQC-2.6.4  An RNA-seq quality control package, multiple functions  python package    trimmomatic  fastq quality trim and adaptor removal  link     This is a list of software already available on the cluster and the command you need to execute to load it in your environment:      software  description  load it on the cluster      Samtools   module load samtools    Bedtools   module load bedtools2./2.25.0    bowtie2  alignment software  module load bowtie2    tophat2  alignment software  module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0    R  language for statistical analysis  module load intel/17.0.4 R-Project/3.4.1     This is a list of other software you might find useful:     software  description  link      GSEA  genome set enrichment analysis  link    IGV  Interactive Genome Viewer  link    Cytoscape  Network visualization softwar  link",
            "title": "Genomic Software"
        },
        {
            "location": "/workshop/#setup",
            "text": "",
            "title": "Setup"
        },
        {
            "location": "/workshop/#connect-to-the-cluster-login-node",
            "text": "Do this by one of the following methods:    via a terminal : if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as  putty  or  moba-xterm . Then from your terminal connect to the cluster by executing the following command:   ssh -X <your net id>@amarel.hpc.rutgers.edu   \n   This is the preferred way, as your copy-pasting will most likely work best.   via FastX : in your browser, go to  https://amarel.hpc.rutgers.edu:3443",
            "title": "Connect to the cluster login node"
        },
        {
            "location": "/workshop/#get-resources-on-the-compute-node",
            "text": "You get to the cluster to execute your computations by running the following command in your terminal:    srun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash   \nNotice that the name in your terminal will change from  amarel  to node name like  hal0025  or  slepner086 . This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command:      command part  meaning      srun  slurm  run, i.e. allocate resources and run via  slurm  scheduler    -p main  on the main partition, one of several queues on the cluster    --reservation=genomics  we reserved some compute nodes for this workshop to not wait long for resources    -N 1  ask for one node    -c 2  ask for two cores    -n 1  this will be 1 most times    -t 01:40:00  run this for a maximum time of 1 hour 40 minutes    --pty /bin/bash  run the terminal shell in an interactive mode",
            "title": "Get resources on the compute node"
        },
        {
            "location": "/workshop/#prepare-some-directories-for-the-data",
            "text": "You have two main spaces on the Amarel cluster. These are:    your home directory -  /home/netid/   (e.g.  /home/kp807/  for my netid) -  your scratch directory -  /scratch/netid/   They differ in how often they are backed up and by size (100Gb for  /home  and 500Gb for  /scratch ). So we will install programs in  /home , while the data and output will be in  /scratch . Execute these commands:                   cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated",
            "title": "Prepare some directories for the data"
        },
        {
            "location": "/workshop/#install-programs",
            "text": "Each program will have slightly different installation instructions. Here is a handy sequence of commands that will install them:   ##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                      Moreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e.  samtools  command will work if you execute  module load samtools ):   module load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1",
            "title": "Install programs"
        },
        {
            "location": "/workshop/#download-data",
            "text": "We will download human RNA-seq data with  GEO accession GSE52778 . The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated  We will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored.  sratoolkit  will be in your home directory, under  Programs , and the  vdb-config  might be under the  bin  directory. You will enter  /scratch/your_netID/Genomics_Workshop/download  for the path - NOTE you have to replace  your_netID  with your true netId, e.g.  kp807 . Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them.                   vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y  Then execute the following commands to get the data. Both  prefetch  and  fastq-dump  are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]  prefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ????   You have to pay attention to where you are putting your data. So these two commands will actually be several:                   cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516  The commands above showed how to do it for one sample. You need to do it for 6 samples total.                   SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)",
            "title": "Download data"
        },
        {
            "location": "/workshop/#running-bioinformatics-jobs",
            "text": "",
            "title": "Running bioinformatics jobs"
        },
        {
            "location": "/workshop/#fastqc-raw-data-qc",
            "text": "Explain what is fastqc is doing here - TODO          cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq  FastQC produces an html page as output,  fastqc/SRR1039508_1_fastqc.html , with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf",
            "title": "FastQC - raw data QC"
        },
        {
            "location": "/workshop/#trimmomatic-quality-trimadaptor-removal",
            "text": "##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35",
            "title": "Trimmomatic - quality trim/adaptor removal"
        },
        {
            "location": "/ressentials/",
            "text": "To start R shell on the cluster\n\n\nSee workshop for explanation of the various options of \nsrun\n\n\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R\n\n\n\n\nPackages used from BioConductor\n\n\nIf these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: \n\n\n        source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")\n\n\n\n\nExample: Calculate gene length\n\n\nGet some data from ENSEMBLE\n\n\nwget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz\n\n\nIn R shell, you can execute these commands to compute gene lengths: \n\n\n\n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')    \n\n\n\n\nSome R essentials\n\n\nArithmetic functions\n\n\n        2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)\n\n\n\n\nCreating objects\n\n\n        ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()\n\n\n\n\nVector and Matrix\n\n\n        x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x\n\n\n\n\nData Frames\n\n\n        z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C\n\n\n\n\nMore ways to subset dataframes\n\n\n        z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]\n\n\n\n\nLists\n\n\n        mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)\n\n\n\n\nFunctions\n\n\n        my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)\n\n\n\n\nVarious directory/file/library manipulations\n\n\n        library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")\n\n\n\n\nSave all the objects you have created to your workspace\n\n\n        save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file\n\n\n\n\nRemove objects from your workspace\n\n\n        ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()\n\n\n\n\nSave a history of all the commands entered\n\n\n        savehistory(\"introhistory.Rhistory\")",
            "title": "R tutorial"
        },
        {
            "location": "/ressentials/#to-start-r-shell-on-the-cluster",
            "text": "See workshop for explanation of the various options of  srun          module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R",
            "title": "To start R shell on the cluster"
        },
        {
            "location": "/ressentials/#packages-used-from-bioconductor",
            "text": "If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands:           source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")",
            "title": "Packages used from BioConductor"
        },
        {
            "location": "/ressentials/#example-calculate-gene-length",
            "text": "Get some data from ENSEMBLE  wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz  In R shell, you can execute these commands to compute gene lengths:   \n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')",
            "title": "Example: Calculate gene length"
        },
        {
            "location": "/ressentials/#some-r-essentials",
            "text": "",
            "title": "Some R essentials"
        },
        {
            "location": "/ressentials/#arithmetic-functions",
            "text": "2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)",
            "title": "Arithmetic functions"
        },
        {
            "location": "/ressentials/#creating-objects",
            "text": "ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()",
            "title": "Creating objects"
        },
        {
            "location": "/ressentials/#vector-and-matrix",
            "text": "x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x",
            "title": "Vector and Matrix"
        },
        {
            "location": "/ressentials/#data-frames",
            "text": "z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C",
            "title": "Data Frames"
        },
        {
            "location": "/ressentials/#more-ways-to-subset-dataframes",
            "text": "z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]",
            "title": "More ways to subset dataframes"
        },
        {
            "location": "/ressentials/#lists",
            "text": "mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)",
            "title": "Lists"
        },
        {
            "location": "/ressentials/#functions",
            "text": "my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)",
            "title": "Functions"
        },
        {
            "location": "/ressentials/#various-directoryfilelibrary-manipulations",
            "text": "library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")",
            "title": "Various directory/file/library manipulations"
        },
        {
            "location": "/ressentials/#save-all-the-objects-you-have-created-to-your-workspace",
            "text": "save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file",
            "title": "Save all the objects you have created to your workspace"
        },
        {
            "location": "/ressentials/#remove-objects-from-your-workspace",
            "text": "ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()",
            "title": "Remove objects from your workspace"
        },
        {
            "location": "/ressentials/#save-a-history-of-all-the-commands-entered",
            "text": "savehistory(\"introhistory.Rhistory\")",
            "title": "Save a history of all the commands entered"
        },
        {
            "location": "/resources/",
            "text": "Here are some resources you might want to consult to learn more about how to use an HPC cluster\n\n\nSlurm\n\n\n\n\nOARC cluster user guide Amarel/Perceval\n - this is a must-read for any new users, even if you are an experienced Linux user \n\n\nOARC cluster community\n - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it\n\n\nintro videos by Kristina\n\n\nterse slurm tips\n\n\n\n\nLinux tutorials\n\n\n\n\nlinux tutorial by Galen\n\n\nto be continued\n\n\n\n\nGraphical user interface\n\n\n\n\nweb-based access to the cluster (still testing) - only from campus or VPN\n\n\n\n\nResources on the web\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources"
        },
        {
            "location": "/resources/#slurm",
            "text": "OARC cluster user guide Amarel/Perceval  - this is a must-read for any new users, even if you are an experienced Linux user   OARC cluster community  - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it  intro videos by Kristina  terse slurm tips",
            "title": "Slurm"
        },
        {
            "location": "/resources/#linux-tutorials",
            "text": "linux tutorial by Galen  to be continued",
            "title": "Linux tutorials"
        },
        {
            "location": "/resources/#graphical-user-interface",
            "text": "web-based access to the cluster (still testing) - only from campus or VPN",
            "title": "Graphical user interface"
        },
        {
            "location": "/resources/#resources-on-the-web",
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources on the web"
        }
    ]
}