{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is a university-wide initiative that aims to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to \nget access\n or \nbecome owners\n, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \ncluster user guide\n as it has slurm tips and examples. \n\n\nWARNING - READ!\n\n\n\n\nDo not run large computational jobs on the \nlogin\n node. Use slurm to allocate resources on the compute node. Failure to respect the golden rule can get your account suspended.\n\n\nNumber of jobs to submit should not exceed 5,000 at a time - queue is currently 10,000 and slurm will go to sleep if that is exceeded.\n\n\nRight-size your jobs: jobs may run sooner if the resources requested are smaller and your future priority is degraded if you have asked for (even if not used) a lot of resources lately.\n\n\nuse \n-n\n option in sbatch only if you know what you are doing. Multithreaded but not multi-node jobs should use \n-c\n option. \n\n\nTODO: add more complete list\n\n\n\n\nLearning paths\n\n\n\n\nIf you understand material in \nlmod cheatsheet\n and \nslurm cheatsheet\n you are good to go.\n\n\nFor users familiar with Linux but new to \nslurm\n, check out \nenvironment modules\n and \nintro to slurm\n. \n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets, or use \nOnDemand\n until you feel more comfortable with Linux.", 
            "title": "Home"
        }, 
        {
            "location": "/#warning-read", 
            "text": "Do not run large computational jobs on the  login  node. Use slurm to allocate resources on the compute node. Failure to respect the golden rule can get your account suspended.  Number of jobs to submit should not exceed 5,000 at a time - queue is currently 10,000 and slurm will go to sleep if that is exceeded.  Right-size your jobs: jobs may run sooner if the resources requested are smaller and your future priority is degraded if you have asked for (even if not used) a lot of resources lately.  use  -n  option in sbatch only if you know what you are doing. Multithreaded but not multi-node jobs should use  -c  option.   TODO: add more complete list", 
            "title": "WARNING - READ!"
        }, 
        {
            "location": "/#learning-paths", 
            "text": "If you understand material in  lmod cheatsheet  and  slurm cheatsheet  you are good to go.  For users familiar with Linux but new to  slurm , check out  environment modules  and  intro to slurm .   For users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets, or use  OnDemand  until you feel more comfortable with Linux.", 
            "title": "Learning paths"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/", 
            "text": "General Information\n\n\nThere are several clusters administered by OARC. Each cluster is using the resource scheduler \nSlurm\n to allocate users the requested resources. \n\n\n\n\nAmarel\n - a general access cluster. \nRequest access\n or \nbecome an owner\n\n\nPerceval\n - similar to Amarel, but paid for, and to be used by exclusively by, NIH grants and grantees\n\n\nNM3\n - \nNewark Massive Memory Machine\n\n\nSirius\n - a single big machine with no resource scheduler, acts as a big desktop with multiple concurrent users\n\n\n\n\nCluster Resources\n\n\nAmarel\n\n\nAmarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource.\n\n\nAmarel includes the following hardware (this list may already be outdated since the cluster is actively growing):\n\n\n52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM\n20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM\n4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard\n2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM\n53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM\n5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM\n26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM\n4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM\n3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard\n2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard\n1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard\n\n\n\n\nDefault run time = 2 hours in the 'main' partition\nMaximum run time = 3 days in the 'main' partition\n\n\nPerceval\n\n\nPerceval has the same setup as Amarel. The same file system is mounted on both clusters and changes made to the filesystem on one cluster will be reflected in the other cluster as well. \n\n\nPerceval includes the following hardware (this list may already be outdated since the cluster is actively growing):\n\n\n132 CPU-only nodes, each with 24 Intel Xeon E5-2680 cores + 128 GB RAM\n8 GPU nodes with 24 Intel Xeon E5-2680 cores + 128 GB RAM\n1 Large Memory node with 48 Intel Xeon E5-2680 cores + 1.5 TB RAM\n\n\nDefault run time = 2 hours for all partitions\nMaximum run time = 7 days in the 'main' partition, 2 days in the 'gpu' partition\n\n\nBasic operations - connecting and moving files\n\n\nConnecting to the cluster\n\n\n\n\nAmarel is currently accessed using a single hostname, \namarel.rutgers.edu\n\n\nPerceval is currently accessed using a single hostname, \nperceval.rutgers.edu\n\n\n\n\nFor the example purposes, we will assume you are connecting to Amarel. When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc. So, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using.\n\n\nssh [your NetID]@amarel.rutgers.edu\n\n\nIf you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See \nhere\n for details.\n\n\nMoving files\n\n\nThere are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory,\n\n\nscp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID]\n\n\nTo pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command),\n\n\nscp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt  .\n\n\nIf you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it:\n\n\ntar -czf my-directory.tar.gz my-directory\n\n\nAfter moving it, you can unpack that .tar.gz file to get your original directory and contents:\n\n\ntar -xzf my-directory.tar.gz\n\n\nA handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel:\n\n\nrsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir\n\n\nIn this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data)\n\n\nTo sync a local directory with updated data from Amarel:\n\n\nrsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir\n\n\nHere, we've simply reversed the order of the local and remote locations.\n\n\nFor added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case):\n\n\nrsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir\n\n\nOnDemand - GUI for the cluster\n\n\nFor users not familiar with Linux, there is an option to connect to the cluster via a web browser (either from campus or through VPN) until you get more confortable with Linux. Both connecting and moving files can be achieved through \nthis interface\n. However, you are strongly encouraged to get comfortable with Linux, as your productivity will soar and a GUI is never as flexible as a command line interface.  \n\n\nFunctionalities of OnDemand: \n\n\n\n\nfile upload\n\n\nfile editor\n\n\nlinux shell \n\n\nlaunch Jupyter notebook\n\n\nlaunch RStudio\n\n\ncompose slurm job from a template\n\n\nsubmit slurm job\n\n\nview job queue \n\n\n\n\nListing available resources\n\n\nBefore requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources.\n\n\nExample of using the \nsinfo\n command:\n\n\nsinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*        up 3-00:00:00      4 drain* hal[0050-0051,0055,0093]\nmain*        up 3-00:00:00      5  down* slepner[084-088]\nmain*        up 3-00:00:00      4  drain hal[0023,0025-0027]\nmain*        up 3-00:00:00     86    mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082]\nmain*        up 3-00:00:00     84  alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083]\nmain*        up 3-00:00:00      2   down gpu[001-002]\ngpu          up 3-00:00:00      8    mix gpu[003,006],pascal[001-006]\ngpu          up 3-00:00:00      1  alloc gpu005\ngpu          up 3-00:00:00      2   down gpu[001-002]\nphi          up 3-00:00:00      1    mix gpu004\nmem          up 3-00:00:00      1    mix mem002\nmem          up 3-00:00:00      1  alloc mem001\n\n\n\n\nUnderstanding this output:\n\n\n\n\nThere are 4 basic partitions, main (traditional compute nodes, CPUs only), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors.\n\n\nThe upper limit for a job\u2019s run time is 3 days (72 hours).\n\n\n\n\n\n\n\n\n\n\nTerm\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAllocated (alloc)\n\n\nnodes that are currently running jobs.\n\n\n\n\n\n\nMixed (mix)\n\n\nnodes have jobs using some, but not all, CPU cores onboard.\n\n\n\n\n\n\nIdle\n\n\nnodes are currently available for new jobs.\n\n\n\n\n\n\nDrained (drain, drng)\n\n\nnodes are not available for use and may be offline for maintenance.\n\n\n\n\n\n\nSlepner, Norse Mythology\n\n\n\"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores).\n\n\n\n\n\n\nHal\n\n\nHal is a dependable member of the Discovery One crew who does an excellent job of following instructions.\n\n\n\n\n\n\nPascal\n\n\nFrench mathematician and the name of one of NVIDIA's GPU architectures.\n\n\n\n\n\n\nCUDA\n\n\nThis is the name of a parallel computing platform and application programming interface (API) model created by Nvidia\n\n\n\n\n\n\n\n\nLoading software modules\n\n\nWhen you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.\n\n\nLmod video walkthrough\n\n\n\n\nCommands used in the video: \n\n\nlmod = https://lmod.readthedocs.io = A New Environment Module System\nSolves the problem of setting environment variables so you can run different software easily\n\nMain commands: \nmodule avail                                  # which modules are available\nmodule spider                                 # find information about a software\nmodule load                                   # load a particular software\nmodule use  /projects/community/modulefiles   # user contributed software, unsupported\nmodule purge                                  # unload all modules\nmodule list                                   # list currently loaded modules\n\ngotcha: will not show modules until you load up their dependencies - e.g. R-Project\n\nmodule avail               # available system-installed modules\nwhich R                    # no results\nmodule spider R            # too many results\nmodule spider R-           # keyword search for specific module\nmodule load intel/17.0.4   # after load, we will see R-Project in the list of modules\nmodule avail               \nmodule load R-Project/3.4.1# loading R\nmodule list\nwhich R                    # now R is in my path\n\nsrun -N 1 -n 1 -c 1 -t 10:00 R --no-save\n\nmodule use  /projects/community/modulefiles   # user contributed software, unsupported\nmodule avail                                  # which modules are available\nmodule load spark/2.3.0-kp807                 # load a particular software\nwhich spark-shell                             # shows where the software is located\n\n\n\n\nLonger explanation\n\n\nThe module avail command will show a list of the core (primary) modules available:\n\n\nmodule avail\n\n-------------------------------------------------- /opt/sw/modulefiles/Core --------------------------------------------------\n   ARACNE/20110228         blat/35                  gcc/4.9.3               intel_mkl/16.0.3 (D)    mvapich2/2.2        (D)\n   HISAT2/2.0.4            bowtie2/2.2.6            gcc/4.9.4               intel_mkl/17.0.0        openmpi/2.1.1\n   HISAT2/2.1.0     (D)    bowtie2/2.2.9     (D)    gcc/5.3                 intel_mkl/17.0.1        pgi/16.9\n   MATLAB/R2017a           bwa/0.7.12               gcc/5.4          (D)    intel_mkl/17.0.2        pgi/16.10           (D)\n   MATLAB/R2017b    (D)    bwa/0.7.13        (D)    hdf5/1.8.16             java/1.7.0_79           python/2.7.11\n   Mathematica/11.1        cuda/7.5                 intel/16.0.1            java/1.8.0_66           python/2.7.12\n   OpenCV/2.3.1            cuda/8.0                 intel/16.0.3     (D)    java/1.8.0_73           python/3.5.0\n   STAR/2.5.2a             cuda/9.0          (D)    intel/16.0.4            java/1.8.0_121          python/3.5.2        (D)\n   Trinotate/2.0.2         cudnn/7.0.3              intel/17.0.0            java/1.8.0_141          samtools/0.1.19\n   bamtools/2.4.0          cufflinks/2.2.1          intel/17.0.1            java/1.8.0_152   (D)    samtools/1.2\n   bcftools/1.2            delly/0.7.6              intel/17.0.2            modeller/9.16           samtools/1.3.1      (D)\n   bedtools2/2.25.0        gaussian/03revE01        intel/17.0.4            moe/2016.0802           trinityrnaseq/2.1.1\n   blast/2.6.0             gaussian/09revD01 (D)    intel_mkl/16.0.1        mvapich2/2.1\n\n\n\n\nUnderstanding this output:\n\n\nThe packages with a (D) are the default versions for packages where multiple versions are available.\n\n\n\n\nTo see a comprehensive list of all available modules (not just the core modules) use the \nmodule spider\n command, or \nmodule keyword\n command (see CheatSheat section). \n\n\nTo be able to use community-contributed software, execute \nmodule use /projects/community/modulefiles\n before using the the above mentioned commands (see Community section). \n\n\n\n\nmodule spider\n\n---------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------------------------------------------------\n  ARACNE: ARACNE/20110228\n    ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context\n\n  HISAT2: HISAT2/2.0.4, HISAT2/2.1.0\n    HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes\n\n  HMMER: HMMER/3.1b2\n    HMMER: biosequence analysis using profile hidden Markov models\n\n  MATLAB: MATLAB/R2017a, MATLAB/R2017b\n    MATLAB: The Language of Technical Computing\n\n  Mathematica: Mathematica/11.1\n    Wolfram Mathematica: Modern Technical Computing\n\n  NAMD: NAMD/2.10\n    NAMD: Scalable Molecular Dynamics\n\n  ORCA: ORCA/3.0.3\n    ORCA: An ab initio, DFT and semiempirical SCF-MO package\n\n  OpenCV: OpenCV/2.3.1\n    OpenCV: Open Source Computer Vision\n\n\n\n\nLoading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use.   \n\n\n\n\n\n\n\n\ncommand\n\n\nexplanation\n\n\nusage\n\n\n\n\n\n\n\n\n\n\nmodule load\n\n\nload a software module\n\n\nmodule load intel/16.0.3\n\n\n\n\n\n\nmodule unload\n\n\nremove select modules\n\n\nmodule unload intel/16.0.3\n\n\n\n\n\n\nmodule purge\n\n\nload the default version of any software package\n\n\nmodule load intel\n\n\n\n\n\n\n\n\nBelow are some examples.\n\n\nmodule load intel/16.0.3 mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3   2) mvapich2/2.1\n\nmodule unload mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\nmodule purge\nmodule list\nNo modules loaded\n\nmodule load intel\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\n\n\n\nIf you always use the same software modules, your \n~/.bashrc\n (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your \n~/.bashrc\n file to change or remove those commands later.\n\n\nPLEASE NOTE:\n Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.\n\n\nRunning slurm jobs\n\n\nVideo walkthrough\n\n\nDemoing \nsinfo, srun, squeue, scancel\n commands: \n\n\n\n\nRunning a serial (single-core) job\n\n\nHere\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance.\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (\n1 if multithread tasks)\n#SBATCH --mem=2000                   # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1\n\nsrun /scratch/[your NetID]/zipper/2.4.1/bin/zipper \n my-input-file.in\n\n\n\n\nUnderstanding this job script:\n\n\n\n\nA job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job.\n\n\nThe lines beginning with #SBATCH contain commands intended only for the workload manager.\n\n\nMy job will be assigned to the \u201cmain\u201d partition (job queue).\n\n\nIf this job is preempted, it will be returned to the job queue and will start again when required resources are available\n\n\nThis job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.\n\n\nMy job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running.\n\n\nAny output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number.\n\n\n\n\nBe sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable.\n\n\nHere\u2019s how to run a serial batch job, loading modules and using the \nsbatch\n command:\n\n\nsbatch my-job-script.sh\n  \n\n\nThe \nsbatch\n command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.\n\n\nRunning a parallel (multicore MPI) job\n\n\nHere\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here.\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=16                  # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (\n1 if multithread tasks)\n#SBATCH --mem=124000                 # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1 mvapich2/2.1\n\nsrun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper \n my-input-file.in\n\n\n\n\nUnderstanding this job script:\n\n\n\n\nThe srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request.\n\n\nThis job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.\n\n\nNote here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.\n\n\n\n\nHere\u2019s how to run a parallel batch job, loading modules and using the \nsbatch\n command:\n\n\nsbatch my-job-script.sh\n\n\nNote here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.\n\n\nHere\u2019s how to run a parallel batch job, loading modules and using the \nsbatch\n command:\n\n\nsbatch my-job-script.sh\n\n\nRunning array of jobs\n\n\nArray job is an approach to handle multiple jobs with single job script. Here is an example to submit 500 jobs with single job script. \n\n\n#!/bin/bash\n#SBATCH --partition=main             # Name of the partition\n#SBATCH --job-name=arrayjobs         # Name of the job\n#SBATCH --ntasks=1                   # Number of tasks\n#SBATCH --cpus-per-task=1            # Number of CPUs per task\n#SBATCH --mem=1GB                    # Requested memory\n#SBATCH --array=0-499                # Array job will submit 500 jobs\n#SBATCH --time=00:10:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT file\n#SBATCH --error=slurm.%N.%j.err      # STDERR file \n\necho  -n \nExecuting on the machine: \n \nhostname\necho \nArray Task ID : \n $SLURM_ARRAY_TASK_ID \necho \n Random number : \n $RANDOM\n\n\n\n\nIn the above description, the line \n\n#SBATCH --array=0-499\n\nsubmits 500 jobs. \n\n\nThe \"%\" seperator is useful to limit the number of maximum jobs. For example, the following line sets a maximum of 100 jobs to the queue\n\n\n#SBATCH --array=0-499%100\n\n\n\n\nRunning an interactive job\n\n\nAn interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications.\n\n\nWhen submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job:\n\n\n[NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\nsrun: job 1365471 queued and waiting for resources\nsrun: job 1365471 has been allocated resources\n\n[NetID@slepner045 ~]$\n\n\n\n\nNotice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command.\n\n\nMonitoring the status of jobs\n\n\n_\nThe simplest way to quickly check on the status of active jobs is by using the \nsqueue\n command:\n\n\nsqueue -u [your NetID]\n\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n1633383      main   zipper    xx345   R       1:15      1 slepner36\n\n\n\n\nHere, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS).\n\n\nFor summary accounting information (including jobs that have already completed), you can use the \nsacct\n command:\n\n\nsacct\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n1633383          zipper       main      statx         16    RUNNING      0:0\n\n\n\n\nHere, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED.\n\n\nFor complete and detailed job info, you can use the \nscontrol show job [JobID]\n command:\n\n\nscontrol show job 244348\n\nJobId=244348 JobName=XIoT22\n   UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A\n   Priority=5050 Nice=0 Account=oarc QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A\n   SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19\n   StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   Partition=main AllocNode:Sid=amarel1:22391\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=hal0053\n   BatchHost=hal0053\n   NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=28,mem=124000M,node=1\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0\n   Features=(null) Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/gc563/run.STMV.CPU.slurm\n   WorkDir=/scratch/gc563\n   StdErr=/scratch/gc563/slurm.%N.244348.out\n   StdIn=/dev/null\n   StdOut=/scratch/gc563/slurm.%N.244348.out\n   Power=\n\n\n\n\nIf your jobs have already completed (or have been terminated), you can see details about those jobs using the sacct command with your NetID and a start time for the list of jobs this command will produce.\n\n\nsacct --user=[NetID] --starttime=2018-07-03 --format=JobID,Partition,JobName,MaxRSS,NodeList,Elapsed,MaxDiskRead,MaxDiskWrite,State\n\n\nKilling/ cancelling/ terminating jobs\n\n\nTo terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate:\n\n\nscancel 1633383\n\nA job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this:\n\n\nslurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***\n\n\nInstalling your own software\n\n\nPackage management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]).\n\n\nAs an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist:\n\n\n\n\n\n\nDownload your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the \nwget\n utility:\n\nwget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz\n\n\n\n\n\n\nUnzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files:\n\n\ntar -zxf zipper-4.1.5.tar.gz\n\n\n\n\n\n\nRead the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software.  \n\n\n\n\n\n\n  cd zipper-4.1.5 \n  less README\n\n\n\n\n\n\n\n\nLoad the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER:\n\n\nmodule load intel/16.0.3 mvapich2/2.1\n\n\n\n\n\n\nPerform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the \n./configure -\u2013help\n command. What you see below is just a typical example of special options that might be specified.  \n\n\n\n\n\n\n./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared\nmake -j 4\nmake install\n\n\n\n\nSeveral packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation: \n\n      - \n/home/[NetID]/zipper/4.1.5/bin\n where executables will be placed\n      - \n/home/[NetID]/zipper/4.1.5/lib\n where library files will be placed\n      - \n/home/[NetID]/zipper/4.1.5/include\n where header files will be placed\n      - \n/home/[NetID]/zipper/4.1.5/share/man\n where documentation will be placed\n\n\n\n\nConfigure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment:  \n\n\n\n\nexport PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH\nexport C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH\nexport MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH\n\n\n\n\nThese \nexport\n commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.\n\n\nSingularity\n\n\nSingularity\n is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers.\n\n\nDocker\n is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.\n\n\nImportant differences between Docker and Singularity:\n\n\n\n\nDocker and Singularity have their own container formats.\n\n\nDocker containers can be imported and run using Singularity.\n\n\nDocker containers usually run as root, which means you cannot run Docker on a  shared computing system (cluster).\n\n\nSingularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.\n\n\n\n\nImporting a Docker image:\n\n\nIf you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container.\n\n\nHere's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel.\n\n\nIf you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories.\n\n\nStart your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel.\n\n\n$ sudo docker run -it ubuntu:latest bash  \nroot@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc\n\n\n\n\nExporting your Docker image\n\n\nFind the name of your Docker image using the 'docker ps' command,\n\n\n$ sudo docker ps\nCONTAINER ID  IMAGE          COMMAND  CREATED        STATUS        NAMES\n11a87dkw8748  ubuntu:latest  \nbash\n   2 minutes ago  Up 2 minutes  bendakaya_pakodi\n\n\n\n\nIn this example the name of the images is bendakaya_pakodi. Export this image to a tarball,  \n\n\n$ sudo docker export bendakaya_pakodi \n ubuntu.tar\n\n\n\n\nConverting to a Singularity image\n\n\nYou will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel.\n\n\nCreate an empty singularity image, and then import the exported docker image into it,  \n\n\n$ sudo singularity create ubuntu.img\nCreating a sparse image with a maximum size of 1024MiB...\nUsing given image size of 1024\nFormatting image (/sbin/mkfs.ext3)\nDone. Image can be found at: ubuntu.img\n$ sudo singularity import ubuntu.img ubuntu.tar\n\n\n\n\nUsing Singularity containers inside a SLURM job\n\n\nTransfer\n your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel.\n\n\nYou can run any task/program inside the container by prefacing it with\n\n\nsingularity exec [your image name]\n \n\n\nHere is a simple example job script that executes commands inside a container,  \n\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=sing2me           # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (\n1 if multithread tasks)\n#SBATCH --mem=4000                   # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n\nmodule purge\nmodule load singularity/.2.5.1\n\n## Where am I running?\nsrun singularity exec ubuntu.img hostname\n\n## What is the current time and date?\nsrun singularity exec ubuntu.img date\n\n\n\n\nIf you created directories for any Amarel filesystems, you should find they are mounted inside your container,\n\n\nmount | grep gpfs\n/dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime)\n/dev/projects/oarc on /projects/oarc type gpfs (rw,relatime)\n\n\n\n\nNOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.\n\n\nTroubleshooting/ Common Problems\n\n\nFailure to load module dependencies/prerequisites:\n\n\n module load R-Project/3.4.1\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \nR-Project/3.4.1\n\nTry: \nmodule spider R-Project/3.4.1\n to see how to load the module(s).\n\n\n\n\n\nThis software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load:  \n\n\nmodule spider R-Project/3.4.1\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help: \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.\n\n\n\n\nAh-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1\n\n\nAcknowledging Amarel\n\n\nPlease reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include:\n\n\nOffice of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey\n\n\nA suggested acknowledgement is:\n\n\nThe authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu", 
            "title": "Cluster Guides"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#general-information", 
            "text": "There are several clusters administered by OARC. Each cluster is using the resource scheduler  Slurm  to allocate users the requested resources.    Amarel  - a general access cluster.  Request access  or  become an owner  Perceval  - similar to Amarel, but paid for, and to be used by exclusively by, NIH grants and grantees  NM3  -  Newark Massive Memory Machine  Sirius  - a single big machine with no resource scheduler, acts as a big desktop with multiple concurrent users", 
            "title": "General Information"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#cluster-resources", 
            "text": "", 
            "title": "Cluster Resources"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#amarel", 
            "text": "Amarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource.  Amarel includes the following hardware (this list may already be outdated since the cluster is actively growing):  52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM\n20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM\n4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard\n2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM\n53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM\n5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM\n26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM\n4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM\n3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard\n2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard\n1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard  Default run time = 2 hours in the 'main' partition\nMaximum run time = 3 days in the 'main' partition", 
            "title": "Amarel"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#perceval", 
            "text": "Perceval has the same setup as Amarel. The same file system is mounted on both clusters and changes made to the filesystem on one cluster will be reflected in the other cluster as well.   Perceval includes the following hardware (this list may already be outdated since the cluster is actively growing):  132 CPU-only nodes, each with 24 Intel Xeon E5-2680 cores + 128 GB RAM\n8 GPU nodes with 24 Intel Xeon E5-2680 cores + 128 GB RAM\n1 Large Memory node with 48 Intel Xeon E5-2680 cores + 1.5 TB RAM  Default run time = 2 hours for all partitions\nMaximum run time = 7 days in the 'main' partition, 2 days in the 'gpu' partition", 
            "title": "Perceval"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#basic-operations-connecting-and-moving-files", 
            "text": "", 
            "title": "Basic operations - connecting and moving files"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#connecting-to-the-cluster", 
            "text": "Amarel is currently accessed using a single hostname,  amarel.rutgers.edu  Perceval is currently accessed using a single hostname,  perceval.rutgers.edu   For the example purposes, we will assume you are connecting to Amarel. When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc. So, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using.  ssh [your NetID]@amarel.rutgers.edu  If you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See  here  for details.", 
            "title": "Connecting to the cluster"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#moving-files", 
            "text": "There are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory,  scp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID]  To pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command),  scp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt  .  If you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it:  tar -czf my-directory.tar.gz my-directory  After moving it, you can unpack that .tar.gz file to get your original directory and contents:  tar -xzf my-directory.tar.gz  A handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel:  rsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir  In this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data)  To sync a local directory with updated data from Amarel:  rsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir  Here, we've simply reversed the order of the local and remote locations.  For added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case):  rsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir", 
            "title": "Moving files"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#ondemand-gui-for-the-cluster", 
            "text": "For users not familiar with Linux, there is an option to connect to the cluster via a web browser (either from campus or through VPN) until you get more confortable with Linux. Both connecting and moving files can be achieved through  this interface . However, you are strongly encouraged to get comfortable with Linux, as your productivity will soar and a GUI is never as flexible as a command line interface.    Functionalities of OnDemand:    file upload  file editor  linux shell   launch Jupyter notebook  launch RStudio  compose slurm job from a template  submit slurm job  view job queue", 
            "title": "OnDemand - GUI for the cluster"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#listing-available-resources", 
            "text": "Before requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources.  Example of using the  sinfo  command:  sinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmain*        up 3-00:00:00      4 drain* hal[0050-0051,0055,0093]\nmain*        up 3-00:00:00      5  down* slepner[084-088]\nmain*        up 3-00:00:00      4  drain hal[0023,0025-0027]\nmain*        up 3-00:00:00     86    mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082]\nmain*        up 3-00:00:00     84  alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083]\nmain*        up 3-00:00:00      2   down gpu[001-002]\ngpu          up 3-00:00:00      8    mix gpu[003,006],pascal[001-006]\ngpu          up 3-00:00:00      1  alloc gpu005\ngpu          up 3-00:00:00      2   down gpu[001-002]\nphi          up 3-00:00:00      1    mix gpu004\nmem          up 3-00:00:00      1    mix mem002\nmem          up 3-00:00:00      1  alloc mem001  Understanding this output:   There are 4 basic partitions, main (traditional compute nodes, CPUs only), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors.  The upper limit for a job\u2019s run time is 3 days (72 hours).      Term  Meaning      Allocated (alloc)  nodes that are currently running jobs.    Mixed (mix)  nodes have jobs using some, but not all, CPU cores onboard.    Idle  nodes are currently available for new jobs.    Drained (drain, drng)  nodes are not available for use and may be offline for maintenance.    Slepner, Norse Mythology  \"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores).    Hal  Hal is a dependable member of the Discovery One crew who does an excellent job of following instructions.    Pascal  French mathematician and the name of one of NVIDIA's GPU architectures.    CUDA  This is the name of a parallel computing platform and application programming interface (API) model created by Nvidia", 
            "title": "Listing available resources"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#loading-software-modules", 
            "text": "When you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.", 
            "title": "Loading software modules"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#lmod-video-walkthrough", 
            "text": "Commands used in the video:   lmod = https://lmod.readthedocs.io = A New Environment Module System\nSolves the problem of setting environment variables so you can run different software easily\n\nMain commands: \nmodule avail                                  # which modules are available\nmodule spider                                 # find information about a software\nmodule load                                   # load a particular software\nmodule use  /projects/community/modulefiles   # user contributed software, unsupported\nmodule purge                                  # unload all modules\nmodule list                                   # list currently loaded modules\n\ngotcha: will not show modules until you load up their dependencies - e.g. R-Project\n\nmodule avail               # available system-installed modules\nwhich R                    # no results\nmodule spider R            # too many results\nmodule spider R-           # keyword search for specific module\nmodule load intel/17.0.4   # after load, we will see R-Project in the list of modules\nmodule avail               \nmodule load R-Project/3.4.1# loading R\nmodule list\nwhich R                    # now R is in my path\n\nsrun -N 1 -n 1 -c 1 -t 10:00 R --no-save\n\nmodule use  /projects/community/modulefiles   # user contributed software, unsupported\nmodule avail                                  # which modules are available\nmodule load spark/2.3.0-kp807                 # load a particular software\nwhich spark-shell                             # shows where the software is located", 
            "title": "Lmod video walkthrough"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#longer-explanation", 
            "text": "The module avail command will show a list of the core (primary) modules available:  module avail\n\n-------------------------------------------------- /opt/sw/modulefiles/Core --------------------------------------------------\n   ARACNE/20110228         blat/35                  gcc/4.9.3               intel_mkl/16.0.3 (D)    mvapich2/2.2        (D)\n   HISAT2/2.0.4            bowtie2/2.2.6            gcc/4.9.4               intel_mkl/17.0.0        openmpi/2.1.1\n   HISAT2/2.1.0     (D)    bowtie2/2.2.9     (D)    gcc/5.3                 intel_mkl/17.0.1        pgi/16.9\n   MATLAB/R2017a           bwa/0.7.12               gcc/5.4          (D)    intel_mkl/17.0.2        pgi/16.10           (D)\n   MATLAB/R2017b    (D)    bwa/0.7.13        (D)    hdf5/1.8.16             java/1.7.0_79           python/2.7.11\n   Mathematica/11.1        cuda/7.5                 intel/16.0.1            java/1.8.0_66           python/2.7.12\n   OpenCV/2.3.1            cuda/8.0                 intel/16.0.3     (D)    java/1.8.0_73           python/3.5.0\n   STAR/2.5.2a             cuda/9.0          (D)    intel/16.0.4            java/1.8.0_121          python/3.5.2        (D)\n   Trinotate/2.0.2         cudnn/7.0.3              intel/17.0.0            java/1.8.0_141          samtools/0.1.19\n   bamtools/2.4.0          cufflinks/2.2.1          intel/17.0.1            java/1.8.0_152   (D)    samtools/1.2\n   bcftools/1.2            delly/0.7.6              intel/17.0.2            modeller/9.16           samtools/1.3.1      (D)\n   bedtools2/2.25.0        gaussian/03revE01        intel/17.0.4            moe/2016.0802           trinityrnaseq/2.1.1\n   blast/2.6.0             gaussian/09revD01 (D)    intel_mkl/16.0.1        mvapich2/2.1  Understanding this output:  The packages with a (D) are the default versions for packages where multiple versions are available.   To see a comprehensive list of all available modules (not just the core modules) use the  module spider  command, or  module keyword  command (see CheatSheat section).   To be able to use community-contributed software, execute  module use /projects/community/modulefiles  before using the the above mentioned commands (see Community section).    module spider\n\n---------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------------------------------------------------\n  ARACNE: ARACNE/20110228\n    ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context\n\n  HISAT2: HISAT2/2.0.4, HISAT2/2.1.0\n    HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes\n\n  HMMER: HMMER/3.1b2\n    HMMER: biosequence analysis using profile hidden Markov models\n\n  MATLAB: MATLAB/R2017a, MATLAB/R2017b\n    MATLAB: The Language of Technical Computing\n\n  Mathematica: Mathematica/11.1\n    Wolfram Mathematica: Modern Technical Computing\n\n  NAMD: NAMD/2.10\n    NAMD: Scalable Molecular Dynamics\n\n  ORCA: ORCA/3.0.3\n    ORCA: An ab initio, DFT and semiempirical SCF-MO package\n\n  OpenCV: OpenCV/2.3.1\n    OpenCV: Open Source Computer Vision  Loading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use.        command  explanation  usage      module load  load a software module  module load intel/16.0.3    module unload  remove select modules  module unload intel/16.0.3    module purge  load the default version of any software package  module load intel     Below are some examples.  module load intel/16.0.3 mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3   2) mvapich2/2.1\n\nmodule unload mvapich2/2.1\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3\n\nmodule purge\nmodule list\nNo modules loaded\n\nmodule load intel\nmodule list\nCurrently Loaded Modules:\n  1) intel/16.0.3  If you always use the same software modules, your  ~/.bashrc  (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your  ~/.bashrc  file to change or remove those commands later.  PLEASE NOTE:  Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.", 
            "title": "Longer explanation"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#running-slurm-jobs", 
            "text": "", 
            "title": "Running slurm jobs"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#video-walkthrough", 
            "text": "Demoing  sinfo, srun, squeue, scancel  commands:", 
            "title": "Video walkthrough"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#running-a-serial-single-core-job", 
            "text": "Here\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance.  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task ( 1 if multithread tasks)\n#SBATCH --mem=2000                   # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1\n\nsrun /scratch/[your NetID]/zipper/2.4.1/bin/zipper   my-input-file.in  Understanding this job script:   A job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job.  The lines beginning with #SBATCH contain commands intended only for the workload manager.  My job will be assigned to the \u201cmain\u201d partition (job queue).  If this job is preempted, it will be returned to the job queue and will start again when required resources are available  This job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.  My job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running.  Any output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number.   Be sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable.  Here\u2019s how to run a serial batch job, loading modules and using the  sbatch  command:  sbatch my-job-script.sh     The  sbatch  command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.", 
            "title": "Running a serial (single-core) job"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#running-a-parallel-multicore-mpi-job", 
            "text": "Here\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here.  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --requeue                    # Return job to the queue if preempted\n#SBATCH --job-name=zipx001a          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=16                  # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task ( 1 if multithread tasks)\n#SBATCH --mem=124000                 # Real memory (RAM) required (MB)\n#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\ncd /scratch/[your NetID]\n\nmodule purge\nmodule load intel/16.0.3 fftw/3.3.1 mvapich2/2.1\n\nsrun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper   my-input-file.in  Understanding this job script:   The srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request.  This job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.  Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.   Here\u2019s how to run a parallel batch job, loading modules and using the  sbatch  command:  sbatch my-job-script.sh  Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.  Here\u2019s how to run a parallel batch job, loading modules and using the  sbatch  command:  sbatch my-job-script.sh", 
            "title": "Running a parallel (multicore MPI) job"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#running-array-of-jobs", 
            "text": "Array job is an approach to handle multiple jobs with single job script. Here is an example to submit 500 jobs with single job script.   #!/bin/bash\n#SBATCH --partition=main             # Name of the partition\n#SBATCH --job-name=arrayjobs         # Name of the job\n#SBATCH --ntasks=1                   # Number of tasks\n#SBATCH --cpus-per-task=1            # Number of CPUs per task\n#SBATCH --mem=1GB                    # Requested memory\n#SBATCH --array=0-499                # Array job will submit 500 jobs\n#SBATCH --time=00:10:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT file\n#SBATCH --error=slurm.%N.%j.err      # STDERR file \n\necho  -n  Executing on the machine:   \nhostname\necho  Array Task ID :   $SLURM_ARRAY_TASK_ID \necho   Random number :   $RANDOM  In the above description, the line  #SBATCH --array=0-499 \nsubmits 500 jobs.   The \"%\" seperator is useful to limit the number of maximum jobs. For example, the following line sets a maximum of 100 jobs to the queue  #SBATCH --array=0-499%100", 
            "title": "Running array of jobs"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#running-an-interactive-job", 
            "text": "An interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications.  When submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job:  [NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\nsrun: job 1365471 queued and waiting for resources\nsrun: job 1365471 has been allocated resources\n\n[NetID@slepner045 ~]$  Notice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command.", 
            "title": "Running an interactive job"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#monitoring-the-status-of-jobs", 
            "text": "_\nThe simplest way to quickly check on the status of active jobs is by using the  squeue  command:  squeue -u [your NetID]\n\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n1633383      main   zipper    xx345   R       1:15      1 slepner36  Here, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS).  For summary accounting information (including jobs that have already completed), you can use the  sacct  command:  sacct\n\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n1633383          zipper       main      statx         16    RUNNING      0:0  Here, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED.  For complete and detailed job info, you can use the  scontrol show job [JobID]  command:  scontrol show job 244348\n\nJobId=244348 JobName=XIoT22\n   UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A\n   Priority=5050 Nice=0 Account=oarc QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A\n   SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19\n   StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   Partition=main AllocNode:Sid=amarel1:22391\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=hal0053\n   BatchHost=hal0053\n   NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=28,mem=124000M,node=1\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0\n   Features=(null) Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/gc563/run.STMV.CPU.slurm\n   WorkDir=/scratch/gc563\n   StdErr=/scratch/gc563/slurm.%N.244348.out\n   StdIn=/dev/null\n   StdOut=/scratch/gc563/slurm.%N.244348.out\n   Power=  If your jobs have already completed (or have been terminated), you can see details about those jobs using the sacct command with your NetID and a start time for the list of jobs this command will produce.  sacct --user=[NetID] --starttime=2018-07-03 --format=JobID,Partition,JobName,MaxRSS,NodeList,Elapsed,MaxDiskRead,MaxDiskWrite,State", 
            "title": "Monitoring the status of jobs"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#killing-cancelling-terminating-jobs", 
            "text": "To terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate:  scancel 1633383 \nA job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this:  slurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***", 
            "title": "Killing/ cancelling/ terminating jobs"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#installing-your-own-software", 
            "text": "Package management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]).  As an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist:    Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the  wget  utility: wget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz    Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files:  tar -zxf zipper-4.1.5.tar.gz    Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software.        cd zipper-4.1.5 \n  less README    Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER:  module load intel/16.0.3 mvapich2/2.1    Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the  ./configure -\u2013help  command. What you see below is just a typical example of special options that might be specified.      ./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared\nmake -j 4\nmake install  Several packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation:  \n      -  /home/[NetID]/zipper/4.1.5/bin  where executables will be placed\n      -  /home/[NetID]/zipper/4.1.5/lib  where library files will be placed\n      -  /home/[NetID]/zipper/4.1.5/include  where header files will be placed\n      -  /home/[NetID]/zipper/4.1.5/share/man  where documentation will be placed   Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment:     export PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH\nexport C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH\nexport MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH  These  export  commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.", 
            "title": "Installing your own software"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#singularity", 
            "text": "Singularity  is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers.  Docker  is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.", 
            "title": "Singularity"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#important-differences-between-docker-and-singularity", 
            "text": "Docker and Singularity have their own container formats.  Docker containers can be imported and run using Singularity.  Docker containers usually run as root, which means you cannot run Docker on a  shared computing system (cluster).  Singularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.", 
            "title": "Important differences between Docker and Singularity:"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#importing-a-docker-image", 
            "text": "If you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container.  Here's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel.  If you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories.  Start your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel.  $ sudo docker run -it ubuntu:latest bash  \nroot@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc", 
            "title": "Importing a Docker image:"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#exporting-your-docker-image", 
            "text": "Find the name of your Docker image using the 'docker ps' command,  $ sudo docker ps\nCONTAINER ID  IMAGE          COMMAND  CREATED        STATUS        NAMES\n11a87dkw8748  ubuntu:latest   bash    2 minutes ago  Up 2 minutes  bendakaya_pakodi  In this example the name of the images is bendakaya_pakodi. Export this image to a tarball,    $ sudo docker export bendakaya_pakodi   ubuntu.tar", 
            "title": "Exporting your Docker image"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#converting-to-a-singularity-image", 
            "text": "You will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel.  Create an empty singularity image, and then import the exported docker image into it,    $ sudo singularity create ubuntu.img\nCreating a sparse image with a maximum size of 1024MiB...\nUsing given image size of 1024\nFormatting image (/sbin/mkfs.ext3)\nDone. Image can be found at: ubuntu.img\n$ sudo singularity import ubuntu.img ubuntu.tar", 
            "title": "Converting to a Singularity image"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#using-singularity-containers-inside-a-slurm-job", 
            "text": "Transfer  your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel.  You can run any task/program inside the container by prefacing it with  singularity exec [your image name]    Here is a simple example job script that executes commands inside a container,    #SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=sing2me           # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task ( 1 if multithread tasks)\n#SBATCH --mem=4000                   # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n\nmodule purge\nmodule load singularity/.2.5.1\n\n## Where am I running?\nsrun singularity exec ubuntu.img hostname\n\n## What is the current time and date?\nsrun singularity exec ubuntu.img date  If you created directories for any Amarel filesystems, you should find they are mounted inside your container,  mount | grep gpfs\n/dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime)\n/dev/projects/oarc on /projects/oarc type gpfs (rw,relatime)  NOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.", 
            "title": "Using Singularity containers inside a SLURM job"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#troubleshooting-common-problems", 
            "text": "Failure to load module dependencies/prerequisites:   module load R-Project/3.4.1\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested:  R-Project/3.4.1 \nTry:  module spider R-Project/3.4.1  to see how to load the module(s).  This software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load:    module spider R-Project/3.4.1\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help: \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.  Ah-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1", 
            "title": "Troubleshooting/ Common Problems"
        }, 
        {
            "location": "/guides/Cluster_User_Guide/#acknowledging-amarel", 
            "text": "Please reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include:  Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey  A suggested acknowledgement is:  The authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu", 
            "title": "Acknowledging Amarel"
        }, 
        {
            "location": "/guides/Cluster_Examples/", 
            "text": "Using R\n\n\nGenerally, there are 2 approaches for accessing R on Amarel:\n\n1. use one of the pre-installed R modules named R-Project/\nversion\n (these versions come bundled with a very broad range of common and useful tools). \n\n2. install your own custom build of R in your /home directory or in a shared directory (e.g. \n/projects/[group]\n or \n/projects/community\n).\n\n\nUsing pre-installed R modules\n\n\nStart by finding which module you wish to use with the 'module spider R-Project' command:\n\n\nmodule spider R-Project\n--------------------------------------------------\n  R-Project:\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n     Versions:\n        R-Project/3.2.2\n        R-Project/3.2.5\n        R-Project/3.3.3\n        R-Project/3.4.1\n--------------------------------------------------\n  To find detailed information about R-Project please enter the full name.\n  For example:\n     $ module spider R-Project/3.4.1\n--------------------------------------------------\n\n\n\n\nNext, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?):\n\n\n module spider R-Project/3.4.1\n--------------------------------------------------\n  R-Project: R-Project/3.4.1\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help:    \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.\n\n\n\n\nLoad the R-Project module of your choice:\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\nwhich R\n/opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R\n\n\n\n\nWhat R packages are already installed?\n\n\npkgs \n- installed.packages ()\npkgs[,c(\nPackage\n, \nVersion\n)]\n\n                     Package                Version    \nbase                 \nbase\n                 \n3.4.4\n    \nBH                   \nBH\n                   \n1.66.0-1\n \nBiobase              \nBiobase\n              \n2.38.0\n   \nBiocGenerics         \nBiocGenerics\n         \n0.24.0\n   \nBiocInstaller        \nBiocInstaller\n        \n1.28.0\n   \nBiocParallel         \nBiocParallel\n         \n1.12.0\n   \nBiostrings           \nBiostrings\n           \n2.46.0\n   \nbitops               \nbitops\n               \n1.0-6\n    \nboot                 \nboot\n                 \n1.3-20\n   \nBSgenome             \nBSgenome\n             \n1.46.0\n\n...\n\n\n\n\nIt's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account.\n\n\nFirst, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access:\n\n\nmkdir ~/my.R.libs\n\n\nNext, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file:\n\n\nexport R_LIBS=~/my.R.libs\n\nNow, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library.\n\n\n Some important notes about installing packages:\n\n\nThere are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball (\n.tar.gz or \n.tgz file) containing the source code for that version.\n\n\nFor example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the \nwebsite\n. At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command:  \n\n\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz\n, lib=\n~/my.R.libs\n)\n\n\n\n\nThe other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows:\n\n\ninstall.packages(\ngplots\n, lib=\n~/my.R.libs\n)\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz\n, lib=\n~/my.R.libs\n)\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/class_7.3-14.tar.gz\n, lib=\n~/my.R.libs\n)\ninstall.packages(c(\nmvtnorm\n,\nmodeltools\n,\nstrucchange\n,\ncoin\n,\nzoo\n,\nsandwich\n), lib=\n~/my.R.libs\n)\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz\n, lib=\n~/my.R.libs\n)\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz\n, lib=\n~/my.R.libs\n)\ninstall.packages(\nhttps://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz\n, lib=\n~/my.R.libs\n)\n\n\n\n\nOnce all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.\n\n\nInstalling your own build of R\n\n\nFor some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below.\n\n\nHere are the commands to use for installing R-3.4.4 from source:\n\n\nwget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz\ntar -zxf R-3.4.4.tar.gz\ncd R-3.4.4\nmodule load gcc/5.4 java/1.8.0_161\n./configure --prefix=/home/gc563/R/3.4.4 --enable-java\nmake -j 4\nmake install\ncd ..\nrm -rf R-3.4.4*\n\n\n\n\nHere, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages.\n\n\nWhen I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference.\n\n\nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.\n\n\nSince I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory.\n\n\nBefore using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):\n\n\nmodule load gcc/5.4 java/1.8.0_161\nexport PATH=/home/gc563/R/3.4.4/bin:$PATH\nexport LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport MANPATH=/home/gc563/R/3.4.4/share/man\n\n\n\n\nIf you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:\n\n\n$ module list\nCurrently Loaded Modules:\n  1) gcc/5.4   2) java/1.8.0_161\n$ which R\n~/R/3.4.4/bin/R\n\n\n\n\nNow that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library  \n\n\n .libPaths()  \n[1] \n/home/gc563/R/3.4.4/lib64/R/library\n\n\n\n\n\nInstall a package:\n\n\ninstall.packages(\nrJava\n)\nlibrary(rJava)\n\n\n\n\nSaving figures/ plots from R (without a display):\n\nNeed to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example:\n\n\n$ R --no-save\npng('my-figure.png', type='cairo')\nplot(rnorm(10),rnorm(10))\ndev.off()\nq()\n\n\n\n\nUsing Python\n\n\nGenerally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).\n\n\nUsing pre-installed Python modules\n\n\nWith the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package:\n\n\nmodule load python/3.5.2\npip install --user Django\n\n\n\n\nNote: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly:\n\n\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\npip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5)\n$ pip install -U --user pip\nSuccessfully installed pip-10.0.1\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\nTraceback (most recent call last):\n  File \n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n, line 7, in \n    from pip import main\nImportError: cannot import name 'main'\n\n$ .local/bin/pip --version\npip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5)\n$ .local/bin/pip install --user Django\n\n\n\n\nBuilding your own Python installation\n\n\nUsing this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference).\n\nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.\n\n\nwget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz\ntar -zxf Python-3.6.5.tgz\ncd Python-3.6.5\n./configure --prefix=/home/gc563/python/3.6.5\nmake -j 4\nmake install\ncd ..\nrm -rf Python-3.6.5*\n\n\n\n\nBefore using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):\n\n\nexport PATH=/home/gc563/python/3.6.5/bin:$PATH\nexport LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib\nexport MANPATH=/home/gc563/python/3.6.5/share/man\n\n\n\n\nIf you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:\n\n\nwhich python3  \n~/python/3.6.5/bin\n\n\nRunning GROMACS\n\n\nHere is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel.  \n\n\n\n\nDownload a PDB file.\n\n\nwget https://files.rcsb.org/view/5EWT.pdb\n  \n\n\nLoad the GROMACS software module plus any needed prerequisites.\n\n\nmodule purge  \n    module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\n\n\nImport the PDB into GROMACS, while defining the force field and water model to be used for this system.\n\n\ngmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp\n \n\n\nIncrease the size of the unit cell to accommodate a reasonable volume of solvent around the protein.\n\n\ngmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5\n\n\nNow add water molecules into the empty space in the unit cell to solvate the protein.\n\n\ngmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro\n\n\nPrepare your SLURM job script(s). The 2 \nmdrun\n commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2 \nmdrun\n commands below, simply replace the example \nmdrun\n command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate \ngrompp\n step.\n\n\n\n\n#!/bin/bash\n#SBATCH --partition=main                # Partition (job queue)\n#SBATCH --job-name=gmdrun               # Assign an 8-character name to your job\n#SBATCH --nodes=1                       # Number of nodes\n#SBATCH --ntasks=16                     # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1               # Threads per process (or per core)\n#SBATCH --mem=124000                    # Memory per node (MB)\n#SBATCH --time=00:20:00                 # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out        # combined STDOUT and STDERR output file\n#SBATCH --export=ALL                    # Export you current env to the job env\n\nmodule purge\nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\nsrun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\\n                -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\\n                -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log\n\n\n\n\n\n\nPerform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions:\n\n\n\n\nintegrator     = steep\nnsteps         = 200\ncutoff-scheme  = Verlet\ncoulombtype    = PME\npbc            = xyz\nemtol          = 100\n\n\n\n\nThese are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization:  \n\n\ngmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log\n\n\n\n\n\n\nPerform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions:\n\n\n\n\nintegrator               = md\ndt                       = 0.002\nnsteps                   = 5000\nnstlog                   = 50\nnstenergy                = 50\nnstxout                  = 50\ncontinuation             = yes\nconstraints              = all-bonds\nconstraint-algorithm     = lincs\ncutoff-scheme            = Verlet\ncoulombtype              = PME\nrcoulomb                 = 1.0\nvdwtype                  = Cut-off\nrvdw                     = 1.0\nDispCorr                 = EnerPres\ntcoupl                   = V-rescale\ntc-grps                  = Protein  SOL\ntau-t                    = 0.1      0.1\nref-t                    = 300      300\npcoupl                   = Parrinello-Rahman\ntau-p                    = 2.0\ncompressibility          = 4.5e-5\nref-p                    = 1.0\n\n\n\n\nThese are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration:\n\n\ngmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log\n\n\n\n\nRunning TensorFlow with a GPU\n\n\nTensorFlow has two versions of its python package: \ntensorflow\n and \ntensorflow-gpu\n, but confusingly the command to use it is the same in both cases: \nimport tensorflow as tf\n  (and not \nimport tensorflow-gpu as tf\n in case of the GPU version). This means that it really matters which package is installed in your environment. \n\n\n\n\nyou can control your environment using Singularity image (but the problem arises if you need a package not included in the prebuilt image, in which case you need to build the image yourself)\n\n\nyou can control your environment using conda environments (or virtual-env). \n\n\n\n\nUsing Singularity\n\n\nTo do this, you can use the \nSingularity\n container manager and a Docker image containing the TensorFlow software. \n\n\nRunning Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script \nTF_gpu.sh\n)\n\n\n#!/bin/bash\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=TF_gpu            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=2            # Cores per task (\n1 if multithread tasks)\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --mem=16000                  # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\nmodule load singularity/.2.5.1\n\nsrun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python \n\n\n\n\nOnce your job script is ready, submit it using the sbatch command:\n\n\n$ sbatch TF_gpu.sh\n\nAlternatively, you can run Singularity interactively:\n\n\n$ srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu\n\nDocker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu\nCache folder set to /home/user/.singularity/docker\nCreating container runtime...\nImporting: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz\nImporting: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz\nImporting: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz\nImporting: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz\nImporting: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz\nImporting: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz\nImporting: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz\nImporting: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz\nImporting: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz\nImporting: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz\nSingularity: Invoking an interactive shell within container...\n\nSingularity tensorflow:latest-gpu:~\n \n\n\n\n\nNow, you're ready to execute commands:\n\n\n\nSingularity tensorflow:latest-gpu:~\n python -V\nPython 2.7.12\nSingularity tensorflow:latest-gpu:~\n python3 -V\nPython 3.5.2\n\n\n\n\nPlease remember to exit from your interactive job after you are finished with your calculations.  \n\n\nThere are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow:1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be \npython3\n instead of \npython\n in your script.\n\n\nUsing conda\n\n\nYou can either install your own version of Anaconda in your home directory, or you can use a community module. \n\n\nmodule use /projects/community/modulefiles  #loads community-contributed software packages\nmodule keyword anaconda                     #search packages with 'anaconda' in description \n\n\n\n\noutput: \n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThe following modules match your search criteria: \nanaconda\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n  py-data-science-stack: py-data-science-stack/5.1.0-kp807\n    Sets up anaconda 5.1.0 in your environment\n\n\n\n\nSo here are commands with which you can load tensorflow package: \n\n\nmodule load py-data-science-stack/5.1.0-kp807\nconda env list                                 # be patient\nsource activate tensorflow-gpu-1.7.0           #\n\n\n\n\nNote that if you try to import tensorflow on a node without a gpu, the import will fail, because it will try to load cuda driver that is not installed (because there is no GPU on the machine). \nSo you need to include this line in a slurm script, where you request a GPU resource in slurm: \n\n#SBATCH --gres=gpu:1\n and \n--partition=gpu\n and perhaps \n--constraint=pascal\n. Here is an example of a slurm script that trains mnist: \n\n\n#!/bin/bash\n\n#SBATCH --partition=gpu             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=mnist_p100            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --constraint=pascal          # will look for that architecture\n#SBATCH --mem=6000                  # Real memory (RAM) required (MB)\n#SBATCH --time=03:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.p100_gpu1.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.p100_gpu1.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack/5.1.0-kp807\n\nsource activate tensorflow-gpu-1.7.0\n\nexport PYTHONPATH=$PYTHONPATH:/home/kp807/tf/models1.7     #mnist.py needs a package defined in the tensorflow/models, not a standard distribution of TF. \n\nsrun python /home/kp807/tf/benchmarking/mnist.py", 
            "title": "Examples"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-r", 
            "text": "Generally, there are 2 approaches for accessing R on Amarel: \n1. use one of the pre-installed R modules named R-Project/ version  (these versions come bundled with a very broad range of common and useful tools).  \n2. install your own custom build of R in your /home directory or in a shared directory (e.g.  /projects/[group]  or  /projects/community ).", 
            "title": "Using R"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-pre-installed-r-modules", 
            "text": "Start by finding which module you wish to use with the 'module spider R-Project' command:  module spider R-Project\n--------------------------------------------------\n  R-Project:\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n     Versions:\n        R-Project/3.2.2\n        R-Project/3.2.5\n        R-Project/3.3.3\n        R-Project/3.4.1\n--------------------------------------------------\n  To find detailed information about R-Project please enter the full name.\n  For example:\n     $ module spider R-Project/3.4.1\n--------------------------------------------------  Next, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?):   module spider R-Project/3.4.1\n--------------------------------------------------\n  R-Project: R-Project/3.4.1\n--------------------------------------------------\n    Description:\n      R: The R Project for Statistical Computing\n    This module can only be loaded through the following modules:\n      intel/17.0.4\n    Help:    \n      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.  Load the R-Project module of your choice:  module load intel/17.0.4 R-Project/3.4.1\nwhich R\n/opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R  What R packages are already installed?  pkgs  - installed.packages ()\npkgs[,c( Package ,  Version )]\n\n                     Package                Version    \nbase                  base                   3.4.4     \nBH                    BH                     1.66.0-1  \nBiobase               Biobase                2.38.0    \nBiocGenerics          BiocGenerics           0.24.0    \nBiocInstaller         BiocInstaller          1.28.0    \nBiocParallel          BiocParallel           1.12.0    \nBiostrings            Biostrings             2.46.0    \nbitops                bitops                 1.0-6     \nboot                  boot                   1.3-20    \nBSgenome              BSgenome               1.46.0 \n...  It's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account.  First, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access:  mkdir ~/my.R.libs  Next, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file:  export R_LIBS=~/my.R.libs \nNow, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library.   Some important notes about installing packages:  There are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball ( .tar.gz or  .tgz file) containing the source code for that version.  For example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the  website . At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command:    install.packages( https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz , lib= ~/my.R.libs )  The other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows:  install.packages( gplots , lib= ~/my.R.libs )\ninstall.packages( https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz , lib= ~/my.R.libs )\ninstall.packages( https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz , lib= ~/my.R.libs )\ninstall.packages(c( mvtnorm , modeltools , strucchange , coin , zoo , sandwich ), lib= ~/my.R.libs )\ninstall.packages( https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz , lib= ~/my.R.libs )\ninstall.packages( https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz , lib= ~/my.R.libs )\ninstall.packages( https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz , lib= ~/my.R.libs )  Once all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.", 
            "title": "Using pre-installed R modules"
        }, 
        {
            "location": "/guides/Cluster_Examples/#installing-your-own-build-of-r", 
            "text": "For some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below.  Here are the commands to use for installing R-3.4.4 from source:  wget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz\ntar -zxf R-3.4.4.tar.gz\ncd R-3.4.4\nmodule load gcc/5.4 java/1.8.0_161\n./configure --prefix=/home/gc563/R/3.4.4 --enable-java\nmake -j 4\nmake install\ncd ..\nrm -rf R-3.4.4*  Here, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages.  When I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference.  At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.  Since I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory.  Before using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):  module load gcc/5.4 java/1.8.0_161\nexport PATH=/home/gc563/R/3.4.4/bin:$PATH\nexport LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64\nexport MANPATH=/home/gc563/R/3.4.4/share/man  If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:  $ module list\nCurrently Loaded Modules:\n  1) gcc/5.4   2) java/1.8.0_161\n$ which R\n~/R/3.4.4/bin/R  Now that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library     .libPaths()  \n[1]  /home/gc563/R/3.4.4/lib64/R/library   Install a package:  install.packages( rJava )\nlibrary(rJava)  Saving figures/ plots from R (without a display): \nNeed to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example:  $ R --no-save\npng('my-figure.png', type='cairo')\nplot(rnorm(10),rnorm(10))\ndev.off()\nq()", 
            "title": "Installing your own build of R"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-python", 
            "text": "Generally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).", 
            "title": "Using Python"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-pre-installed-python-modules", 
            "text": "With the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package:  module load python/3.5.2\npip install --user Django  Note: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly:  $ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\npip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5)\n$ pip install -U --user pip\nSuccessfully installed pip-10.0.1\n$ which pip\n/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip\n$ pip --version\nTraceback (most recent call last):\n  File  /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip , line 7, in \n    from pip import main\nImportError: cannot import name 'main'\n\n$ .local/bin/pip --version\npip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5)\n$ .local/bin/pip install --user Django", 
            "title": "Using pre-installed Python modules"
        }, 
        {
            "location": "/guides/Cluster_Examples/#building-your-own-python-installation", 
            "text": "Using this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference). \nAt the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.  wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz\ntar -zxf Python-3.6.5.tgz\ncd Python-3.6.5\n./configure --prefix=/home/gc563/python/3.6.5\nmake -j 4\nmake install\ncd ..\nrm -rf Python-3.6.5*  Before using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):  export PATH=/home/gc563/python/3.6.5/bin:$PATH\nexport LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib\nexport MANPATH=/home/gc563/python/3.6.5/share/man  If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:  which python3  \n~/python/3.6.5/bin", 
            "title": "Building your own Python installation"
        }, 
        {
            "location": "/guides/Cluster_Examples/#running-gromacs", 
            "text": "Here is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel.     Download a PDB file.  wget https://files.rcsb.org/view/5EWT.pdb     Load the GROMACS software module plus any needed prerequisites.  module purge  \n    module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1  Import the PDB into GROMACS, while defining the force field and water model to be used for this system.  gmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp    Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein.  gmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5  Now add water molecules into the empty space in the unit cell to solvate the protein.  gmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro  Prepare your SLURM job script(s). The 2  mdrun  commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2  mdrun  commands below, simply replace the example  mdrun  command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate  grompp  step.   #!/bin/bash\n#SBATCH --partition=main                # Partition (job queue)\n#SBATCH --job-name=gmdrun               # Assign an 8-character name to your job\n#SBATCH --nodes=1                       # Number of nodes\n#SBATCH --ntasks=16                     # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1               # Threads per process (or per core)\n#SBATCH --mem=124000                    # Memory per node (MB)\n#SBATCH --time=00:20:00                 # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out        # combined STDOUT and STDERR output file\n#SBATCH --export=ALL                    # Export you current env to the job env\n\nmodule purge\nmodule load intel/17.0.1 mvapich2/2.2 gromacs/2016.1\nsrun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\\n                -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\\n                -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log   Perform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions:   integrator     = steep\nnsteps         = 200\ncutoff-scheme  = Verlet\ncoulombtype    = PME\npbc            = xyz\nemtol          = 100  These are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization:    gmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log   Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions:   integrator               = md\ndt                       = 0.002\nnsteps                   = 5000\nnstlog                   = 50\nnstenergy                = 50\nnstxout                  = 50\ncontinuation             = yes\nconstraints              = all-bonds\nconstraint-algorithm     = lincs\ncutoff-scheme            = Verlet\ncoulombtype              = PME\nrcoulomb                 = 1.0\nvdwtype                  = Cut-off\nrvdw                     = 1.0\nDispCorr                 = EnerPres\ntcoupl                   = V-rescale\ntc-grps                  = Protein  SOL\ntau-t                    = 0.1      0.1\nref-t                    = 300      300\npcoupl                   = Parrinello-Rahman\ntau-p                    = 2.0\ncompressibility          = 4.5e-5\nref-p                    = 1.0  These are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration:  gmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp\n\ngmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log", 
            "title": "Running GROMACS"
        }, 
        {
            "location": "/guides/Cluster_Examples/#running-tensorflow-with-a-gpu", 
            "text": "TensorFlow has two versions of its python package:  tensorflow  and  tensorflow-gpu , but confusingly the command to use it is the same in both cases:  import tensorflow as tf   (and not  import tensorflow-gpu as tf  in case of the GPU version). This means that it really matters which package is installed in your environment.    you can control your environment using Singularity image (but the problem arises if you need a package not included in the prebuilt image, in which case you need to build the image yourself)  you can control your environment using conda environments (or virtual-env).", 
            "title": "Running TensorFlow with a GPU"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-singularity", 
            "text": "To do this, you can use the  Singularity  container manager and a Docker image containing the TensorFlow software.   Running Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script  TF_gpu.sh )  #!/bin/bash\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=TF_gpu            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=2            # Cores per task ( 1 if multithread tasks)\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --mem=16000                  # Real memory (RAM) required (MB)\n#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\nmodule load singularity/.2.5.1\n\nsrun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python   Once your job script is ready, submit it using the sbatch command:  $ sbatch TF_gpu.sh \nAlternatively, you can run Singularity interactively:  $ srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu\n\nDocker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu\nCache folder set to /home/user/.singularity/docker\nCreating container runtime...\nImporting: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz\nImporting: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz\nImporting: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz\nImporting: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz\nImporting: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz\nImporting: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz\nImporting: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz\nImporting: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz\nImporting: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz\nImporting: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz\nImporting: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz\nSingularity: Invoking an interactive shell within container...\n\nSingularity tensorflow:latest-gpu:~    Now, you're ready to execute commands:  \nSingularity tensorflow:latest-gpu:~  python -V\nPython 2.7.12\nSingularity tensorflow:latest-gpu:~  python3 -V\nPython 3.5.2  Please remember to exit from your interactive job after you are finished with your calculations.    There are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow:1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be  python3  instead of  python  in your script.", 
            "title": "Using Singularity"
        }, 
        {
            "location": "/guides/Cluster_Examples/#using-conda", 
            "text": "You can either install your own version of Anaconda in your home directory, or you can use a community module.   module use /projects/community/modulefiles  #loads community-contributed software packages\nmodule keyword anaconda                     #search packages with 'anaconda' in description   output:   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThe following modules match your search criteria:  anaconda \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n  py-data-science-stack: py-data-science-stack/5.1.0-kp807\n    Sets up anaconda 5.1.0 in your environment  So here are commands with which you can load tensorflow package:   module load py-data-science-stack/5.1.0-kp807\nconda env list                                 # be patient\nsource activate tensorflow-gpu-1.7.0           #  Note that if you try to import tensorflow on a node without a gpu, the import will fail, because it will try to load cuda driver that is not installed (because there is no GPU on the machine). \nSo you need to include this line in a slurm script, where you request a GPU resource in slurm:  #SBATCH --gres=gpu:1  and  --partition=gpu  and perhaps  --constraint=pascal . Here is an example of a slurm script that trains mnist:   #!/bin/bash\n\n#SBATCH --partition=gpu             # Partition (job queue)\n#SBATCH --no-requeue                 # Do not re-run job  if preempted\n#SBATCH --job-name=mnist_p100            # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --gres=gpu:1                 # Number of GPUs\n#SBATCH --constraint=pascal          # will look for that architecture\n#SBATCH --mem=6000                  # Real memory (RAM) required (MB)\n#SBATCH --time=03:30:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.p100_gpu1.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.p100_gpu1.err      # STDERR output file (optional)\n#SBATCH --export=ALL                 # Export you current env to the job env\n\nmodule purge\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack/5.1.0-kp807\n\nsource activate tensorflow-gpu-1.7.0\n\nexport PYTHONPATH=$PYTHONPATH:/home/kp807/tf/models1.7     #mnist.py needs a package defined in the tensorflow/models, not a standard distribution of TF. \n\nsrun python /home/kp807/tf/benchmarking/mnist.py", 
            "title": "Using conda"
        }, 
        {
            "location": "/guides/community_resources/", 
            "text": "Useful resources contributed or setup by members of the Amarel Research Computing Community for the benefit of fellow members\n\n\nCommunity-Contributed Software Modules\n\n\n\n\nUsers who wish to share software with the Amarel community of researchers can do so using the guidelines and suggestions presented here.\n\n\nLog-in to Amarel or Perceval and check out /projects/community\n\n\nHave you created a particularly fast build of your favorite simulation tool? Have you compiled a packaged that's notoriously hard to compile? Want to share some software with collaborators within the Amarel user community?\n\n\nOARC has setup a dedicated repository for community-contributed software and associated modulefiles in /projects/community. All Amarel and Perceval users can contribute software to this repository. We have established some basic guidelines to help keep things organized.\n\n\nIf you would like to see the community-contributed software packages when you run the 'module avail' or 'module spider' command, add /projects/community/modulefiles to your MODULEPATH,\n\n\nexport MODULEPATH=$MODULEPATH:/projects/community/modulefiles\n  \n\nor add that line to your ~/.bashrc file for that setting to persist.\n\n\nA Note About Governance\n\n\nPlease follow the guidelines presented here. The OARC research support team will remove or edit contributions that do not conform to these guidelines. If you find any problems or if you have questions, please let us know by sending a message to \n\n\nUser Support\n\n\nContributed software is officially unsupported. However, individual users who setup contributed software may be willing to answer basic questions about usability, performance, or selected build options. We certainly want to encourage users to contribute software, but at the same time, we do not want to burden busy researchers with support expectations. Be considerate if you contact a user about contributed software.\n\n\nPermissions\n\n\nWe've adjusted permissions (and set the sticky bit) so Amarel and Perceval users can write to \n/projects/community\n and its subdirectories.\n\n\n# chmod 777 /projects/community\n# chmod o+t /projects/community\n\n\n\n\nThis means that only a file's owner, a directory's owner, or root can rename or delete those files or directories, thus preventing others from modifying your contributions. You can adjust permissions of the files and directories you create, which may be necessary to enable others to use the software you place there.\n\n\nNaming Conventions\n\n\nNew software packages should be added using the following conventions.\n\n\n\n\n\n\n\n\nSoftware Packages\n\n\nDirectory Structure\n\n\n\n\n\n\n\n\n\n\nModulefiles\n\n\n/projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID\n\n\n\n\n\n\nCore packages\n\n\n/projects/community/pkg-name/version/NetID\n\n\n\n\n\n\nCompiler-dependent packages\n\n\n/projects/community/compiler-name/version/pkg-name/version/NetID\n\n\n\n\n\n\nCompiler-dependent packages using CUDA\n\n\n/projects/community/compiler-name/version/cuda/version/pkg-name/version/NetID\n\n\n\n\n\n\nMPI-Compiler-dependent packages\n\n\n/projects/community/compiler-name/version/mpi-name/version/pkg-name/version/NetID\n\n\n\n\n\n\nMPI-Compiler-dependent packages using CUDA\n\n\n/projects/community/compiler-name/version/mpi-name/version/cuda/version/pkg-name/version/NetID\n\n\n\n\n\n\n\n\nGood Housekeeping and Reproducibility in Research\n\n\n(1) All contributed software packages MUST have a README.OARC file created and stored in the directory containing the installed software. This file must include your contact info and any useful information about the software. This file should include all commands used to build the package since this information is important for anyone wishing to use contributed software for research purposes.\n\n\nThere is a long but useful example README.OARC file in \n/projects/community/gcc/7.3.0/gc563\n\n\n(2) All contributed software packages MUST have an associated \nLmod modulefile\n so users can easily access the software. There are details for doing this and examples below.\n\n\nAn Example: GCC-7.3.0\n\n\nMarch 23, 2018: I want to build GCC, The GNU Compiler Collection, version 7.3.0 since a project I'm working on requires it, but GCC-7.3.0 is not already setup on Amarel. I'll begin by \"bootstrapping\" using the core/default tools that come with the version of CentOS installed on Amarel. That version (at the time of this writing) is\n\n\n$ cat /etc/*release | head -n 1\nCentOS Linux release 7.4.1708 (Core)\n\n\n\n\nplus the additional components and tools that come from the Enterprise Linux repository. This version of CentOS is based on the following kernel and GCC version:\n\n\n$ cat /proc/version\nLinux version 3.10.0-693.21.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Wed Mar 7 19:03:37 UTC 2018\n\n\n\n\nInstalling GCC requires a few prerequisite packages: these must be installed and configured for use before installing GCC. Those prerequisites are: GNU Multiple Precision Library (GMP) version 4.3.2 (or later) MPFR Library version 2.4.2 (or later) MPC Library version 0.8.1 (or later) I'll download, test, and install a recent version of each of these software packages. Since I'm just trying to get setup for installing GCC here, I'll skip the description of these steps because I'll discuss those steps in detail later (below).\n\n\nwget https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2\ntar -jxf gmp-6.1.2.tar.bz2\n./configure --prefix=/projects/community/gmp/6.1.2/gc563 --enable-cxx --enable-fft\nmake -j 4\nmake check\nmake install\ncd .. ; rm -rf gmp-6.1.2*\n\n\n\n\nNote:\n the following environment settings are needed for installing the next package. These environment settings have not yet been set for my shell session, so that's why I'm not prepending them to existing path settings here:\n\n\nexport C_INCLUDE_PATH=/home/gc563/gmp/6.1.2/include\nexport CPLUS_INCLUDE_PATH=/projects/community/gmp/6.1.2/gc563/include\nexport LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib\nexport LD_LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib\n\n\n\n\nwget http://www.mpfr.org/mpfr-current/mpfr-4.0.1.tar.bz2\ntar -jxf mpfr-4.0.1.tar.bz2 \ncd mpfr-4.0.1\n./configure --prefix=/projects/community/mpfr/4.0.1/gc563 --enable-thread-safe\nmake -j 4\nmake install\ncd .. ; rm -rf mpfr-4.0.1*\n\n\n\n\nNote:\n the following environment settings have already been set, so that's why I'm prepending additional segments to existing path settings here:\n\n\nexport C_INCLUDE_PATH=/home/gc563/mpfr/4.0.1/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/projects/community/mpfr/4.0.1/gc563/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LD_LIBRARY_PATH\n\n\n\n\nwget https://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz\ntar -zxf mpc-1.1.0.tar.gz\ncd mpc-1.1.0\n./configure --prefix=/projects/community/mpc/1.1.0/gc563\nmake\nmake install\ncd .. ; rm -rf mpc-1.1.0*\n\n\n\n\nexport C_INCLUDE_PATH=/home/gc563/mpc/1.1.0/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/projects/community/mpc/1.1.0/gc563/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LD_LIBRARY_PATH\n\n\n\n\nNow, I can finally start building GCC. I'll download the latest stable supported release of GCC, open the tarball, and get ready to install it:\n\n\nwget http://mirrors.concertpass.com/gcc/releases/gcc-7.3.0/gcc-7.3.0.tar.gz\ntar -zxf gcc-7.3.0.tar.gz\ncd gcc-7.3.0\n\n\n\n\nLike the 3 prerequisite software packages I just installed, the GCC compiler suite uses the traditional configure/make/make-install installation procedure. The first step is to run the Bash script named 'configure'. The configure script inspects the existing hardware and/or software configuration of the machine where you are about to install this new software. If something is needed and missing (e.g., a needed library or access to specific tools), the configure script should let you know. The result of running this script is the creation of a file named 'Makefile' that contains the instructions for compiling your new software based on the findings from the configure script. In addition, when you run the configure script, you have the opportunity to select important options for how your new software will be setup. Most importantly, you can select where the software will be installed. This is accomplished using the '--prefix=[some location]' option. In my case, I'll specify a location in my /home directory. There are many other options and those options vary depending on what you're installing. To see a summary of the available options, use './configure --help' and for details about what those options mean, see the documentation for your new software.\n\n\n$ ./configure --prefix=/projects/community/gcc/7.3.0/gc563--with-  \nmpc=/projects/community/mpc/1.1.0/gc563 --with-\nmpfr=/projects/community/mpfr/4.0.1/gc563 --with-\ngmp=/projects/community/6.1.2/gc563 --disable-multilib\n\n\n\n\nNext, the 'make' utility will be used to compile your software. It requires the file named 'Makefile' that you created with the configure step. The Makefile indicates the sequence that an options to be used for building various components of your new software. The Makefile uses labels (i.e., names for different sections of the procedures contained therein), so entire sections of the Makefile can be skipped or used in a particular order. Running 'make' to compile your code can take a long time for some packages. You can parallelize this step to some extent using the '-j [n]' option where n is the number of tasks you wish to run simultaneously. This isn't quite the same as running a parallel or multithreaded program, but it can help get a large number of compiling tasks done in a shorter time.\n\n\nmake -j 8\n  \n\n\nFinally, I need to copy my newly created executables and/or libraries to their final destinations (the location I specified with '--prefix=' in the configure step, above). One of the labels present in the Makefile is named 'install' and I can instruct make to run the commands under that label as follows:\n\n\n make install\n cd .. ; rm -rf gcc-7.3.0*\n\n\n\n\nModulefiles for Community-Contributed Packages\n\n\nAll contributed software packages MUST have an associated \nLmod modulefile\n so users can easily access the software.\n\n\nDirectory structure for modulefiles (omit parts that aren't used, like /CUDA/version or /mpi-name/version):\n\n\n/projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID\n\n\nHere's an example Lmod modulefile for the GCC-7.3.0 example (above):\n\n\nhelp(\n[[\nThis module loads the GNU Compiler Collection version 7.3.0. \nThe GNU Compiler Collection includes front ends for C, C++, Fortran, as well as libraries for these languages.\n]])\n\nwhatis(\nDescription: GCC: the GNU Compiler Collection\n)\nwhatis(\nURL: https://gcc.gnu.org\n)\n\nconflict(\ngcc\n)\n\nload(\ngmp/6.1.2-gc563\n)\nload(\nmpfr/4.0.1-gc563\n)\nload(\nmpc/1.1.0-gc563\n)\n\nlocal base = pathJoin(\n/projects/community\n, myModuleName(), \n7.3.0\n, \ngc563\n)\nprepend_path(\nPATH\n, pathJoin(base, \nbin\n))\nprepend_path(\nC_INCLUDE_PATH\n, pathJoin(base, \ninclude\n))\nprepend_path(\nCPLUS_INCLUDE_PATH\n, pathJoin(base, \ninclude\n))\nprepend_path(\nLIBRARY_PATH\n, pathJoin(base, \nlib64\n))\nprepend_path(\nLD_LIBRARY_PATH\n, pathJoin(base, \nlib64\n))\nprepend_path(\nMANPATH\n, pathJoin(base, \nshare/man\n))\n\n\n\n\nOnce created, this file should be named \"7.3.0-gc563.lua\" because that's the name that will appear in the list of modules when a user runs the 'module avail' or 'module spider' command.\n\n\nUnderstanding this file:\n\n\nA 'conflict' can be specified to prevent loading a potentially conflicting module (e.g., loading 2 different versions of GCC at the same time)\n\n\nIn this example, 3 prerequisite modules are automatically loaded when this GCC module is loaded. Alternatively, you can simply specify a prerequisite using 'prereq(\"module/version\")' to notify a user of a prerequisite without automatically trying to load it.\n\n\nThe 'local base' statement establishes the general path for your software's various subdirectories\n\n\nThe 'prepend_path' statements define the specific path additions for those subdirectories", 
            "title": "Community"
        }, 
        {
            "location": "/guides/community_resources/#community-contributed-software-modules", 
            "text": "Users who wish to share software with the Amarel community of researchers can do so using the guidelines and suggestions presented here.", 
            "title": "Community-Contributed Software Modules"
        }, 
        {
            "location": "/guides/community_resources/#log-in-to-amarel-or-perceval-and-check-out-projectscommunity", 
            "text": "Have you created a particularly fast build of your favorite simulation tool? Have you compiled a packaged that's notoriously hard to compile? Want to share some software with collaborators within the Amarel user community?  OARC has setup a dedicated repository for community-contributed software and associated modulefiles in /projects/community. All Amarel and Perceval users can contribute software to this repository. We have established some basic guidelines to help keep things organized.  If you would like to see the community-contributed software packages when you run the 'module avail' or 'module spider' command, add /projects/community/modulefiles to your MODULEPATH,  export MODULEPATH=$MODULEPATH:/projects/community/modulefiles    \nor add that line to your ~/.bashrc file for that setting to persist.", 
            "title": "Log-in to Amarel or Perceval and check out /projects/community"
        }, 
        {
            "location": "/guides/community_resources/#a-note-about-governance", 
            "text": "Please follow the guidelines presented here. The OARC research support team will remove or edit contributions that do not conform to these guidelines. If you find any problems or if you have questions, please let us know by sending a message to", 
            "title": "A Note About Governance"
        }, 
        {
            "location": "/guides/community_resources/#user-support", 
            "text": "Contributed software is officially unsupported. However, individual users who setup contributed software may be willing to answer basic questions about usability, performance, or selected build options. We certainly want to encourage users to contribute software, but at the same time, we do not want to burden busy researchers with support expectations. Be considerate if you contact a user about contributed software.", 
            "title": "User Support"
        }, 
        {
            "location": "/guides/community_resources/#permissions", 
            "text": "We've adjusted permissions (and set the sticky bit) so Amarel and Perceval users can write to  /projects/community  and its subdirectories.  # chmod 777 /projects/community\n# chmod o+t /projects/community  This means that only a file's owner, a directory's owner, or root can rename or delete those files or directories, thus preventing others from modifying your contributions. You can adjust permissions of the files and directories you create, which may be necessary to enable others to use the software you place there.", 
            "title": "Permissions"
        }, 
        {
            "location": "/guides/community_resources/#naming-conventions", 
            "text": "New software packages should be added using the following conventions.     Software Packages  Directory Structure      Modulefiles  /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID    Core packages  /projects/community/pkg-name/version/NetID    Compiler-dependent packages  /projects/community/compiler-name/version/pkg-name/version/NetID    Compiler-dependent packages using CUDA  /projects/community/compiler-name/version/cuda/version/pkg-name/version/NetID    MPI-Compiler-dependent packages  /projects/community/compiler-name/version/mpi-name/version/pkg-name/version/NetID    MPI-Compiler-dependent packages using CUDA  /projects/community/compiler-name/version/mpi-name/version/cuda/version/pkg-name/version/NetID", 
            "title": "Naming Conventions"
        }, 
        {
            "location": "/guides/community_resources/#good-housekeeping-and-reproducibility-in-research", 
            "text": "(1) All contributed software packages MUST have a README.OARC file created and stored in the directory containing the installed software. This file must include your contact info and any useful information about the software. This file should include all commands used to build the package since this information is important for anyone wishing to use contributed software for research purposes.  There is a long but useful example README.OARC file in  /projects/community/gcc/7.3.0/gc563  (2) All contributed software packages MUST have an associated  Lmod modulefile  so users can easily access the software. There are details for doing this and examples below.", 
            "title": "Good Housekeeping and Reproducibility in Research"
        }, 
        {
            "location": "/guides/community_resources/#an-example-gcc-730", 
            "text": "March 23, 2018: I want to build GCC, The GNU Compiler Collection, version 7.3.0 since a project I'm working on requires it, but GCC-7.3.0 is not already setup on Amarel. I'll begin by \"bootstrapping\" using the core/default tools that come with the version of CentOS installed on Amarel. That version (at the time of this writing) is  $ cat /etc/*release | head -n 1\nCentOS Linux release 7.4.1708 (Core)  plus the additional components and tools that come from the Enterprise Linux repository. This version of CentOS is based on the following kernel and GCC version:  $ cat /proc/version\nLinux version 3.10.0-693.21.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Wed Mar 7 19:03:37 UTC 2018  Installing GCC requires a few prerequisite packages: these must be installed and configured for use before installing GCC. Those prerequisites are: GNU Multiple Precision Library (GMP) version 4.3.2 (or later) MPFR Library version 2.4.2 (or later) MPC Library version 0.8.1 (or later) I'll download, test, and install a recent version of each of these software packages. Since I'm just trying to get setup for installing GCC here, I'll skip the description of these steps because I'll discuss those steps in detail later (below).  wget https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2\ntar -jxf gmp-6.1.2.tar.bz2\n./configure --prefix=/projects/community/gmp/6.1.2/gc563 --enable-cxx --enable-fft\nmake -j 4\nmake check\nmake install\ncd .. ; rm -rf gmp-6.1.2*  Note:  the following environment settings are needed for installing the next package. These environment settings have not yet been set for my shell session, so that's why I'm not prepending them to existing path settings here:  export C_INCLUDE_PATH=/home/gc563/gmp/6.1.2/include\nexport CPLUS_INCLUDE_PATH=/projects/community/gmp/6.1.2/gc563/include\nexport LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib\nexport LD_LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib  wget http://www.mpfr.org/mpfr-current/mpfr-4.0.1.tar.bz2\ntar -jxf mpfr-4.0.1.tar.bz2 \ncd mpfr-4.0.1\n./configure --prefix=/projects/community/mpfr/4.0.1/gc563 --enable-thread-safe\nmake -j 4\nmake install\ncd .. ; rm -rf mpfr-4.0.1*  Note:  the following environment settings have already been set, so that's why I'm prepending additional segments to existing path settings here:  export C_INCLUDE_PATH=/home/gc563/mpfr/4.0.1/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/projects/community/mpfr/4.0.1/gc563/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LD_LIBRARY_PATH  wget https://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz\ntar -zxf mpc-1.1.0.tar.gz\ncd mpc-1.1.0\n./configure --prefix=/projects/community/mpc/1.1.0/gc563\nmake\nmake install\ncd .. ; rm -rf mpc-1.1.0*  export C_INCLUDE_PATH=/home/gc563/mpc/1.1.0/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=/projects/community/mpc/1.1.0/gc563/include:$CPLUS_INCLUDE_PATH\nexport LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LIBRARY_PATH\nexport LD_LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LD_LIBRARY_PATH  Now, I can finally start building GCC. I'll download the latest stable supported release of GCC, open the tarball, and get ready to install it:  wget http://mirrors.concertpass.com/gcc/releases/gcc-7.3.0/gcc-7.3.0.tar.gz\ntar -zxf gcc-7.3.0.tar.gz\ncd gcc-7.3.0  Like the 3 prerequisite software packages I just installed, the GCC compiler suite uses the traditional configure/make/make-install installation procedure. The first step is to run the Bash script named 'configure'. The configure script inspects the existing hardware and/or software configuration of the machine where you are about to install this new software. If something is needed and missing (e.g., a needed library or access to specific tools), the configure script should let you know. The result of running this script is the creation of a file named 'Makefile' that contains the instructions for compiling your new software based on the findings from the configure script. In addition, when you run the configure script, you have the opportunity to select important options for how your new software will be setup. Most importantly, you can select where the software will be installed. This is accomplished using the '--prefix=[some location]' option. In my case, I'll specify a location in my /home directory. There are many other options and those options vary depending on what you're installing. To see a summary of the available options, use './configure --help' and for details about what those options mean, see the documentation for your new software.  $ ./configure --prefix=/projects/community/gcc/7.3.0/gc563--with-  \nmpc=/projects/community/mpc/1.1.0/gc563 --with-\nmpfr=/projects/community/mpfr/4.0.1/gc563 --with-\ngmp=/projects/community/6.1.2/gc563 --disable-multilib  Next, the 'make' utility will be used to compile your software. It requires the file named 'Makefile' that you created with the configure step. The Makefile indicates the sequence that an options to be used for building various components of your new software. The Makefile uses labels (i.e., names for different sections of the procedures contained therein), so entire sections of the Makefile can be skipped or used in a particular order. Running 'make' to compile your code can take a long time for some packages. You can parallelize this step to some extent using the '-j [n]' option where n is the number of tasks you wish to run simultaneously. This isn't quite the same as running a parallel or multithreaded program, but it can help get a large number of compiling tasks done in a shorter time.  make -j 8     Finally, I need to copy my newly created executables and/or libraries to their final destinations (the location I specified with '--prefix=' in the configure step, above). One of the labels present in the Makefile is named 'install' and I can instruct make to run the commands under that label as follows:   make install\n cd .. ; rm -rf gcc-7.3.0*", 
            "title": "An Example: GCC-7.3.0"
        }, 
        {
            "location": "/guides/community_resources/#modulefiles-for-community-contributed-packages", 
            "text": "All contributed software packages MUST have an associated  Lmod modulefile  so users can easily access the software.  Directory structure for modulefiles (omit parts that aren't used, like /CUDA/version or /mpi-name/version):  /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID  Here's an example Lmod modulefile for the GCC-7.3.0 example (above):  help(\n[[\nThis module loads the GNU Compiler Collection version 7.3.0. \nThe GNU Compiler Collection includes front ends for C, C++, Fortran, as well as libraries for these languages.\n]])\n\nwhatis( Description: GCC: the GNU Compiler Collection )\nwhatis( URL: https://gcc.gnu.org )\n\nconflict( gcc )\n\nload( gmp/6.1.2-gc563 )\nload( mpfr/4.0.1-gc563 )\nload( mpc/1.1.0-gc563 )\n\nlocal base = pathJoin( /projects/community , myModuleName(),  7.3.0 ,  gc563 )\nprepend_path( PATH , pathJoin(base,  bin ))\nprepend_path( C_INCLUDE_PATH , pathJoin(base,  include ))\nprepend_path( CPLUS_INCLUDE_PATH , pathJoin(base,  include ))\nprepend_path( LIBRARY_PATH , pathJoin(base,  lib64 ))\nprepend_path( LD_LIBRARY_PATH , pathJoin(base,  lib64 ))\nprepend_path( MANPATH , pathJoin(base,  share/man ))  Once created, this file should be named \"7.3.0-gc563.lua\" because that's the name that will appear in the list of modules when a user runs the 'module avail' or 'module spider' command.  Understanding this file:  A 'conflict' can be specified to prevent loading a potentially conflicting module (e.g., loading 2 different versions of GCC at the same time)  In this example, 3 prerequisite modules are automatically loaded when this GCC module is loaded. Alternatively, you can simply specify a prerequisite using 'prereq(\"module/version\")' to notify a user of a prerequisite without automatically trying to load it.  The 'local base' statement establishes the general path for your software's various subdirectories  The 'prepend_path' statements define the specific path additions for those subdirectories", 
            "title": "Modulefiles for Community-Contributed Packages"
        }, 
        {
            "location": "/howtos/jupyter/", 
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \nport forwarding\n - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (\n1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \nany ip as interface\n\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u \nyour net id\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:", 
            "title": "Jupyter"
        }, 
        {
            "location": "/howtos/jupyter/#tunneling", 
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish  port forwarding  - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:", 
            "title": "Tunneling"
        }, 
        {
            "location": "/howtos/jupyter/#how-to-launch-jupyter-notebook-on-the-cluster", 
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task ( 1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means  any ip as interface \nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u  your net id  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:", 
            "title": "How to launch Jupyter notebook on the cluster"
        }, 
        {
            "location": "/howtos/fastx/", 
            "text": "Connecting to the remote Linux cluster makes running graphical programs remotely tricky, because the graphical program runs on the remote computer, yet, it must be displayed on the local machine such as laptop. In general, command line interaction with the cluster is normally preferred and even more efficient than a GUI. However, there are times when running a graphical program cannot be avoided, for example, running a debugger for a code running on the cluster. \n\n\nThere is a convenient way to run a graphical program remotely, for example, using FastX. Here is the procedure: \n\n\n\n\nGo to htts://amarel.hpc.rutgers.edu:3443  and log in (you must be either on campus, or connected through VPN)\n\n\nClick on Launch session\n\n\nClick on xterm\n\n\nRun this command that will ask the resource scheduler to put you on a compute node, rather than a login node (where you shouldn't be running intensive computations)\n\n\nsrun -p main -N 1 -c 2 --mem=4Gb -t 1:00:00 --pty /bin/bash\n\nThis command puts you on main partition, asks for 2 cores on 1 node, asks for 4 Gb or memory and time of 1 hour and runs interactive shell. After executing this, you should notice that the name of the node changed from \namarel\n to \nslepner036\n or some such. You can request whatever resources you deem necessary for your work, but keep in mind that bigger requests are typically placed further down in the queue. \n\n\nStart the program with graphical interface from the terminal window.\n\n\n\n\nHere is the video walking through these steps:", 
            "title": "FastX"
        }, 
        {
            "location": "/ressentials/", 
            "text": "To start R shell on the cluster\n\n\nSee workshop for explanation of the various options of \nsrun\n\n\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R\n\n\n\n\nPackages used from BioConductor\n\n\nIf these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: \n\n\n        source(\nhttps://bioconductor.org/biocLite.R\n) \n        biocLite(\nape\n)\n        biocLite(\nMKmisc\n)\n        biocLite(\nHeatplus\n)\n        biocLite(\naffycoretools\n)\n        biocLite(\nflashClust\n)\n        biocLite(\naffy\n)\n\n\n\n\nExample: Calculate gene length\n\n\nGet some data from ENSEMBLE\n\n\nwget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz\n\n\nIn R shell, you can execute these commands to compute gene lengths: \n\n\n\n         library(GenomicFeatures)\n         gtfdb \n- makeTxDbFromGFF(\nHomo_sapiens.GRCh38.78.gtf\n,format=\ngtf\n)\n         exons.list.per.gene \n- exonsBy(gtfdb,by=\ngene\n)\n         exonic.gene.sizes \n- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength \n-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) \n- paste('geneLength')    \n\n\n\n\nSome R essentials\n\n\nArithmetic functions\n\n\n        2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)\n\n\n\n\nCreating objects\n\n\n        ls()  #see what objects are in the workspace\n        x \n- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \nis the left side equivalent to the right side?\n\n        x + 2   #objects can be used in equations\n        y \n- \nanyname\n\n        y\n        class(x)\n        class(y)\n        ls()\n\n\n\n\nVector and Matrix\n\n\n        x1 \n- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x \n- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) \n-c(\nA\n,\nB\n)\n        rownames(x) \n-c(\nC\n,\nD\n,\nE\n,\nF\n,\nG\n)\n        x\n\n\n\n\nData Frames\n\n\n        z \n- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C\n\n\n\n\nMore ways to subset dataframes\n\n\n        z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]\n\n\n\n\nLists\n\n\n        mylist \n- list(first=z,second=x,third=c(\nW\n,\nX\n,\nY\n,\nZ\n))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)\n\n\n\n\nFunctions\n\n\n        my.add \n- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)\n\n\n\n\nVarious directory/file/library manipulations\n\n\n        library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\n????????????????\n)  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\ntest.csv\n)\n\n\n\n\nSave all the objects you have created to your workspace\n\n\n        save.image()                #creates a default file named \n.RData\n\n        save.image(\nintro.Rdata\n)   #creates a named file\n\n\n\n\nRemove objects from your workspace\n\n\n        ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\nintro.Rdata\n)\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()\n\n\n\n\nSave a history of all the commands entered\n\n\n        savehistory(\nintrohistory.Rhistory\n)", 
            "title": "R tutorial"
        }, 
        {
            "location": "/ressentials/#to-start-r-shell-on-the-cluster", 
            "text": "See workshop for explanation of the various options of  srun          module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R", 
            "title": "To start R shell on the cluster"
        }, 
        {
            "location": "/ressentials/#packages-used-from-bioconductor", 
            "text": "If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands:           source( https://bioconductor.org/biocLite.R ) \n        biocLite( ape )\n        biocLite( MKmisc )\n        biocLite( Heatplus )\n        biocLite( affycoretools )\n        biocLite( flashClust )\n        biocLite( affy )", 
            "title": "Packages used from BioConductor"
        }, 
        {
            "location": "/ressentials/#example-calculate-gene-length", 
            "text": "Get some data from ENSEMBLE  wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz  In R shell, you can execute these commands to compute gene lengths:   \n         library(GenomicFeatures)\n         gtfdb  - makeTxDbFromGFF( Homo_sapiens.GRCh38.78.gtf ,format= gtf )\n         exons.list.per.gene  - exonsBy(gtfdb,by= gene )\n         exonic.gene.sizes  - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength  -do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength)  - paste('geneLength')", 
            "title": "Example: Calculate gene length"
        }, 
        {
            "location": "/ressentials/#some-r-essentials", 
            "text": "", 
            "title": "Some R essentials"
        }, 
        {
            "location": "/ressentials/#arithmetic-functions", 
            "text": "2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)", 
            "title": "Arithmetic functions"
        }, 
        {
            "location": "/ressentials/#creating-objects", 
            "text": "ls()  #see what objects are in the workspace\n        x  - 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks  is the left side equivalent to the right side? \n        x + 2   #objects can be used in equations\n        y  -  anyname \n        y\n        class(x)\n        class(y)\n        ls()", 
            "title": "Creating objects"
        }, 
        {
            "location": "/ressentials/#vector-and-matrix", 
            "text": "x1  - c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x  - cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x)  -c( A , B )\n        rownames(x)  -c( C , D , E , F , G )\n        x", 
            "title": "Vector and Matrix"
        }, 
        {
            "location": "/ressentials/#data-frames", 
            "text": "z  - data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C", 
            "title": "Data Frames"
        }, 
        {
            "location": "/ressentials/#more-ways-to-subset-dataframes", 
            "text": "z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]", 
            "title": "More ways to subset dataframes"
        }, 
        {
            "location": "/ressentials/#lists", 
            "text": "mylist  - list(first=z,second=x,third=c( W , X , Y , Z ))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)", 
            "title": "Lists"
        }, 
        {
            "location": "/ressentials/#functions", 
            "text": "my.add  - function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)", 
            "title": "Functions"
        }, 
        {
            "location": "/ressentials/#various-directoryfilelibrary-manipulations", 
            "text": "library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd( ???????????????? )  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file= test.csv )", 
            "title": "Various directory/file/library manipulations"
        }, 
        {
            "location": "/ressentials/#save-all-the-objects-you-have-created-to-your-workspace", 
            "text": "save.image()                #creates a default file named  .RData \n        save.image( intro.Rdata )   #creates a named file", 
            "title": "Save all the objects you have created to your workspace"
        }, 
        {
            "location": "/ressentials/#remove-objects-from-your-workspace", 
            "text": "ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load( intro.Rdata )\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()", 
            "title": "Remove objects from your workspace"
        }, 
        {
            "location": "/ressentials/#save-a-history-of-all-the-commands-entered", 
            "text": "savehistory( introhistory.Rhistory )", 
            "title": "Save a history of all the commands entered"
        }, 
        {
            "location": "/CheatSheet/", 
            "text": "Bash cheatsheet (command line):\n\n\n\n\n\n\n\n\ncommand\n\n\ndescription\n\n\nusage example\n\n\n\n\n\n\n\n\n\n\nls -ltra mydir\n\n\nlist all files in mydir in descending order of creation, with permissions\n\n\nls -ltra .\n\n\n\n\n\n\ndu\n\n\ndisk usage, e.g. how much space does your directory occupy, -h human-readable\n\n\ndu -h mydir\n\n\n\n\n\n\nwhich \n\n\nsee where command is installed\n\n\nwhich python\n\n\n\n\n\n\npwd\n\n\nwhich directory I'm in\n\n\npwd\n\n\n\n\n\n\nman \n\n\nmanual page for command\n\n\nman cut\n\n\n\n\n\n\ngrep \n\n\nfilter for lines which fit pattern\n\n\ncat myfile \n grep GATK\n\n\n\n\n\n\ncut -d\n -f\n\n\nsplit line by delimiter and get field number 3\n\n\ncat myfile \n cut -d'_' -f3\n\n\n\n\n\n\nsort \n\n\nsort lines, often used with \nuniq\n\n\nsort myfile \n uniq\n\n\n\n\n\n\nuniq\n\n\nsuppress repeated lines, works only if sorted\n\n\nsee above example\n\n\n\n\n\n\nless\n\n\npaginated output\n\n\nless myfile\n\n\n\n\n\n\n\n\nredirect output (e.g. list files and save filenames in aaa.txt)\n\n\nls \n aaa.txt\n\n\n\n\n\n\n\n\nappend output to existing file\n\n\necho \"blah\" \n aaa.txt\n\n\n\n\n\n\nfind\n\n\nfind files with some properties e.g. display all files recursively from current directory\n\n\nfind .\n\n\n\n\n\n\nchmod\n\n\nchange permissions on a file or directory, eg. make myscript.sh executable for user\n\n\nchmod u+x myscript.sh\n\n\n\n\n\n\ntop\n\n\ndisplay most intensive processes\n\n\ntop\n\n\n\n\n\n\nps auxw\n\n\nlist processes\n\n\nps auxw\n\n\n\n\n\n\ntime myscript.sh\n\n\nmeasure how much time does myscript.sh take to finish execution\n\n\n\n\n\n\n\n\necho\n $variable\n\n\noutput the value of the variable\n\n\necho $USER\n\n\n\n\n\n\n\n\nFor a more complete cheatsheet including ifs, loops and functions, see \nthis website\n\nHere is a sample: \n\n\ndirname /home/kp807/projects/cluster_reports/cluster.csv       # everything but the last part\nbasename /home/kp807/projects/cluster_reports/cluster.csv      # last part of path\nCURRENT_DIR=`pwd` ; echo $CURRENT_DIR                          # backtick for execution of bash command\necho 'projects_cluster_file.csv' | cut -d '_' -f1              # split name to retain a part of file\necho $((1 + 2))                                                # double parenthesis for arithmetic expressions\necho filename_fly{5..10}.csv                                   # list comprehensions   {start..end}\n\n# for-loop: \nfor file in filename_fly{5..10}.csv; do  echo $file ; done     \n\n# if-statement - 2 examples\nif [ 1 -gt 2 ]; then  echo '1 \n 2' ; else echo '1\n2' ; fi      \nif [ -d \nnewdir\n ]; then  echo 'directory exists' ; else echo 'this directory doesnt exist' ; fi                        \n#variable assignment\na=10; echo $a        #good - no spaces\nb = 10; echo $b      #bad - spaces around =\n\n\n\n\nEnvironment modules (lmod) cheatsheet\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmodule avail\n\n\nshow a list of the modules available\n\n\n\n\n\n\nmodule load java\n\n\nloads the named software module i.e. java\n\n\n\n\n\n\nmodule use /projects/community/modulefiles\n\n\nNow if you do module avail, it also shows modules created by other users\n\n\n\n\n\n\nmodule spider\n\n\nshows a comprehensive list of all available modules or type name after spider to show details about specific module\n\n\n\n\n\n\nmodule keyword anaconda\n\n\nlooks for the keyword \nanaconda\n in the description of a module\n\n\n\n\n\n\nmodule purge\n\n\nremoves all loaded module\n\n\n\n\n\n\nml\n\n\nlists modules loaded\n\n\n\n\n\n\n\n\nTo learm more about environments and modules, see \nlmod documenation\n. \n\n\nSlurm cheatsheet\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsinfo\n\n\ninfo about utilization and resources\n\n\n\n\n\n\nsbatch my_slurm_script.sh\n\n\nsubmit a slurm script\n\n\n\n\n\n\nsrun -N 1 -c 2 -t 1:00:00 --pty bash\n\n\nrun interactive shell on a compute node for 1 hour\n\n\n\n\n\n\nscancel  123456\n\n\ncancel job 123456 that's currently running or queued\n\n\n\n\n\n\nsacct -u kp807 --start=2018-05-02\n\n\nshow all jobs by user kp807 since some date\n\n\n\n\n\n\nscontrol show job 123456 -dd\n\n\ndetails about job that's running\n\n\n\n\n\n\nscontrol show node hal0001\n\n\ndetails about node \nha0001\n\n\n\n\n\n\nscontrol --help\n\n\ngeneric way to invoke quick help on any command\n\n\n\n\n\n\nsacct -o MaxRSS,job,jobName,state\n\n\nkeep track of how much memory you are using\n\n\n\n\n\n\nsacct -o Elapsed,Start,End,job,jobName\n\n\nestimate the wall time\n\n\n\n\n\n\n\n\nTo learn more about slurm, see \nslurm documentation\n\n\nCheck quota cheatsheet\n\n\nNOTE: \n/home\n and \n/scratch\n are two different filesystems, with different backup policies. \n\n\n\n\n\n\n\n\nCommand\n\n\nDescription of which file usage\n\n\n\n\n\n\n\n\n\n\nmmlsquota scratch --block-size=auto\n\n\nfile usage on \n/scratch/netid\n for the user's netid\n\n\n\n\n\n\nmmlsquota home --block-size=auto\n\n\nfile usage on \n/home/netid\n for the user's netid\n\n\n\n\n\n\nmmlsquota home:foran --block-size=auto\n\n\nfile usage in the shared folder foran (/projects/foran)\n\n\n\n\n\n\nmmlsquota -j foran home  --block-size=auto\n\n\nquota and usage of the whole fileset foran\n\n\n\n\n\n\ndu -hs /directory/to/query/*\n\n\nhuman-readable sizes of all 1st-level subdirectories of \n/directory/to/query/\n\n\n\n\n\n\n\n\nLinux basic commands\n\n\nFile tree\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nls\n\n\nlist files and directories\n\n\n\n\n\n\nls -a\n\n\nlist all files and directories\n\n\n\n\n\n\nmkdir\n\n\nmake a directory\n\n\n\n\n\n\ncd directory\n\n\nchange to named directory\n\n\n\n\n\n\ncd\n\n\nchange to home-directory\n\n\n\n\n\n\ncd ~\n\n\nchange to home-directory\n\n\n\n\n\n\ncd ..\n\n\nchange to parent directory\n\n\n\n\n\n\npwd\n\n\ndisplay the path of the current directory\n\n\n\n\n\n\n\n\nMoving and viewing files\n\n\n\n\n\n\n\n\nCommand\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\ncp file1 file2\n\n\ncopy file1 and call it file2\n\n\n\n\n\n\nmv file1 file2\n\n\nmove or rename file1 to file2\n\n\n\n\n\n\nrm file\n\n\nremove a file\n\n\n\n\n\n\nrmdir directory\n\n\nremove a directory (only if empty)\n\n\n\n\n\n\ncat file\n\n\ndisplay a file\n\n\n\n\n\n\nless file\n\n\ndisplay a file a page at a time\n\n\n\n\n\n\nhead -19 file\n\n\ndisplay the first 19 lines of a file\n\n\n\n\n\n\ntail -19 file\n\n\ndisplay the last 19 lines of a file\n\n\n\n\n\n\ngrep Finally myfile.txt\n\n\nsearch myfile.txt for word Finally\n\n\n\n\n\n\nwc file\n\n\ncount number of lines/words/characters in file\n\n\n\n\n\n\n\n\nPiping\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncommand \n file\n\n\nredirect standard output to a file\n\n\n\n\n\n\ncommand \n file\n\n\nappend standard output to a file\n\n\n\n\n\n\ncommand \n file\n\n\nredirect standard input from a file\n\n\n\n\n\n\ncommand1 \n command2\n\n\npipe the output of command1 to the input of command2\n\n\n\n\n\n\ncat file1 file2 \n file0\n\n\nconcatenate file1 and file2 to file0\n\n\n\n\n\n\nsort\n\n\nsort data\n\n\n\n\n\n\nwho\n\n\nlist users currently logged in\n\n\n\n\n\n\n*\n\n\nmatch any number of characters\n\n\n\n\n\n\n?\n\n\nmatch one character\n\n\n\n\n\n\nman command\n\n\nread the online manual page for a command\n\n\n\n\n\n\nwhatis command\n\n\nbrief description of a command\n\n\n\n\n\n\napropos keyword\n\n\nmatch commands with keyword in their man pages\n\n\n\n\n\n\n\n\nPermissions\n\n\n\n\n\n\n\n\nSymbol\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nu\n\n\nuser\n\n\n\n\n\n\ng\n\n\ngroup\n\n\n\n\n\n\no\n\n\nother\n\n\n\n\n\n\na\n\n\nall\n\n\n\n\n\n\nr\n\n\nread\n\n\n\n\n\n\nw\n\n\nwrite (and delete)\n\n\n\n\n\n\nx\n\n\nexecute (and access directory)\n\n\n\n\n\n\nu+x\n\n\nadd executing permission for user\n\n\n\n\n\n\nu-x\n\n\ntake away executing permission\n\n\n\n\n\n\n\n\nOutput of \nls -ltra\n\n\ndrwxr-x--x  12 kp807 kp807      4096 Jun 26 23:33 .rstudio\n\n\n\n\nd = it's a directory\n\n\nrwx = first group of permissions, readable, writeable and executable by user\n\n\nr-x = second group of permissions, readable and executable by group, but not writeable by group\n\n\n--x = third group of permissions, executable by others, but not readable and writeable by others\n\n\n.rstudio = hidden directory\n\n\n\n\nLinux on the web\n\n\n\n\nGalen's tutorial\n\n\nExcellent short lessons\n\n\nAlexei's course he runs for engineering students every semester\n\n\nCornell virtual workshop\n - interactive if you have xsede login; many HPC topics available\n\n\nSoftware carpentry\n - list of introductory programming courses", 
            "title": "Cheat Sheets"
        }, 
        {
            "location": "/CheatSheet/#bash-cheatsheet-command-line", 
            "text": "command  description  usage example      ls -ltra mydir  list all files in mydir in descending order of creation, with permissions  ls -ltra .    du  disk usage, e.g. how much space does your directory occupy, -h human-readable  du -h mydir    which   see where command is installed  which python    pwd  which directory I'm in  pwd    man   manual page for command  man cut    grep   filter for lines which fit pattern  cat myfile   grep GATK    cut -d  -f  split line by delimiter and get field number 3  cat myfile   cut -d'_' -f3    sort   sort lines, often used with  uniq  sort myfile   uniq    uniq  suppress repeated lines, works only if sorted  see above example    less  paginated output  less myfile     redirect output (e.g. list files and save filenames in aaa.txt)  ls   aaa.txt     append output to existing file  echo \"blah\"   aaa.txt    find  find files with some properties e.g. display all files recursively from current directory  find .    chmod  change permissions on a file or directory, eg. make myscript.sh executable for user  chmod u+x myscript.sh    top  display most intensive processes  top    ps auxw  list processes  ps auxw    time myscript.sh  measure how much time does myscript.sh take to finish execution     echo  $variable  output the value of the variable  echo $USER     For a more complete cheatsheet including ifs, loops and functions, see  this website \nHere is a sample:   dirname /home/kp807/projects/cluster_reports/cluster.csv       # everything but the last part\nbasename /home/kp807/projects/cluster_reports/cluster.csv      # last part of path\nCURRENT_DIR=`pwd` ; echo $CURRENT_DIR                          # backtick for execution of bash command\necho 'projects_cluster_file.csv' | cut -d '_' -f1              # split name to retain a part of file\necho $((1 + 2))                                                # double parenthesis for arithmetic expressions\necho filename_fly{5..10}.csv                                   # list comprehensions   {start..end}\n\n# for-loop: \nfor file in filename_fly{5..10}.csv; do  echo $file ; done     \n\n# if-statement - 2 examples\nif [ 1 -gt 2 ]; then  echo '1   2' ; else echo '1 2' ; fi      \nif [ -d  newdir  ]; then  echo 'directory exists' ; else echo 'this directory doesnt exist' ; fi                        \n#variable assignment\na=10; echo $a        #good - no spaces\nb = 10; echo $b      #bad - spaces around =", 
            "title": "Bash cheatsheet (command line):"
        }, 
        {
            "location": "/CheatSheet/#environment-modules-lmod-cheatsheet", 
            "text": "Command  Description      module avail  show a list of the modules available    module load java  loads the named software module i.e. java    module use /projects/community/modulefiles  Now if you do module avail, it also shows modules created by other users    module spider  shows a comprehensive list of all available modules or type name after spider to show details about specific module    module keyword anaconda  looks for the keyword  anaconda  in the description of a module    module purge  removes all loaded module    ml  lists modules loaded     To learm more about environments and modules, see  lmod documenation .", 
            "title": "Environment modules (lmod) cheatsheet"
        }, 
        {
            "location": "/CheatSheet/#slurm-cheatsheet", 
            "text": "Command  Description      sinfo  info about utilization and resources    sbatch my_slurm_script.sh  submit a slurm script    srun -N 1 -c 2 -t 1:00:00 --pty bash  run interactive shell on a compute node for 1 hour    scancel  123456  cancel job 123456 that's currently running or queued    sacct -u kp807 --start=2018-05-02  show all jobs by user kp807 since some date    scontrol show job 123456 -dd  details about job that's running    scontrol show node hal0001  details about node  ha0001    scontrol --help  generic way to invoke quick help on any command    sacct -o MaxRSS,job,jobName,state  keep track of how much memory you are using    sacct -o Elapsed,Start,End,job,jobName  estimate the wall time     To learn more about slurm, see  slurm documentation", 
            "title": "Slurm cheatsheet"
        }, 
        {
            "location": "/CheatSheet/#check-quota-cheatsheet", 
            "text": "NOTE:  /home  and  /scratch  are two different filesystems, with different backup policies.      Command  Description of which file usage      mmlsquota scratch --block-size=auto  file usage on  /scratch/netid  for the user's netid    mmlsquota home --block-size=auto  file usage on  /home/netid  for the user's netid    mmlsquota home:foran --block-size=auto  file usage in the shared folder foran (/projects/foran)    mmlsquota -j foran home  --block-size=auto  quota and usage of the whole fileset foran    du -hs /directory/to/query/*  human-readable sizes of all 1st-level subdirectories of  /directory/to/query/", 
            "title": "Check quota cheatsheet"
        }, 
        {
            "location": "/CheatSheet/#linux-basic-commands", 
            "text": "", 
            "title": "Linux basic commands"
        }, 
        {
            "location": "/CheatSheet/#file-tree", 
            "text": "Command  Meaning      ls  list files and directories    ls -a  list all files and directories    mkdir  make a directory    cd directory  change to named directory    cd  change to home-directory    cd ~  change to home-directory    cd ..  change to parent directory    pwd  display the path of the current directory", 
            "title": "File tree"
        }, 
        {
            "location": "/CheatSheet/#moving-and-viewing-files", 
            "text": "Command  Meaning      cp file1 file2  copy file1 and call it file2    mv file1 file2  move or rename file1 to file2    rm file  remove a file    rmdir directory  remove a directory (only if empty)    cat file  display a file    less file  display a file a page at a time    head -19 file  display the first 19 lines of a file    tail -19 file  display the last 19 lines of a file    grep Finally myfile.txt  search myfile.txt for word Finally    wc file  count number of lines/words/characters in file", 
            "title": "Moving and viewing files"
        }, 
        {
            "location": "/CheatSheet/#piping", 
            "text": "Command  Description      command   file  redirect standard output to a file    command   file  append standard output to a file    command   file  redirect standard input from a file    command1   command2  pipe the output of command1 to the input of command2    cat file1 file2   file0  concatenate file1 and file2 to file0    sort  sort data    who  list users currently logged in    *  match any number of characters    ?  match one character    man command  read the online manual page for a command    whatis command  brief description of a command    apropos keyword  match commands with keyword in their man pages", 
            "title": "Piping"
        }, 
        {
            "location": "/CheatSheet/#permissions", 
            "text": "Symbol  Description      u  user    g  group    o  other    a  all    r  read    w  write (and delete)    x  execute (and access directory)    u+x  add executing permission for user    u-x  take away executing permission     Output of  ls -ltra  drwxr-x--x  12 kp807 kp807      4096 Jun 26 23:33 .rstudio   d = it's a directory  rwx = first group of permissions, readable, writeable and executable by user  r-x = second group of permissions, readable and executable by group, but not writeable by group  --x = third group of permissions, executable by others, but not readable and writeable by others  .rstudio = hidden directory", 
            "title": "Permissions"
        }, 
        {
            "location": "/CheatSheet/#linux-on-the-web", 
            "text": "Galen's tutorial  Excellent short lessons  Alexei's course he runs for engineering students every semester  Cornell virtual workshop  - interactive if you have xsede login; many HPC topics available  Software carpentry  - list of introductory programming courses", 
            "title": "Linux on the web"
        }, 
        {
            "location": "/resources/", 
            "text": "Code Examples\n\n\n\n\nOARC-provided slurm examples\n - collection of slurm batch scripts that can serve as templates for your jobs\n\n\n\n\nServices\n\n\n\n\nRutgers free web pages\n - WordPress-based websites hosting, free for Rutgers community\n\n\n\n\nTools\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown\n\n\n\n\nEducation\n\n\n\n\nOARC events\n - OARC holds a number of workshops on Linux, HPC, genomics and others\n\n\nCornell virtual workshops\n - A number of courses designed to get you up to speed using HPC and programming. \n\n\nSoftware Carpentry\n - Software Carpentry has a number of lectures and workshops on many computing subjects", 
            "title": "Resources"
        }, 
        {
            "location": "/resources/#code-examples", 
            "text": "OARC-provided slurm examples  - collection of slurm batch scripts that can serve as templates for your jobs", 
            "title": "Code Examples"
        }, 
        {
            "location": "/resources/#services", 
            "text": "Rutgers free web pages  - WordPress-based websites hosting, free for Rutgers community", 
            "title": "Services"
        }, 
        {
            "location": "/resources/#tools", 
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown", 
            "title": "Tools"
        }, 
        {
            "location": "/resources/#education", 
            "text": "OARC events  - OARC holds a number of workshops on Linux, HPC, genomics and others  Cornell virtual workshops  - A number of courses designed to get you up to speed using HPC and programming.   Software Carpentry  - Software Carpentry has a number of lectures and workshops on many computing subjects", 
            "title": "Education"
        }
    ]
}