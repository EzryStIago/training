{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is university-wide initiative that is to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to get access or become owners, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \nAmarel user guide\n as it has slurm tips and examples. \n\n\nWARNING - READ!\n\n\n\n\nDo not run large computational jobs on the \nlogin\n node. Use slurm to allocate resources on the compute node. \n\n\nTODO: add more complete list\n\n\n\n\nLearning paths\n\n\n\n\nFor users familiar with Linux but new to \nslurm\n, follow \nthis path\n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Home"
        },
        {
            "location": "/#warning-read",
            "text": "Do not run large computational jobs on the  login  node. Use slurm to allocate resources on the compute node.   TODO: add more complete list",
            "title": "WARNING - READ!"
        },
        {
            "location": "/#learning-paths",
            "text": "For users familiar with Linux but new to  slurm , follow  this path  For users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Learning paths"
        },
        {
            "location": "/howtos/jupyter/",
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u <your net id>\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:",
            "title": "Jupyter"
        },
        {
            "location": "/howtos/jupyter/#tunneling",
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:",
            "title": "Tunneling"
        },
        {
            "location": "/howtos/jupyter/#how-to-launch-jupyter-notebook-on-the-cluster",
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u <your net id>  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:",
            "title": "How to launch Jupyter notebook on the cluster"
        },
        {
            "location": "/howtos/fastx/",
            "text": "Connecting to the remote Linux cluster makes running graphical programs remotely tricky, because the graphical program runs on the remote computer, yet, it must be displayed on the local machine such as laptop. In general, command line interaction with the cluster is normally preferred and even more efficient than a GUI. However, there are times when running a graphical program cannot be avoided, for example, running a debugger for a code running on the cluster. \n\n\nThere is a convenient way to run a graphical program remotely, for example, using FastX. Here is the procedure: \n\n\n\n\nGo to htts://amarel.hpc.rutgers.edu:3443  and log in (you must be either on campus, or connected through VPN)\n\n\nClick on Launch session\n\n\nClick on xterm\n\n\nRun this command that will ask the resource scheduler to put you on a compute node, rather than a login node (where you shouldn't be running intensive computations)\n\n\nsrun -p main -N 1 -c 2 --mem=4Gb -t 1:00:00 --pty /bin/bash\n\nThis command puts you on main partition, asks for 2 cores on 1 node, asks for 4 Gb or memory and time of 1 hour and runs interactive shell. After executing this, you should notice that the name of the node changed from \namarel\n to \nslepner036\n or some such. You can request whatever resources you deem necessary for your work, but keep in mind that bigger requests are typically placed further down in the queue. \n\n\nStart the program with graphical interface from the terminal window.\n\n\n\n\nHere is the video walking through these steps:",
            "title": "FastX"
        },
        {
            "location": "/workshops/workshop/",
            "text": "Overview\n\n\nThese are notes from the workshop on genomics. What's covered in these notes: \n1.vk1 Setup\n\n\nGenomic Software\n\n\nThis is a list of software to install for the workshop\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nSeqtk\n\n\nvery handy and fast for processing fastq/a files\n\n\nlink\n\n\n\n\n\n\nsratoolkit\n\n\ndownloading and processing data from GEO/SRA database\n\n\nlink\n\n\n\n\n\n\nhtseq-count\n\n\ncounting the  reads mapped on to genomics feature\n\n\nlink\n\n\n\n\n\n\nfastQC\n\n\nwidely used for sequencing read QC\n\n\nlink\n\n\n\n\n\n\nRSeQC-2.6.4\n\n\nAn RNA-seq quality control package, multiple functions\n\n\npython package\n\n\n\n\n\n\ntrimmomatic\n\n\nfastq quality trim and adaptor removal\n\n\nlink\n\n\n\n\n\n\n\n\nThis is a list of software already available on the cluster and the command you need to execute to load it in your environment: \n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nload it on the cluster\n\n\n\n\n\n\n\n\n\n\nSamtools\n\n\n\n\nmodule load samtools\n\n\n\n\n\n\nBedtools\n\n\n\n\nmodule load bedtools2./2.25.0\n\n\n\n\n\n\nbowtie2\n\n\nalignment software\n\n\nmodule load bowtie2\n\n\n\n\n\n\ntophat2\n\n\nalignment software\n\n\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n\n\n\n\n\n\nR\n\n\nlanguage for statistical analysis\n\n\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\n\n\n\n\nThis is a list of other software you might find useful:\n\n\n\n\n\n\n\n\nsoftware\n\n\ndescription\n\n\nlink\n\n\n\n\n\n\n\n\n\n\nGSEA\n\n\ngenome set enrichment analysis\n\n\nlink\n\n\n\n\n\n\nIGV\n\n\nInteractive Genome Viewer\n\n\nlink\n\n\n\n\n\n\nCytoscape\n\n\nNetwork visualization softwar\n\n\nlink\n\n\n\n\n\n\n\n\nSetup\n\n\nConnect to the cluster login node\n\n\nThe preferred method to connect to the cluster is through a web browser and fastX client\n - \nvia FastX\n: in your browser, go to `https://amarel.hpc.rutgers.edu:3443\nIf the above method doesn't work you may still connect \n\n\n\n\nvia a terminal\n: if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as \nmobaXterm\n [link] (https://mobaxterm.mobatek.net/). Then from your terminal connect to the cluster by executing the following command: \n\n\nssh -X <your net id>@amarel.hpc.rutgers.edu\n   \n\n\n\n\nGet resources on the compute node\n\n\nYou get to the cluster to execute your computations by running the following command in your terminal:  \n\n\nsrun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash\n \n\nNotice that the name in your terminal will change from \namarel\n to node name like \nhal0025\n or \nslepner086\n. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command: \n\n\n\n\n\n\n\n\ncommand part\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\nsrun\n\n\nslurm\n run, i.e. allocate resources and run via \nslurm\n scheduler\n\n\n\n\n\n\n-p main\n\n\non the main partition, one of several queues on the cluster\n\n\n\n\n\n\n--reservation=genomics\n\n\nwe reserved some compute nodes for this workshop to not wait long for resources\n\n\n\n\n\n\n-N 1\n\n\nask for one node\n\n\n\n\n\n\n-c 2\n\n\nask for two cores\n\n\n\n\n\n\n-n 1\n\n\nthis will be 1 most times\n\n\n\n\n\n\n-t 01:40:00\n\n\nrun this for a maximum time of 1 hour 40 minutes\n\n\n\n\n\n\n--pty /bin/bash\n\n\nrun the terminal shell in an interactive mode\n\n\n\n\n\n\n\n\nPrepare some directories for the data\n\n\nYou have two main spaces on the Amarel cluster. These are: \n\n\n\n\nyour home directory - \n/home/netid/\n  (e.g. \n/home/kp807/\n for my netid) -\n\n\nyour scratch directory - \n/scratch/netid/\n\n\n\n\nThey differ in how often they are backed up and by size (100Gb for \n/home\n and 500Gb for \n/scratch\n). So we will install programs in \n/home\n, while the data and output will be in \n/scratch\n. Execute these commands: \n\n\n                cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here\n\n\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nInstall programs\n\n\nEach program will have slightly different installation instructions. Here is a handy sequence of commands that will install them: \n\n\n##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                    \n\n\n\n\n\nMoreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e. \nsamtools\n command will work if you execute \nmodule load samtools\n): \n\n\nmodule load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1\n\n\n\n\nDownload data\n\n\nWe will download human RNA-seq data with \nGEO accession GSE52778\n. The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:\n\n\n                mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated\n\n\n\n\nWe will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored. \nsratoolkit\n will be in your home directory, under \nPrograms\n, and the \nvdb-config\n might be under the \nbin\n directory. You will enter \n/scratch/your_netID/Genomics_Workshop/download\n for the path - NOTE you have to replace \nyour_netID\n with your true netId, e.g. \nkp807\n. Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them. \n\n\n                vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y\n\n\n\n\nThen execute the following commands to get the data. Both \nprefetch\n and \nfastq-dump\n are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]\n\n\nprefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ???? \n\n\n\n\nYou have to pay attention to where you are putting your data. So these two commands will actually be several: \n\n\n                cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516\n\n\n\n\nThe commands above showed how to do it for one sample. You need to do it for 6 samples total. \n\n\n                SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)\n\n\n\n\nRunning bioinformatics jobs\n\n\nFastQC - raw data QC\n\n\nExplain what is fastqc is doing here - TODO\n\n\n        cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq\n\n\n\n\nFastQC produces an html page as output, \nfastqc/SRR1039508_1_fastqc.html\n, with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf\n\n\nTrimmomatic - quality trim/adaptor removal\n\n\n    ##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n\n\n\nNOTE:\n  the above is a one line command, illustrated as the following:\n\n\n        java -jar trimmomatic-0.36.jar PE \\\n        -phred33 -trimlog trim.log \\\n        input_1.fq  input_2.fq \\\n        output_1_paired.fq  output_1_unpaired.fq \\\n        output_2_paired.fq  output_2_unpaired.fq \\\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 / \n\n        ## Once it started run, you shall see the following:\n        TrimmomaticPE: Started with arguments:\n        -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n        Multiple cores found: Using 2 threads\n        Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n        ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n        Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%)\n        TrimmomaticPE: Completed successfully\n\n        ##view the output, the trim.log file, .e.g.  length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained)\n\n   ##you may also try fastx_quality_stats from the FASTX\u2014toolkit\n\n\n\n\n\nFastQC - Run on cleaned reads, compare result\n\n\n     module load java\n     fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n\n     ## /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf , helpful in viewing and interpreting the output\n\n\n\n\nDownload reference and reference indexing\n\n\nHuman genome indexing will take hours, we have the reference pre-prepared. Stored at  \n/projects/oarc/Genomics_Workshop/Reference/\n\nFor in class practice, we will do this on E.coli genome\n\n\n       cd /scratch/$USER/Genomics_Workshop/\n        mkdir Reference\n        cd Reference\n\n        wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz\n\n        gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz\n        module load bowtie2\n        bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic\n\n###if download from ENSEMBLE\n        wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-38/fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/dna/Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.dna.toplevel.fa.gz\n\n\n\n\nMapping with tophat2, (STAR, HISAT2)\n\n\nNow, go to your data folder\n        \ncd  /scratch/$USER/Genomics_Workshop/untreated\n\n\ncd  /scratch/$USER/Genomics_Workshop/untreated\n        module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n        module load samtools   #bowtie2 is loaded already\n        mkdir tophat_out\n        tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039508_1\n        0k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/  hg20_transciptome/GR\n        Ch38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel\n        SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n## you shall modify the -p value to be consistent with the -c value you requested in the beginning\n\n\n\n\nYou shall see something like:\n\n\n   [2018-03-30 11:48:57] Beginning TopHat run (v2.1.0)\n-----------------------------------------------\n[2018-03-30 11:48:57] Checking for Bowtie\n                  Bowtie version:        2.2.9.0\n[2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome)..\n[2018-03-30 11:48:58] Checking for Bowtie index files (genome)..\n[2018-03-30 11:48:58] Checking for reference FASTA file\n[2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen\nce/hg20/Homo_sapiens.GRCh38.dna.toplevel\n[2018-03-30 11:49:09] Reading known junctions from GTF file\n[2018-03-30 11:49:27] Preparing reads\n         left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded)\n        right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded)\n[2018-03-30 11:49:29] Using pre-built transcriptome data..\n[2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads\n[2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t\noplevel with Bowtie2\n[2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n[2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2\n[2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\n\n\n\n\nThe transcriptome index was built  by pointing to gtf file first,  here we have it prepared already, just so we can save time.  The following would be the command to generate the transcriptome index while running tophat alignment.\n\n\n   tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039516 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/ Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039516_1.fastq.pairedOut.fastq SRR1039516_2.fastq.pairedOut.fastq\n\n\n\n\n\nThe output folder \ntophat_out/untreated_SRR1039508/\n shall contain the following files/folder (in blue): \n\n\n     cd  /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508\n        $ ll\n   total 2183632\n -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam\n -rw-rw-r-- 1 yc759 oarc        565 Jan 16 21:57 align_summary.txt\n -rw-rw-r-- 1 yc759 oarc    1921529 Jan 16 21:57 deletions.bed\n -rw-rw-r-- 1 yc759 oarc    2239884 Jan 16 21:57 insertions.bed\n -rw-rw-r-- 1 yc759 oarc   14181618 Jan 16 21:57 junctions.bed\n drwxrwsr-x 2 yc759 oarc       4096 Jan 16 21:57 logs\n -rw-rw-r-- 1 yc759 oarc        184 Jan 16 21:57 prep_reads.info\n -rw-rw-r-- 1 yc759 oarc   42846571 Jan 16 21:57 unmapped.bam\n\n\n\n\nRead counts using htseq-count\n\n\nGO TO WHERE YOUR ALIGNMENT OUTPUT FOLDER IS, FOR EXAMPLE: \n\n\n  cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n  ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.bam accepted_hits.bam   \n\n  ##make a soft link to the full bam file we already prepared, if you didn\u2019t have the bam ready yet\n\n     module load samtools intel/17.0.2 python/2.7.12 \n     samtools sort -n  accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name  -/projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf > untreated08.txt\n\n\n\n\nUse samtools to sort the bam file by name:  because htseq-count accepts bam file sorted by name as default, but tophat generates bam sorted by coordinates by default\nThe same way to generate the counts file \nuntreated12.txt\n, \nuntreated16.txt\n,\ndex09.txt\n, \ndex13.txt\n, \ndex17.txt\n\n\nPerform Mapping QC using RSeQC\n\n\nNow,  quality control using RSeQC \u2013a few examples here, please go to the website for more functions \nhttp://rseqc.sourceforge.net/\n\n\n  cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n        module load python/2.7.12\n        module load intel/17.0.4\n\n        $ read_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/\nReference/Homo_sapiens.GRCh38.79.bed\nprocessing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n        processing accepted_hits.bam ... Finished\n\n        Total Reads                   43474036\n        Total Tags                    54438789\n        Total Assigned Tags           53991382\n        =====================================================================\n        Group               Total_bases         Tag_count           Tags/Kb\n        CDS_Exons           103371993           43264842            418.54\n        5'UTR_Exons         5217678             583447              111.82\n        3'UTR_Exons         29324747            8145122             277.76\n        Introns             1500197093          1805034             1.20\n        TSS_up_1kb          33306654            18893               0.57\n        TSS_up_5kb          148463534           41165               0.28\n        TSS_up_10kb         265823549           55644               0.21\n        TES_down_1kb        35215293            50954               1.45\n        TES_down_5kb        152556214           113325              0.74\n        TES_down_10kb       268614580           137293              0.51\n        =====================================================================\n\n $ bam_stat.py -i accepted_hits.bam\n        Load BAM file ...\n\n        Done\n\n        #==================================================\n        #All numbers are READ count\n        #==================================================\n\n        Total records:                          52528699\n\n        QC failed:                              0\n        Optical/PCR duplicate:                  0\n        Non primary hits                        9054663\n        Unmapped reads:                         0\n        mapq < mapq_cut (non-unique):           2684801\n\n        mapq >= mapq_cut (unique):              40789235\n        Read-1:                                 20414530\n        Read-2:                                 20374705\n        Reads map to '+':                       20393901\n        Reads map to '-':                       20395334\n        Non-splice reads:                       30860931\n        Splice reads:                           9928304\n        Reads mapped in proper pairs:           32386536\n        Proper-paired reads map to different chrom:312\n\n\n\n\n\nThe script does genebody coverage calculation requires the input bam files to be sorted and indexed (we will do it using samtools). The calculation and plot will require R\nGo to one of the tophat_out sample folder\n\n\n$ module load intel/17.0.4  R-Project/3.4.1    \n        $ module load samtools\n        $ samtools sort accepted_hits.bam \u2013o accepted_hits.sorted.bam \n        ##this may take a while, you may use the one already prepared for you by making a soft link\n        ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam\n\n\n        $ samtools index accepted_hits.sorted.bam\n\n        $ geneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test\n@ 2018-01-14 13:17:33: Read BED file (reference gene model) ...\n@ 2018-01-14 13:17:33: Total 3802 transcripts loaded\n@ 2018-01-14 13:17:33: Get BAM file(s) ...\n        accepted_hits.sorted.bam\n@ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ...\n        3800 transcripts finished\n\n\n        Sample  Skewness\n        accepted_hits.sorted    -3.61577607436\n@ 2018-01-14 13:28:59: Running R script ...\nnull device\n          1\n\n\n\n\n\noutput files:        test.geneBodyCoverage.r\n                                test.geneBodyCoverage.txt\n                log.txt\n                test.geneBodyCoverage.curves.pdf\n\n\ndownload the gene.bed files :  \nhttps://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/\n\nBe careful that the genome version, be consistent between reference genome used in mapping and now. For now, you may use what\u2019s provided \n\n\nif you want to find out whether the sequencing read is strand specific or not, do:\n\n\n        $ infer_experiment.py -r  /projects/oarc/Genomics_Workshop/Reference/ Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam\n\n    Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n    Loading SAM/BAM file ...  Total 200000 usable reads were sampled\n\n\nThis is PairEnd Data\nFraction of reads failed to determine: 0.1406\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292",
            "title": "Genomics"
        },
        {
            "location": "/workshops/workshop/#overview",
            "text": "These are notes from the workshop on genomics. What's covered in these notes: \n1.vk1 Setup",
            "title": "Overview"
        },
        {
            "location": "/workshops/workshop/#genomic-software",
            "text": "This is a list of software to install for the workshop     software  description  link      Seqtk  very handy and fast for processing fastq/a files  link    sratoolkit  downloading and processing data from GEO/SRA database  link    htseq-count  counting the  reads mapped on to genomics feature  link    fastQC  widely used for sequencing read QC  link    RSeQC-2.6.4  An RNA-seq quality control package, multiple functions  python package    trimmomatic  fastq quality trim and adaptor removal  link     This is a list of software already available on the cluster and the command you need to execute to load it in your environment:      software  description  load it on the cluster      Samtools   module load samtools    Bedtools   module load bedtools2./2.25.0    bowtie2  alignment software  module load bowtie2    tophat2  alignment software  module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0    R  language for statistical analysis  module load intel/17.0.4 R-Project/3.4.1     This is a list of other software you might find useful:     software  description  link      GSEA  genome set enrichment analysis  link    IGV  Interactive Genome Viewer  link    Cytoscape  Network visualization softwar  link",
            "title": "Genomic Software"
        },
        {
            "location": "/workshops/workshop/#setup",
            "text": "",
            "title": "Setup"
        },
        {
            "location": "/workshops/workshop/#connect-to-the-cluster-login-node",
            "text": "The preferred method to connect to the cluster is through a web browser and fastX client\n -  via FastX : in your browser, go to `https://amarel.hpc.rutgers.edu:3443\nIf the above method doesn't work you may still connect    via a terminal : if you have a Mac or Linux, terminal is part of your standard apps. If you have Windows, install an SSH client such as  mobaXterm  [link] (https://mobaxterm.mobatek.net/). Then from your terminal connect to the cluster by executing the following command:   ssh -X <your net id>@amarel.hpc.rutgers.edu",
            "title": "Connect to the cluster login node"
        },
        {
            "location": "/workshops/workshop/#get-resources-on-the-compute-node",
            "text": "You get to the cluster to execute your computations by running the following command in your terminal:    srun  -p main --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --export=ALL --pty /bin/bash   \nNotice that the name in your terminal will change from  amarel  to node name like  hal0025  or  slepner086 . This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. The following table explains the parts of this command:      command part  meaning      srun  slurm  run, i.e. allocate resources and run via  slurm  scheduler    -p main  on the main partition, one of several queues on the cluster    --reservation=genomics  we reserved some compute nodes for this workshop to not wait long for resources    -N 1  ask for one node    -c 2  ask for two cores    -n 1  this will be 1 most times    -t 01:40:00  run this for a maximum time of 1 hour 40 minutes    --pty /bin/bash  run the terminal shell in an interactive mode",
            "title": "Get resources on the compute node"
        },
        {
            "location": "/workshops/workshop/#prepare-some-directories-for-the-data",
            "text": "You have two main spaces on the Amarel cluster. These are:    your home directory -  /home/netid/   (e.g.  /home/kp807/  for my netid) -  your scratch directory -  /scratch/netid/   They differ in how often they are backed up and by size (100Gb for  /home  and 500Gb for  /scratch ). So we will install programs in  /home , while the data and output will be in  /scratch . Execute these commands:                   cd ~                      # change directory to your home directory\n                mkdir Genomics_Workshop\n                cd Genomics_Workshop\n                mkdir Programs            # download and install programs here                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated",
            "title": "Prepare some directories for the data"
        },
        {
            "location": "/workshops/workshop/#install-programs",
            "text": "Each program will have slightly different installation instructions. Here is a handy sequence of commands that will install them:   ##We are going to do some modifications to a system file .bashrc, be careful doing it and make sure that you created a copy of your .bashrc file\n                cd\n                cp .bashrc .bashrc_20180118\n                nano .bashrc\n##At the end of the file add the line  \u201c##  Genomics_Workshop 06/27/2018 settings\u201d\n##Exit nano (ctrl+x)\n\n#Seqtk:   https://github.com/lh3/seqtk   ##  very handy and fast for processing fastq/a files\n                cd\n                cd Genomics_Workshop/Programs/\n                git clone https://github.com/lh3/seqtk.git \n                cd seqtk\n                make\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH\u2019 >>  ~/.bashrc\n                source ~/.bashrc\n\n#sratoolkit     https://www.ncbi.nlm.nih.gov/books/NBK158900/\n                        https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software\n##for downloading and processing data from GEO/SRA database\n                cd\n                cd Genomics_Workshop/Programs/\n                wget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.8.2/sratoolkit.2.8.2-centos_linux64.tar.gz\n                tar zxvf sratoolkit.2.8.2-centos_linux64.tar.gz\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc \n\n#htseq-count    http://htseq.readthedocs.io/en/master/install.html ##for counting the  reads mapped on to genomics feature\n\n                cd\n                module load intel/17.0.2 python/2.7.12\n                pip install HTSeq --user\n\n#fastQC          #widely used for sequencing read QC\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.6.zip\n                unzip  fastqc_v0.11.6.zip\n                echo \u2018export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH\u2019 >> ~/.bashrc\n                source ~/.bashrc\n\n\n#FASTX-toolkit    http://hannonlab.cshl.edu/fastx_toolkit/   (##also a tool kit for fastq processing, quality trim, adaptor removal, etc. try if time allows)\n\n#RSeQC-2.6.4     ##An RNA-seq quality control package, multiple functions\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                module load python/2.7.12\n                module load intel/17.0.2\n                pip install RSeQC --user\n\n\n#trimmomatic             ##for fastq quality trim and adaptor removal\n\n                cd\n                cd  Genomics_Workshop/Programs/\n                wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip\n                unzip Trimmomatic-0.36.zip \n                java -jar ~/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar #-h                      Moreover, execute the following commands to load system-installed sofware so the system knows where to find it (i.e.  samtools  command will work if you execute  module load samtools ):   module load samtools       \nmodule load bedtools2./2.25.0\nmodule load bowtie2\nmodule load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\nmodule load intel/17.0.4 R-Project/3.4.1",
            "title": "Install programs"
        },
        {
            "location": "/workshops/workshop/#download-data",
            "text": "We will download human RNA-seq data with  GEO accession GSE52778 . The samples we download are in NCBI's short read archive format (SRA). To unpack the original sequence files can be a bit tricky at first. Please put them in different directories:                  mkdir -p  /scratch/$USER/Genomics_Workshop/\n                cd /scratch/$USER/Genomics_Workshop/\n                mkdir untreated  \n                mkdir dex_treated  We will use sratoolkit programs to download data but first we need to configure a location where all data files will be stored.  sratoolkit  will be in your home directory, under  Programs , and the  vdb-config  might be under the  bin  directory. You will enter  /scratch/your_netID/Genomics_Workshop/download  for the path - NOTE you have to replace  your_netID  with your true netId, e.g.  kp807 . Do not copy blindly! So your downloads will always go to this directory and you will need to move it out to wherever you want to have them.                   vdb-config   --interactive-mode textual     ### dash-dash before interactive-mode\n                         Your choice > 4\n## type new path in\n                        /scratch/your_netID/Genomics_Workshop/download\n                        Your choice > Y  Then execute the following commands to get the data. Both  prefetch  and  fastq-dump  are part of sratools. Downloading can take some time! [TODO: check how much time for these files!]  prefetch -v SRR1039508                           # fetches the SRA data\nfastq-dump --gzip --split-files SRR1039508       # ????   You have to pay attention to where you are putting your data. So these two commands will actually be several:                   cd  untreated                       # now you are in /scratch/..../Genomics_Workshop/untreated\n                prefetch -v SRR1039508\n                mv /scratch/$USER/Genomics_Workshop/download/sra/SRR1039508.sra .  # moving from download to actual directory \n                fastq-dump --gzip --split-files SRR1039516  The commands above showed how to do it for one sample. You need to do it for 6 samples total.                   SRR1039508  SRR1039512 SRR1039516   (untreated)\n                SRR1039509  SRR1039513  SRR1039517  (dex_treated)",
            "title": "Download data"
        },
        {
            "location": "/workshops/workshop/#running-bioinformatics-jobs",
            "text": "",
            "title": "Running bioinformatics jobs"
        },
        {
            "location": "/workshops/workshop/#fastqc-raw-data-qc",
            "text": "Explain what is fastqc is doing here - TODO          cd /scratch/$USER/Genomics_Workshop/untreated         \n        module load java  ## fastqc is written in java; we need to load java before using fastqc\n        mkdir fastqc      ## create a folder to store the QC output \n        fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq  FastQC produces an html page as output,  fastqc/SRR1039508_1_fastqc.html , with different kinds of views of data (and Phred scores). You can download this file to your local machine and open it in browser. It is also possible to open browser on the cluster, but the cluster is not really designed for that. To see more about FastQC, see this pdf file - /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf",
            "title": "FastQC - raw data QC"
        },
        {
            "location": "/workshops/workshop/#trimmomatic-quality-trimadaptor-removal",
            "text": "##for demonstration purpose, we will take a small subset data using seqtk\n    cd /scratch/$USER/Genomics_Workshop/untreated\n    seqtk sample -s100  SRR1039508_1.fastq 10000 > SRR1039508_1_10k.fastq \n    seqtk sample -s100  SRR1039508_2.fastq 10000 > SRR1039508_2_10k.fastq \n    ## /projects/oarc/Genomics_Workshop/Labs/Seqtk_Examples.docx\n    ## This file contains useful examples how to use seqtk\n\n    ##now, run trimmomatic to trim the read quality , and remove adaptor\n    module load java    ### because trimmomatic\n    java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35  NOTE:   the above is a one line command, illustrated as the following:          java -jar trimmomatic-0.36.jar PE \\\n        -phred33 -trimlog trim.log \\\n        input_1.fq  input_2.fq \\\n        output_1_paired.fq  output_1_unpaired.fq \\\n        output_2_paired.fq  output_2_unpaired.fq \\\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 / \n\n        ## Once it started run, you shall see the following:\n        TrimmomaticPE: Started with arguments:\n        -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\n        Multiple cores found: Using 2 threads\n        Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\n        ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\n        Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%)\n        TrimmomaticPE: Completed successfully\n\n        ##view the output, the trim.log file, .e.g.  length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained)\n\n   ##you may also try fastx_quality_stats from the FASTX\u2014toolkit",
            "title": "Trimmomatic - quality trim/adaptor removal"
        },
        {
            "location": "/workshops/workshop/#fastqc-run-on-cleaned-reads-compare-result",
            "text": "module load java\n     fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n\n     ## /projects/oarc/Genomics_Workshop/Labs/FastQC_details.pdf , helpful in viewing and interpreting the output",
            "title": "FastQC - Run on cleaned reads, compare result"
        },
        {
            "location": "/workshops/workshop/#download-reference-and-reference-indexing",
            "text": "Human genome indexing will take hours, we have the reference pre-prepared. Stored at   /projects/oarc/Genomics_Workshop/Reference/ \nFor in class practice, we will do this on E.coli genome         cd /scratch/$USER/Genomics_Workshop/\n        mkdir Reference\n        cd Reference\n\n        wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz\n\n        gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz\n        module load bowtie2\n        bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic\n\n###if download from ENSEMBLE\n        wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-38/fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/dna/Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.dna.toplevel.fa.gz",
            "title": "Download reference and reference indexing"
        },
        {
            "location": "/workshops/workshop/#mapping-with-tophat2-star-hisat2",
            "text": "Now, go to your data folder\n         cd  /scratch/$USER/Genomics_Workshop/untreated  cd  /scratch/$USER/Genomics_Workshop/untreated\n        module load mvapich2/2.1  boost/1.59.0  tophat2/2.1.0\n        module load samtools   #bowtie2 is loaded already\n        mkdir tophat_out\n        tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039508_1\n        0k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/  hg20_transciptome/GR\n        Ch38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel\n        SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq\n## you shall modify the -p value to be consistent with the -c value you requested in the beginning  You shall see something like:     [2018-03-30 11:48:57] Beginning TopHat run (v2.1.0)\n-----------------------------------------------\n[2018-03-30 11:48:57] Checking for Bowtie\n                  Bowtie version:        2.2.9.0\n[2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome)..\n[2018-03-30 11:48:58] Checking for Bowtie index files (genome)..\n[2018-03-30 11:48:58] Checking for reference FASTA file\n[2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen\nce/hg20/Homo_sapiens.GRCh38.dna.toplevel\n[2018-03-30 11:49:09] Reading known junctions from GTF file\n[2018-03-30 11:49:27] Preparing reads\n         left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded)\n        right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded)\n[2018-03-30 11:49:29] Using pre-built transcriptome data..\n[2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2\n[2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads\n[2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t\noplevel with Bowtie2\n[2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n[2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2\n[2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2)\n[2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2)\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.  The transcriptome index was built  by pointing to gtf file first,  here we have it prepared already, just so we can save time.  The following would be the command to generate the transcriptome index while running tophat alignment.     tophat2 -p 10 --library-type fr-unstranded  -o tophat_out/untreated_SRR1039516 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/ Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039516_1.fastq.pairedOut.fastq SRR1039516_2.fastq.pairedOut.fastq  The output folder  tophat_out/untreated_SRR1039508/  shall contain the following files/folder (in blue):        cd  /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508\n        $ ll\n   total 2183632\n -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam\n -rw-rw-r-- 1 yc759 oarc        565 Jan 16 21:57 align_summary.txt\n -rw-rw-r-- 1 yc759 oarc    1921529 Jan 16 21:57 deletions.bed\n -rw-rw-r-- 1 yc759 oarc    2239884 Jan 16 21:57 insertions.bed\n -rw-rw-r-- 1 yc759 oarc   14181618 Jan 16 21:57 junctions.bed\n drwxrwsr-x 2 yc759 oarc       4096 Jan 16 21:57 logs\n -rw-rw-r-- 1 yc759 oarc        184 Jan 16 21:57 prep_reads.info\n -rw-rw-r-- 1 yc759 oarc   42846571 Jan 16 21:57 unmapped.bam",
            "title": "Mapping with tophat2, (STAR, HISAT2)"
        },
        {
            "location": "/workshops/workshop/#read-counts-using-htseq-count",
            "text": "GO TO WHERE YOUR ALIGNMENT OUTPUT FOLDER IS, FOR EXAMPLE:     cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n  ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.bam accepted_hits.bam   \n\n  ##make a soft link to the full bam file we already prepared, if you didn\u2019t have the bam ready yet\n\n     module load samtools intel/17.0.2 python/2.7.12 \n     samtools sort -n  accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name  -/projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf > untreated08.txt  Use samtools to sort the bam file by name:  because htseq-count accepts bam file sorted by name as default, but tophat generates bam sorted by coordinates by default\nThe same way to generate the counts file  untreated12.txt ,  untreated16.txt , dex09.txt ,  dex13.txt ,  dex17.txt",
            "title": "Read counts using htseq-count"
        },
        {
            "location": "/workshops/workshop/#perform-mapping-qc-using-rseqc",
            "text": "Now,  quality control using RSeQC \u2013a few examples here, please go to the website for more functions  http://rseqc.sourceforge.net/    cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 \n        module load python/2.7.12\n        module load intel/17.0.4\n\n        $ read_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/\nReference/Homo_sapiens.GRCh38.79.bed\nprocessing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n        processing accepted_hits.bam ... Finished\n\n        Total Reads                   43474036\n        Total Tags                    54438789\n        Total Assigned Tags           53991382\n        =====================================================================\n        Group               Total_bases         Tag_count           Tags/Kb\n        CDS_Exons           103371993           43264842            418.54\n        5'UTR_Exons         5217678             583447              111.82\n        3'UTR_Exons         29324747            8145122             277.76\n        Introns             1500197093          1805034             1.20\n        TSS_up_1kb          33306654            18893               0.57\n        TSS_up_5kb          148463534           41165               0.28\n        TSS_up_10kb         265823549           55644               0.21\n        TES_down_1kb        35215293            50954               1.45\n        TES_down_5kb        152556214           113325              0.74\n        TES_down_10kb       268614580           137293              0.51\n        =====================================================================\n\n $ bam_stat.py -i accepted_hits.bam\n        Load BAM file ...\n\n        Done\n\n        #==================================================\n        #All numbers are READ count\n        #==================================================\n\n        Total records:                          52528699\n\n        QC failed:                              0\n        Optical/PCR duplicate:                  0\n        Non primary hits                        9054663\n        Unmapped reads:                         0\n        mapq < mapq_cut (non-unique):           2684801\n\n        mapq >= mapq_cut (unique):              40789235\n        Read-1:                                 20414530\n        Read-2:                                 20374705\n        Reads map to '+':                       20393901\n        Reads map to '-':                       20395334\n        Non-splice reads:                       30860931\n        Splice reads:                           9928304\n        Reads mapped in proper pairs:           32386536\n        Proper-paired reads map to different chrom:312  The script does genebody coverage calculation requires the input bam files to be sorted and indexed (we will do it using samtools). The calculation and plot will require R\nGo to one of the tophat_out sample folder  $ module load intel/17.0.4  R-Project/3.4.1    \n        $ module load samtools\n        $ samtools sort accepted_hits.bam \u2013o accepted_hits.sorted.bam \n        ##this may take a while, you may use the one already prepared for you by making a soft link\n        ln \u2013s /projects/oarc/Genomics_Workshop/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam\n\n\n        $ samtools index accepted_hits.sorted.bam\n\n        $ geneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test\n@ 2018-01-14 13:17:33: Read BED file (reference gene model) ...\n@ 2018-01-14 13:17:33: Total 3802 transcripts loaded\n@ 2018-01-14 13:17:33: Get BAM file(s) ...\n        accepted_hits.sorted.bam\n@ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ...\n        3800 transcripts finished\n\n\n        Sample  Skewness\n        accepted_hits.sorted    -3.61577607436\n@ 2018-01-14 13:28:59: Running R script ...\nnull device\n          1  output files:        test.geneBodyCoverage.r\n                                test.geneBodyCoverage.txt\n                log.txt\n                test.geneBodyCoverage.curves.pdf  download the gene.bed files :   https://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/ \nBe careful that the genome version, be consistent between reference genome used in mapping and now. For now, you may use what\u2019s provided   if you want to find out whether the sequencing read is strand specific or not, do:          $ infer_experiment.py -r  /projects/oarc/Genomics_Workshop/Reference/ Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam\n\n    Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done\n    Loading SAM/BAM file ...  Total 200000 usable reads were sampled\n\n\nThis is PairEnd Data\nFraction of reads failed to determine: 0.1406\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292",
            "title": "Perform Mapping QC using RSeQC"
        },
        {
            "location": "/workshops/Lab_1/",
            "text": "LAB 1: Visualization using IGV\n\n\nIGV\n  --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis\n The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command\n\n\n  cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508\n        module load samtools                                                             \n        samtools sort accepted_hits.bam -o accepted_hits.sorted.bam  ##this step  takes about 10 minutes to complete \n        samtools index accepted_hits.sorted.bam ## it takes about 30 seconds\n\n\n\n\nThe resulting files: accepted_hits.sorted.bam  \n\n                      accepted_hits.sorted.bam.bai\n\n                                are the files to be uploaded to IGV\n\n\nYou need to repeat these steps for every sample \n\n\nShortcut Lab 1\n\n\nWe have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at: \n/projects/oarc/Genomics_Workshop/Bam_for_IGV/\n . Make a soft link (see the following command), or copy them into your scratch  folder, then we use IGV to analyze them. \n\n\ncd /scratch/$USER/Genomics_Workshop/\n ln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV  Bam_for_IGV  ## this step was  done when you ran lab_PartII.sh\n #start IGV \n        module load java\n        /projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh \n\n\n\n\nPractice and get familiar with:  \n\n\n\n\nHow to Load genome and data track\n\n\nHow to navigate\n\n\nHow and what to visualize:\n\n\nExamine coverage\n\n\nLow mapping quality\n\n\nMis-alignment\n\n\nTranslocation\n\n\nNovel genes/transcript\n\n\nAlternative splicing\n\n\nInversion\n\n\nLook for SNPs\n\n\nCNV, ChipSeq, RNASeq, WGS alignmentSNP\n\n\n\n\nMore detailed explanation \nhere\n\n\nThe following is a \nsample\n snap shot of the above two files loaded to IGV.\n\n  CLOSE your interactive session on a node  when done with IGV by typing exit",
            "title": "Lab I"
        },
        {
            "location": "/workshops/Lab_1/#lab-1-visualization-using-igv",
            "text": "IGV   --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis\n The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command    cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508\n        module load samtools                                                             \n        samtools sort accepted_hits.bam -o accepted_hits.sorted.bam  ##this step  takes about 10 minutes to complete \n        samtools index accepted_hits.sorted.bam ## it takes about 30 seconds  The resulting files: accepted_hits.sorted.bam   \n                      accepted_hits.sorted.bam.bai \n                                are the files to be uploaded to IGV  You need to repeat these steps for every sample",
            "title": "LAB 1: Visualization using IGV"
        },
        {
            "location": "/workshops/Lab_1/#shortcut-lab-1",
            "text": "We have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at:  /projects/oarc/Genomics_Workshop/Bam_for_IGV/  . Make a soft link (see the following command), or copy them into your scratch  folder, then we use IGV to analyze them.   cd /scratch/$USER/Genomics_Workshop/\n ln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV  Bam_for_IGV  ## this step was  done when you ran lab_PartII.sh\n #start IGV \n        module load java\n        /projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh   Practice and get familiar with:     How to Load genome and data track  How to navigate  How and what to visualize:  Examine coverage  Low mapping quality  Mis-alignment  Translocation  Novel genes/transcript  Alternative splicing  Inversion  Look for SNPs  CNV, ChipSeq, RNASeq, WGS alignmentSNP   More detailed explanation  here  The following is a  sample  snap shot of the above two files loaded to IGV. \n  CLOSE your interactive session on a node  when done with IGV by typing exit",
            "title": "Shortcut Lab 1"
        },
        {
            "location": "/workshops/Lab_2/",
            "text": "LAB 2:  data processing and expression analysis using edgeR\n\n\nAll the htseq-count output files should be present in one folder. Here we created the folder read_counts.\n\n\n            read_counts/dex09.txt\n                        dex13.txt\n                        dex17.txt\n                        untreated08.txt\n                        untreated12.txt\n                        untreated16.txt\n\n\n\n\nGo to this folder containing all above six counts output files\n \ncd /scratch/$USER/Genomics_Workshop/read_counts\n\n\nWe need to combine all counts into one file, which will be imported into R for further analysis.\n The following shows how this can be done with bash commands on linux ( you may do it in excel too)\n\n\n paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt > merged_counts.txt    \n    ## merge files by columns\n cut -f1,2,4,6,8,10,13 merged_counts.txt  > merged_counts_clean.txt     ## extract the relevant columns  \n head -n -5 merged_counts_clean.txt > merged_counts_clean2.txt      ##remove the last 5 line stats summary\n\n\n\n\nWe also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file.  You may construct in excel. \n \nThe file should contain the following info\n\n\n\n\n\n\n\n\nlabel\n\n\nsample\n\n\ngroup\n\n\ntreatment\n\n\n\n\n\n\n\n\n\n\n1\n\n\ndex09\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n2\n\n\ndex13\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n3\n\n\ndex17\n\n\ndex\n\n\ndex_treated\n\n\n\n\n\n\n4\n\n\nuntreated08\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n5\n\n\nuntreated12\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n6\n\n\nuntreated16\n\n\n\n\ncontrol untreated\n\n\n\n\n\n\n\n\nThis file has been prepared for you. You will need to Copy it into your folder, see later Lab2_2:\n\n\nLab2.2: Pre-processing the data in R\n\n\ncd /scratch/$USER/Genomics_Workshop/\n        mkdir  DE_analysis  ##set working directory to run differential expression analysis\n        cd DE_analysis\n## Copy the needed files here\n\n        cp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD                           ##This file denotes the experimental group\n        cp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD              ##This file contains read counts on genes for all samples\n        cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD            ###annotation file needed to calculate exonic gene length -- needed for FPKM calculation\n\n\n\n\nHOMEWORK CATCH-UP\n\n\nFrom your homework assignment you should have the following packages to be installed already :).\nIf you didn't, install them now.\n\n\nOpen a terminal for  amarel2 login node.\n\nssh -X amarel2.hpc.rutgers.edu\nOn the login node start R\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        R\n\n then in the R workspace do the following:\n        > source(\"https://bioconductor.org/biocLite.R\")\n        > biocLite(\"MKmisc\")\n        Would you like to use a personal library instead?  (y/n)  y\n\n        Would you like to create a personal library\n        ~/R/x86_64-pc-linux-gnu-library/3.4\n        to install packages into?  (y/n) y\n\n Wait till it finishes.\n\n        > biocLite(\"Heatplus\")\n        > biocLite(\"affycoretools\")\n        > biocLite(\"flashClust\")\n        > biocLite(\"affy\")\n        > biocLite(\"GenomicFeatures\")\n        > quit()   ###quit R, no save\nSave workspace image? [y/n/c]: n\n\n\n\n\nStarting the Job\n\n\nNow,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running)\n\n\nsrun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i\n\nGo to your working directory\n \ncd /scratch/$USER/Genomics_Workshop/DE_analysis/\n\n Then, start R\n \nmodule load intel/17.0.4\n module load R-Project/3.4.1\n\n\n>getwd()\n #check which directory you are in\n\n You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory :\n \n>setwd(\"/scratch/<your_netID>/Genomics_Workshop/DE_analysis/\")\n ##set the working directory\n\n\nNow load up the libraries needed for the analysis\n\n\n    >library(edgeR)\n    >library(MKmisc)\n    >library(affy)\n    >library(flashClust)\n    >library(affycoretools)\n    >library(Heatplus)\n    >library(GenomicFeatures)\n\n\n\n> raw.data <- read.delim(\"merged_counts_clean2.txt\", header=F, row.names=1)  \n##import count data to R\n> head(raw.data)  ## check the beginning portion of the imported file, now an object\n> dim(raw.data)   ##check the dimention of this object\n> class(raw.data)  ## check what class of this object\n> apply(raw.data, 2, summary) ## check the range of counts per sample\n> range(rowSums(raw.data))      ## check the range of counts per gene\n> colnames(raw.data) <- c(\"dex09\",\"dex13\",\"dex17\",\"untreated08\",\"untreated12\",\"untreated16\")\n##add column header\n\n>raw.data2 <- raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples\n> dim(raw.data2)\n\n> cpm.values <- cpm(raw.data2) #calculate counts per million mapped reads without any other normalization\n\n>above1cpm <- apply(cpm.values, 1, function(x) sum(x >=1)) ##How many samples/genes had at least 1 cpm \n> counts.use <- raw.data2[above1cpm >= 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group)\n>dim(counts.use)\n>colSums(counts.use) / colSums(raw.data)  ##the % of total counts kept after filtering\n>nrow(counts.use) / nrow(raw.data)  ##the % of genes kept after filtering\n\n> targets <- readTargets()      ##import targets file that contains experiment group info.\n> targets$GpF <- factor(targets$group)   ##change character to factor\n> targets$GpN <- as.numeric(targets$GpF) ##change factor to numeric (optional)\n\n> ls()  #see that objects have been loaded\n>save.image(\"RNASeqDemo.RData\")   ##save the work workspace\n>savehistory(\"RNASeqDemo.Rhistory\")  ##save command history\n\n\n\n\n\nLab 2.3: To calculate expression values as fpkm\n\n\nFirst, compute the gene length as described in Lab6--partI\n\n\n> library(GenomicFeatures)  ##may skip, because we already loaded at start\n> gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n> exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n> exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n> class(exonic.gene.sizes)\n>Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n> colnames(Hg20_geneLength) <- paste('geneLength')    \n\n>Hg20_geneLength2 <- data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain \nthe same number genes in the same order as in the filtered read counts file\n>colnames(Hg20_geneLength2) <- paste('geneLength')  ## to change column name, make    it neat\n> fpkm.data <- cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm\n>  min.count <- apply(fpkm.data,1,min)\n> write.csv(fpkm.data,file=\"fpkm_values.csv\")  #### To output FPKM data  \n\n\n\n\n\nLab 2.4:  Analysis QC ---or sample diagnosis\n\n\n ## density distribution##\n > plotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2)\n > legend(\"topright\", legend=levels(targets$GpF),fill=1:4)\n\n\n\n\nchange data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization\n\n\nClustering\n\n\nhc.raw <- flashClust(dist(t(log2(raw.data2+0.1))),method=\"average\")\nplot(hc.raw,hang = -1, main = \"RNASeqDemo, raw values\", sub = \"\", xlab = \"\",cex=0.9, labels=targets$sample)\n##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization\n\n#####PCA#######\n>  plotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on FPKM\")\n\n###do it after edgeR analysis, otherwise some values not existing yet###\n\n>  plotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on logCPM\")\n\n\n#####heatmap####\n>test <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"PValue\")$table ####sort gene list according to P values\n>test2 <- test[1:500,]  ###Take the top 500 significant genes\n>logCPM2 <- logCPM[rownames(test2),]  ###Extract logCPM values of these 500 genes\n> meanFC <- logCPM2 - rowMeans(logCPM2)\n> color.meanFC <- heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c(\"blue\",\"white\",\"red\"))(128))\n> heatmap_2(meanFC, col=color.meanFC, legend=3, scale=\"none\")\n\n\n\n #####Volcano plot####\n> with(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2),ylim=c(0,25)))\n> with(subset(eR.dex_Ctl.detailed,FDR<0.05), points(logFC, -log10(PValue), pch=20, col=\"red\"))\n> with(subset(eR.dex_Ctl.detailed, abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"orange\"))\n> with(subset(eR.dex_Ctl.detailed, FDR<0.05&abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"green\"))\n\n\n\n\n\nLab 2.5: Differential expression analysis  with edgeR\n\n\nCreate the design matrix\n\n\n> groups <- factor(targets$group)\n> design <- model.matrix(~0+groups)\n> colnames(design) <- levels(groups)\n> rownames(design) <- targets$sample   \n> design\n\n\n\n\n                                        control dex\ndex09             0   1\ndex13             0   1\ndex17             0   1\nuntreated08       1   0\nuntreated12       1   0\nuntreated16       1   0\nattr(,\"assign\")\n[1] 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$groups\n[1] \"contr.treatment\"\n\n\n\n\nCreate contrast matrix\n\n\n>cont.matrix <- makeContrasts(dex_Ctl= dex - control, levels=design)\n\n\nCreate DGEList object\n\n\n>  d <- DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF)\n> class(d)\n> names(d)    ## the names of the items in the list\n> d$counts[1:5,]    ## The counts are stored in the $counts:\n>d$samples   ## The group info and library sizes stored in $samples\n> d <- calcNormFactors(d)  ## an additional normalization factor using a TMM method\n> d$samples   ## now the norm.factors are no longer 1\n\n###this can be useful when diagnose problem)###\n> apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100\n##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. \n\n>logCPM <-  cpm(d$counts, log = TRUE)   ## modified logCPM values in edgeR, can be used for clustering, heatmap\n\n\n\n\n\nNow,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among \nreplicates.\n\n\n> d <- estimateGLMCommonDisp(d, design, verbose=TRUE)   \nDisp = 0.06037 , BCV = 0.2457 \n> d <- estimateGLMTrendedDisp(d, design)\n> d <- estimateGLMTagwiseDisp(d, design)\n> plotBCV(d)   ## the relationship between the overall abundance and the tagwise dispersion estimates\n\n\n> fit.edgeR <- glmFit(d, design)  ## Estimate model coefficients from count data and design matrix\n> names(fit.edgeR)\n\n\n\n\nSpecify contrasts of interest,  do empirical Bayes \"shrinkage\" of  variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models)\n\n\n> eR.dex_Ctl <- glmLRT(fit.edgeR, contrast=cont.matrix[,1])\n\n> summary(decideTestsDGE(eR.dex_Ctl)) [,1]  ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS)\n\n> eR.dex_Ctl.detailed <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"none\")$table ## Get detailed output for a single contrast\n\n>  eR.dex_Ctl.detailed[1:5,]\n                      logFC   logCPM         LR    PValue       FDR\nENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740\nENSG00000000419  0.18626588 4.674100 0.79016266 0.3740509 0.8571700\nENSG00000000457  0.04304803 3.838702 0.03317218 0.8554789 0.9905954\nENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274\nENSG00000000971  0.35401120 8.007586 1.55551391 0.2123233 0.7070242\n\n## Fold change is simply group A-B (if on the log scale), or A/B (if on   raw scale).\n## logCPM = the average log2-counts-per-million \n## LR = likelihood ratio statistics \n##PValue = the two-sided p-value \n## FDR = adjusted p-value \n\n#### To back-translate logFC to regular FC with down-reg as -FC\n> eR.dex_Ctl.detailed$FC <- 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC)\n\n> write.csv(eR.dex_Ctl.detailed,file=\"Demo_eR.dex_Ctl_results.csv\")  ##output the file",
            "title": "Lab II"
        },
        {
            "location": "/workshops/Lab_2/#lab-2-data-processing-and-expression-analysis-using-edger",
            "text": "All the htseq-count output files should be present in one folder. Here we created the folder read_counts.              read_counts/dex09.txt\n                        dex13.txt\n                        dex17.txt\n                        untreated08.txt\n                        untreated12.txt\n                        untreated16.txt  Go to this folder containing all above six counts output files\n  cd /scratch/$USER/Genomics_Workshop/read_counts  We need to combine all counts into one file, which will be imported into R for further analysis.\n The following shows how this can be done with bash commands on linux ( you may do it in excel too)   paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt > merged_counts.txt    \n    ## merge files by columns\n cut -f1,2,4,6,8,10,13 merged_counts.txt  > merged_counts_clean.txt     ## extract the relevant columns  \n head -n -5 merged_counts_clean.txt > merged_counts_clean2.txt      ##remove the last 5 line stats summary  We also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file.  You may construct in excel. \n  The file should contain the following info     label  sample  group  treatment      1  dex09  dex  dex_treated    2  dex13  dex  dex_treated    3  dex17  dex  dex_treated    4  untreated08   control untreated    5  untreated12   control untreated    6  untreated16   control untreated     This file has been prepared for you. You will need to Copy it into your folder, see later Lab2_2:",
            "title": "LAB 2:  data processing and expression analysis using edgeR"
        },
        {
            "location": "/workshops/Lab_2/#lab22-pre-processing-the-data-in-r",
            "text": "cd /scratch/$USER/Genomics_Workshop/\n        mkdir  DE_analysis  ##set working directory to run differential expression analysis\n        cd DE_analysis\n## Copy the needed files here\n\n        cp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD                           ##This file denotes the experimental group\n        cp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD              ##This file contains read counts on genes for all samples\n        cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD            ###annotation file needed to calculate exonic gene length -- needed for FPKM calculation",
            "title": "Lab2.2: Pre-processing the data in R"
        },
        {
            "location": "/workshops/Lab_2/#homework-catch-up",
            "text": "From your homework assignment you should have the following packages to be installed already :).\nIf you didn't, install them now.  Open a terminal for  amarel2 login node.\n\nssh -X amarel2.hpc.rutgers.edu\nOn the login node start R\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        R\n\n then in the R workspace do the following:\n        > source(\"https://bioconductor.org/biocLite.R\")\n        > biocLite(\"MKmisc\")\n        Would you like to use a personal library instead?  (y/n)  y\n\n        Would you like to create a personal library\n        ~/R/x86_64-pc-linux-gnu-library/3.4\n        to install packages into?  (y/n) y\n\n Wait till it finishes.\n\n        > biocLite(\"Heatplus\")\n        > biocLite(\"affycoretools\")\n        > biocLite(\"flashClust\")\n        > biocLite(\"affy\")\n        > biocLite(\"GenomicFeatures\")\n        > quit()   ###quit R, no save\nSave workspace image? [y/n/c]: n",
            "title": "HOMEWORK CATCH-UP"
        },
        {
            "location": "/workshops/Lab_2/#starting-the-job",
            "text": "Now,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running)  srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i \nGo to your working directory\n  cd /scratch/$USER/Genomics_Workshop/DE_analysis/ \n Then, start R\n  module load intel/17.0.4\n module load R-Project/3.4.1  >getwd()  #check which directory you are in \n You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory :\n  >setwd(\"/scratch/<your_netID>/Genomics_Workshop/DE_analysis/\")  ##set the working directory  Now load up the libraries needed for the analysis      >library(edgeR)\n    >library(MKmisc)\n    >library(affy)\n    >library(flashClust)\n    >library(affycoretools)\n    >library(Heatplus)\n    >library(GenomicFeatures)  > raw.data <- read.delim(\"merged_counts_clean2.txt\", header=F, row.names=1)  \n##import count data to R\n> head(raw.data)  ## check the beginning portion of the imported file, now an object\n> dim(raw.data)   ##check the dimention of this object\n> class(raw.data)  ## check what class of this object\n> apply(raw.data, 2, summary) ## check the range of counts per sample\n> range(rowSums(raw.data))      ## check the range of counts per gene\n> colnames(raw.data) <- c(\"dex09\",\"dex13\",\"dex17\",\"untreated08\",\"untreated12\",\"untreated16\")\n##add column header\n\n>raw.data2 <- raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples\n> dim(raw.data2)\n\n> cpm.values <- cpm(raw.data2) #calculate counts per million mapped reads without any other normalization\n\n>above1cpm <- apply(cpm.values, 1, function(x) sum(x >=1)) ##How many samples/genes had at least 1 cpm \n> counts.use <- raw.data2[above1cpm >= 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group)\n>dim(counts.use)\n>colSums(counts.use) / colSums(raw.data)  ##the % of total counts kept after filtering\n>nrow(counts.use) / nrow(raw.data)  ##the % of genes kept after filtering\n\n> targets <- readTargets()      ##import targets file that contains experiment group info.\n> targets$GpF <- factor(targets$group)   ##change character to factor\n> targets$GpN <- as.numeric(targets$GpF) ##change factor to numeric (optional)\n\n> ls()  #see that objects have been loaded\n>save.image(\"RNASeqDemo.RData\")   ##save the work workspace\n>savehistory(\"RNASeqDemo.Rhistory\")  ##save command history",
            "title": "Starting the Job"
        },
        {
            "location": "/workshops/Lab_2/#lab-23-to-calculate-expression-values-as-fpkm",
            "text": "First, compute the gene length as described in Lab6--partI  > library(GenomicFeatures)  ##may skip, because we already loaded at start\n> gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n> exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n> exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n> class(exonic.gene.sizes)\n>Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n> colnames(Hg20_geneLength) <- paste('geneLength')    \n\n>Hg20_geneLength2 <- data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain \nthe same number genes in the same order as in the filtered read counts file\n>colnames(Hg20_geneLength2) <- paste('geneLength')  ## to change column name, make    it neat\n> fpkm.data <- cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm\n>  min.count <- apply(fpkm.data,1,min)\n> write.csv(fpkm.data,file=\"fpkm_values.csv\")  #### To output FPKM data",
            "title": "Lab 2.3: To calculate expression values as fpkm"
        },
        {
            "location": "/workshops/Lab_2/#lab-24-analysis-qc-or-sample-diagnosis",
            "text": "## density distribution##\n > plotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2)\n > legend(\"topright\", legend=levels(targets$GpF),fill=1:4)  change data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization",
            "title": "Lab 2.4:  Analysis QC ---or sample diagnosis"
        },
        {
            "location": "/workshops/Lab_2/#clustering",
            "text": "hc.raw <- flashClust(dist(t(log2(raw.data2+0.1))),method=\"average\")\nplot(hc.raw,hang = -1, main = \"RNASeqDemo, raw values\", sub = \"\", xlab = \"\",cex=0.9, labels=targets$sample)\n##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization\n\n#####PCA#######\n>  plotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on FPKM\")\n\n###do it after edgeR analysis, otherwise some values not existing yet###\n\n>  plotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main=\"PCA on logCPM\")\n\n\n#####heatmap####\n>test <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"PValue\")$table ####sort gene list according to P values\n>test2 <- test[1:500,]  ###Take the top 500 significant genes\n>logCPM2 <- logCPM[rownames(test2),]  ###Extract logCPM values of these 500 genes\n> meanFC <- logCPM2 - rowMeans(logCPM2)\n> color.meanFC <- heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c(\"blue\",\"white\",\"red\"))(128))\n> heatmap_2(meanFC, col=color.meanFC, legend=3, scale=\"none\")\n\n\n\n #####Volcano plot####\n> with(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2),ylim=c(0,25)))\n> with(subset(eR.dex_Ctl.detailed,FDR<0.05), points(logFC, -log10(PValue), pch=20, col=\"red\"))\n> with(subset(eR.dex_Ctl.detailed, abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"orange\"))\n> with(subset(eR.dex_Ctl.detailed, FDR<0.05&abs(logFC)>1), points(logFC, -log10(PValue), pch=20, col=\"green\"))",
            "title": "Clustering"
        },
        {
            "location": "/workshops/Lab_2/#lab-25-differential-expression-analysis-with-edger",
            "text": "Create the design matrix  > groups <- factor(targets$group)\n> design <- model.matrix(~0+groups)\n> colnames(design) <- levels(groups)\n> rownames(design) <- targets$sample   \n> design                                          control dex\ndex09             0   1\ndex13             0   1\ndex17             0   1\nuntreated08       1   0\nuntreated12       1   0\nuntreated16       1   0\nattr(,\"assign\")\n[1] 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$groups\n[1] \"contr.treatment\"",
            "title": "Lab 2.5: Differential expression analysis  with edgeR"
        },
        {
            "location": "/workshops/Lab_2/#create-contrast-matrix",
            "text": ">cont.matrix <- makeContrasts(dex_Ctl= dex - control, levels=design)",
            "title": "Create contrast matrix"
        },
        {
            "location": "/workshops/Lab_2/#create-dgelist-object",
            "text": ">  d <- DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF)\n> class(d)\n> names(d)    ## the names of the items in the list\n> d$counts[1:5,]    ## The counts are stored in the $counts:\n>d$samples   ## The group info and library sizes stored in $samples\n> d <- calcNormFactors(d)  ## an additional normalization factor using a TMM method\n> d$samples   ## now the norm.factors are no longer 1\n\n###this can be useful when diagnose problem)###\n> apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100\n##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. \n\n>logCPM <-  cpm(d$counts, log = TRUE)   ## modified logCPM values in edgeR, can be used for clustering, heatmap  Now,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among \nreplicates.  > d <- estimateGLMCommonDisp(d, design, verbose=TRUE)   \nDisp = 0.06037 , BCV = 0.2457 \n> d <- estimateGLMTrendedDisp(d, design)\n> d <- estimateGLMTagwiseDisp(d, design)\n> plotBCV(d)   ## the relationship between the overall abundance and the tagwise dispersion estimates\n\n\n> fit.edgeR <- glmFit(d, design)  ## Estimate model coefficients from count data and design matrix\n> names(fit.edgeR)  Specify contrasts of interest,  do empirical Bayes \"shrinkage\" of  variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models)  > eR.dex_Ctl <- glmLRT(fit.edgeR, contrast=cont.matrix[,1])\n\n> summary(decideTestsDGE(eR.dex_Ctl)) [,1]  ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS)\n\n> eR.dex_Ctl.detailed <- topTags(eR.dex_Ctl,n=Inf,sort.by=\"none\")$table ## Get detailed output for a single contrast\n\n>  eR.dex_Ctl.detailed[1:5,]\n                      logFC   logCPM         LR    PValue       FDR\nENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740\nENSG00000000419  0.18626588 4.674100 0.79016266 0.3740509 0.8571700\nENSG00000000457  0.04304803 3.838702 0.03317218 0.8554789 0.9905954\nENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274\nENSG00000000971  0.35401120 8.007586 1.55551391 0.2123233 0.7070242\n\n## Fold change is simply group A-B (if on the log scale), or A/B (if on   raw scale).\n## logCPM = the average log2-counts-per-million \n## LR = likelihood ratio statistics \n##PValue = the two-sided p-value \n## FDR = adjusted p-value \n\n#### To back-translate logFC to regular FC with down-reg as -FC\n> eR.dex_Ctl.detailed$FC <- 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC)\n\n> write.csv(eR.dex_Ctl.detailed,file=\"Demo_eR.dex_Ctl_results.csv\")  ##output the file",
            "title": "Create DGEList object"
        },
        {
            "location": "/workshops/Lab_3/",
            "text": "Lab 3  Running enrichment analysis using GSEA\n\n\nThe command to start the gsea:\n\n\nsrun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i  ##get onto a reserved compute node\n        module load java\n        java -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar      \n\n\n\n\n\nPrepare files required to run GSEA\n\nFor detailed file format, see \nhere\n\n\n\n\nExpression data files   \n\n\n\n\nGene cluster text file format (.gct)\n\n\n\n\nGene set files\n\n\n\n\ngene matrix file format  (.gmx)\ngene matrix transposed format (.gmt)\n\n\n\n\nPhenotype data files\n\n\n\n\nCategorical class file (.cls)  (defining experimental group)\n\n\n\n\n\n\nA set of following sample files are prepared for GSEA analysis practice, which are located at  \n/projects/oarc/Genomics_Workshop/GSEA/\n\n\nfpkm_values.ready.gct    gct file (expression fpkm values)\n        fpkm_values.ready.cls     cls file  (defining experimental group)\n        Mouse_Human_NCI_Nature_November_01_2017_symbol.gmt     gmt file (gene set file biological function set)\n\n\n\n\n\nIn practice analysis, use online broad C2 geneset instead of the above .gmt file\n\n Results are located at \n/home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/\n  \n\n\nTo view your result:\n\n\n  cd ~/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/  \n    ## (same as: cd /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnn/)\n  firefox index.html\n\n\n\n\nAdditional gene set database downloading source:\n\n\n\n\nhttp://software.broadinstitute.org/gsea/msigdb/index.jsp\n\n\nhttp://download.baderlab.org/EM_Genesets/\n\n\nhttp://www.go2msig.org/cgi-bin/prebuilt.cgi\n\n\nor build your \nown",
            "title": "Lab III"
        },
        {
            "location": "/workshops/Lab_3/#lab-3-running-enrichment-analysis-using-gsea",
            "text": "The command to start the gsea:  srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i  ##get onto a reserved compute node\n        module load java\n        java -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar        Prepare files required to run GSEA \nFor detailed file format, see  here   Expression data files      Gene cluster text file format (.gct)   Gene set files   gene matrix file format  (.gmx)\ngene matrix transposed format (.gmt)   Phenotype data files   Categorical class file (.cls)  (defining experimental group)    A set of following sample files are prepared for GSEA analysis practice, which are located at   /projects/oarc/Genomics_Workshop/GSEA/  fpkm_values.ready.gct    gct file (expression fpkm values)\n        fpkm_values.ready.cls     cls file  (defining experimental group)\n        Mouse_Human_NCI_Nature_November_01_2017_symbol.gmt     gmt file (gene set file biological function set)  In practice analysis, use online broad C2 geneset instead of the above .gmt file \n Results are located at  /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/     To view your result:    cd ~/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnnnn/  \n    ## (same as: cd /home/Net_ID/gsea_home/output/<jun27>/my_analysis.Gsea.nnnnnnnnnnn/)\n  firefox index.html  Additional gene set database downloading source:   http://software.broadinstitute.org/gsea/msigdb/index.jsp  http://download.baderlab.org/EM_Genesets/  http://www.go2msig.org/cgi-bin/prebuilt.cgi  or build your  own",
            "title": "Lab 3  Running enrichment analysis using GSEA"
        },
        {
            "location": "/workshops/Lab_4/",
            "text": "Lab 4 Running the GO term analysis\n\n\nOpen \nthis\n link\n\nGene list from de-analysis of our downloaded data (selected based on FDR and FC):\n\n\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927\nENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B \nENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904\nENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244\nENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218\nENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513\nENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498\nENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175\nENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924\nENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139\nENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812\nENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813\nENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383\nENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869\nENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626\nENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591\nENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386\nENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442\nENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490\nENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571\nENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675\nENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793\nENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437\nENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171\nENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260\nENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647\nENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265\nENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878\nENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431\nENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959\nENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493\nENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772\nENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675\nENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930\nENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569\nENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244\nENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738\nENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448\nENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697\nENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214\nENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194\nENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637\nENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715\nENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138\nENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218\nENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950\nENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672\nENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829\nENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741\nENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661\nENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368\nENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480\nENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072\nENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899\nENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928\nENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798\nENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614\nENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869\nENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251\nENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017\nENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907\nENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381\nENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364\nENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549\nENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718\nENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554\nENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401\nENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167\nENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390\nENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285\n\nFDR<0.05, FC < -2.5             FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000164761         ENSG00000177283 ENSG00000268913\nENSG00000149633         ENSG00000230018 ENSG00000103196\nENSG00000012048         ENSG00000261468 ENSG00000162630\nENSG00000126860         ENSG00000197301 ENSG00000247311\nENSG00000092621         ENSG00000154734 ENSG00000046653\nENSG00000154263         ENSG00000169271 ENSG00000167641\nENSG00000079462         ENSG00000124440 ENSG00000135821\nENSG00000182580         ENSG00000099998 ENSG00000136237\nENSG00000167771         ENSG00000120162 ENSG00000099337\nENSG00000205208         ENSG00000126803 ENSG00000120129\nENSG00000172986         ENSG00000068831 ENSG00000004799\nENSG00000272341         ENSG00000123685 ENSG00000221866\n                ENSG00000128045 ENSG00000157150\n                ENSG00000101342 ENSG00000102760\n                ENSG00000096060 ENSG00000198624\n                ENSG00000128917 ENSG00000179094\n                ENSG00000163083 ENSG00000179300\n                ENSG00000173838 ENSG00000136383\n                ENSG00000143127 ENSG00000189221\n                ENSG00000163884 ENSG00000174697\n                ENSG00000168309 ENSG00000112936\n                ENSG00000152583 ENSG00000165995\n                ENSG00000127954 ENSG00000157514\n                ENSG00000250978 ENSG00000233117\n                ENSG00000109906 ENSG00000157152\n                ENSG00000179593 ENSG00000187193\n                ENSG00000101347 ENSG00000152779\n                ENSG00000211445 ENSG00000170214",
            "title": "Lab IV"
        },
        {
            "location": "/workshops/Lab_4/#lab-4-running-the-go-term-analysis",
            "text": "Open  this  link \nGene list from de-analysis of our downloaded data (selected based on FDR and FC):  FDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927\nENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B \nENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904\nENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244\nENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218\nENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513\nENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498\nENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175\nENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924\nENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139\nENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812\nENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813\nENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383\nENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869\nENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626\nENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591\nENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386\nENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442\nENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490\nENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571\nENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675\nENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793\nENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437\nENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171\nENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260\nENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647\nENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265\nENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878\nENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431\nENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959\nENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979\nFDR<0.05, FC < -2.5     FDR<0.05, FC < -2.5     FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493\nENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772\nENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675\nENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930\nENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569\nENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244\nENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738\nENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448\nENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697\nENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214\nENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194\nENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637\nENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715\nENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138\nENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218\nENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950\nENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672\nENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829\nENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741\nENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661\nENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368\nENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480\nENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072\nENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899\nENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928\nENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798\nENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614\nENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869\nENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251\nENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017\nENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907\nENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381\nENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364\nENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549\nENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718\nENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554\nENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401\nENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167\nENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390\nENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285\n\nFDR<0.05, FC < -2.5             FDR<0.05, FC > 2.5      FDR<0.05, FC > 2.5\nENSG00000164761         ENSG00000177283 ENSG00000268913\nENSG00000149633         ENSG00000230018 ENSG00000103196\nENSG00000012048         ENSG00000261468 ENSG00000162630\nENSG00000126860         ENSG00000197301 ENSG00000247311\nENSG00000092621         ENSG00000154734 ENSG00000046653\nENSG00000154263         ENSG00000169271 ENSG00000167641\nENSG00000079462         ENSG00000124440 ENSG00000135821\nENSG00000182580         ENSG00000099998 ENSG00000136237\nENSG00000167771         ENSG00000120162 ENSG00000099337\nENSG00000205208         ENSG00000126803 ENSG00000120129\nENSG00000172986         ENSG00000068831 ENSG00000004799\nENSG00000272341         ENSG00000123685 ENSG00000221866\n                ENSG00000128045 ENSG00000157150\n                ENSG00000101342 ENSG00000102760\n                ENSG00000096060 ENSG00000198624\n                ENSG00000128917 ENSG00000179094\n                ENSG00000163083 ENSG00000179300\n                ENSG00000173838 ENSG00000136383\n                ENSG00000143127 ENSG00000189221\n                ENSG00000163884 ENSG00000174697\n                ENSG00000168309 ENSG00000112936\n                ENSG00000152583 ENSG00000165995\n                ENSG00000127954 ENSG00000157514\n                ENSG00000250978 ENSG00000233117\n                ENSG00000109906 ENSG00000157152\n                ENSG00000179593 ENSG00000187193\n                ENSG00000101347 ENSG00000152779\n                ENSG00000211445 ENSG00000170214",
            "title": "Lab 4 Running the GO term analysis"
        },
        {
            "location": "/workshops/Lab_5/",
            "text": "Lab 5 ID mapping and conversion\n\n\nLearn about gene identifiers, g:profiler, Synergizer and BioMart\n\n\n\n\nUse the above gene list:  \n\n1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler \n  Explore more functions, what the site can do for you \n\n2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart\n\n3. Do it again with your own gene list",
            "title": "Lab V"
        },
        {
            "location": "/workshops/Lab_5/#lab-5-id-mapping-and-conversion",
            "text": "Learn about gene identifiers, g:profiler, Synergizer and BioMart   Use the above gene list:   \n1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler \n  Explore more functions, what the site can do for you  \n2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart \n3. Do it again with your own gene list",
            "title": "Lab 5 ID mapping and conversion"
        },
        {
            "location": "/ressentials/",
            "text": "To start R shell on the cluster\n\n\nSee workshop for explanation of the various options of \nsrun\n\n\n        module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R\n\n\n\n\nPackages used from BioConductor\n\n\nIf these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: \n\n\n        source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")\n\n\n\n\nExample: Calculate gene length\n\n\nGet some data from ENSEMBLE\n\n\nwget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz\n\n\nIn R shell, you can execute these commands to compute gene lengths: \n\n\n\n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')    \n\n\n\n\nSome R essentials\n\n\nArithmetic functions\n\n\n        2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)\n\n\n\n\nCreating objects\n\n\n        ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()\n\n\n\n\nVector and Matrix\n\n\n        x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x\n\n\n\n\nData Frames\n\n\n        z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C\n\n\n\n\nMore ways to subset dataframes\n\n\n        z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]\n\n\n\n\nLists\n\n\n        mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)\n\n\n\n\nFunctions\n\n\n        my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)\n\n\n\n\nVarious directory/file/library manipulations\n\n\n        library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")\n\n\n\n\nSave all the objects you have created to your workspace\n\n\n        save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file\n\n\n\n\nRemove objects from your workspace\n\n\n        ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()\n\n\n\n\nSave a history of all the commands entered\n\n\n        savehistory(\"introhistory.Rhistory\")",
            "title": "R tutorial"
        },
        {
            "location": "/ressentials/#to-start-r-shell-on-the-cluster",
            "text": "See workshop for explanation of the various options of  srun          module load intel/17.0.4\n        module load R-Project/3.4.1\n        ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way\n        srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash\n        ##start R on compute node now\n        R",
            "title": "To start R shell on the cluster"
        },
        {
            "location": "/ressentials/#packages-used-from-bioconductor",
            "text": "If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands:           source(\"https://bioconductor.org/biocLite.R\") \n        biocLite(\"ape\")\n        biocLite(\"MKmisc\")\n        biocLite(\"Heatplus\")\n        biocLite(\"affycoretools\")\n        biocLite(\"flashClust\")\n        biocLite(\"affy\")",
            "title": "Packages used from BioConductor"
        },
        {
            "location": "/ressentials/#example-calculate-gene-length",
            "text": "Get some data from ENSEMBLE  wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz  In R shell, you can execute these commands to compute gene lengths:   \n         library(GenomicFeatures)\n         gtfdb <- makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\")\n         exons.list.per.gene <- exonsBy(gtfdb,by=\"gene\")\n         exonic.gene.sizes <- lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))})\n         class(exonic.gene.sizes)\n\n         Hg20_geneLength <-do.call(rbind, exonic.gene.sizes)\n         colnames(Hg20_geneLength) <- paste('geneLength')",
            "title": "Example: Calculate gene length"
        },
        {
            "location": "/ressentials/#some-r-essentials",
            "text": "",
            "title": "Some R essentials"
        },
        {
            "location": "/ressentials/#arithmetic-functions",
            "text": "2+2\n        3*3\n        3*8+2\n        log10(1000)\n        log2(8)\n        abs(-10)\n        sqrt(81)",
            "title": "Arithmetic functions"
        },
        {
            "location": "/ressentials/#creating-objects",
            "text": "ls()  #see what objects are in the workspace\n        x <- 4\n        x\n        x = 3  #a single = is an assignment operator\n        x\n        x == 5 #a double == asks \"is the left side equivalent to the right side?\"\n        x + 2   #objects can be used in equations\n        y <- \"anyname\"\n        y\n        class(x)\n        class(y)\n        ls()",
            "title": "Creating objects"
        },
        {
            "location": "/ressentials/#vector-and-matrix",
            "text": "x1 <- c(1,2,3,4,5)\n        x1\n        class(x1)\n        length(x1)\n        x <- cbind(x1, x1+1)    #1 will be added to all the numbers in x1\n        x\n        class(x)       #what kind of object is x?\n        dim(x)         #the dimension of matrix\n        x1[1:3]        #use [] to get subsets of a vector\n        x[1,]          #use [,] to get subsets of a matrix (or dataframe)\n        x[,1]\n        x[,-1]\n        x[c(1,2),]\n        x[-c(1,3),]\n        colnames(x)\n        colnames(x) <-c(\"A\",\"B\")\n        rownames(x) <-c(\"C\",\"D\",\"E\",\"F\",\"G\")\n        x",
            "title": "Vector and Matrix"
        },
        {
            "location": "/ressentials/#data-frames",
            "text": "z <- data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F)\n        class(z)\n        names(z)\n        dim(z)\n        class(z$A)\n        class(z$B)\n        class(z$C)\n        class(z$D)\n        z$B\n        z$C",
            "title": "Data Frames"
        },
        {
            "location": "/ressentials/#more-ways-to-subset-dataframes",
            "text": "z$B\n        z[[2]]\n        z[,2]   #these first 3 give equivalent results\n        z[,1:2]\n        z[,c(1,3)]\n        z[c(1,3:5),]",
            "title": "More ways to subset dataframes"
        },
        {
            "location": "/ressentials/#lists",
            "text": "mylist <- list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\"))\n        class(mylist)\n        mylist\n        names(mylist)\n        class(mylist$first)\n        class(mylist$second)",
            "title": "Lists"
        },
        {
            "location": "/ressentials/#functions",
            "text": "my.add <- function(a, b) {a - b}\n        class(my.add)\n        my.add(4,99)\n        my.add(99,4)\n        my.add(b = 99, a = 4)",
            "title": "Functions"
        },
        {
            "location": "/ressentials/#various-directoryfilelibrary-manipulations",
            "text": "library(limma)  #load the limma package\n\n\n        #### Make sure the working directory is set to your file on the computer;\n\n        getwd()  #see what the current working directory is\n        setwd(\"????????????????\")  #change the working directory\n\n\n        #### Output a single object as a comma separated value file\n\n        write.csv(z, file=\"test.csv\")",
            "title": "Various directory/file/library manipulations"
        },
        {
            "location": "/ressentials/#save-all-the-objects-you-have-created-to-your-workspace",
            "text": "save.image()                #creates a default file named \".RData\"\n        save.image(\"intro.Rdata\")   #creates a named file",
            "title": "Save all the objects you have created to your workspace"
        },
        {
            "location": "/ressentials/#remove-objects-from-your-workspace",
            "text": "ls()\n        rm(x)          #remove a single object by name\n        ls()\n        rm(z,x1)       #remove multiple objects by name\n        ls()\n        load(\"intro.Rdata\")\n        ls()\n        rm(list=ls())  #remove all objects\n        ls()",
            "title": "Remove objects from your workspace"
        },
        {
            "location": "/ressentials/#save-a-history-of-all-the-commands-entered",
            "text": "savehistory(\"introhistory.Rhistory\")",
            "title": "Save a history of all the commands entered"
        },
        {
            "location": "/resources/",
            "text": "Here are some resources you might want to consult to learn more about how to use an HPC cluster\n\n\nSlurm\n\n\n\n\nOARC cluster user guide Amarel/Perceval\n - this is a must-read for any new users, even if you are an experienced Linux user \n\n\nOARC cluster community\n - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it\n\n\nintro videos by Kristina\n\n\nterse slurm tips\n\n\n\n\nLinux tutorials\n\n\n\n\nLinux tutorial by Galen\n\n\nUNIX Tutorial for Beginners\n\n\nRutgers Linux course\n Rutgers course teaching Linux for engineering students, runs every semester\n\n\nCornell virtual workshop stuff\n - interactive if you have xsede login (?)\n\n\nSoftware Carpentry\n - Software Carpentry has a number of lectures and workshops on many computing subjects\n\n\n\n\nGraphical user interface\n\n\n\n\nweb-based access to the cluster (still testing) - only from campus or VPN\n\n\n\n\nResources on the web\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources"
        },
        {
            "location": "/resources/#slurm",
            "text": "OARC cluster user guide Amarel/Perceval  - this is a must-read for any new users, even if you are an experienced Linux user   OARC cluster community  - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it  intro videos by Kristina  terse slurm tips",
            "title": "Slurm"
        },
        {
            "location": "/resources/#linux-tutorials",
            "text": "Linux tutorial by Galen  UNIX Tutorial for Beginners  Rutgers Linux course  Rutgers course teaching Linux for engineering students, runs every semester  Cornell virtual workshop stuff  - interactive if you have xsede login (?)  Software Carpentry  - Software Carpentry has a number of lectures and workshops on many computing subjects",
            "title": "Linux tutorials"
        },
        {
            "location": "/resources/#graphical-user-interface",
            "text": "web-based access to the cluster (still testing) - only from campus or VPN",
            "title": "Graphical user interface"
        },
        {
            "location": "/resources/#resources-on-the-web",
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources on the web"
        },
        {
            "location": "/cheatsheet/",
            "text": "Command\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nls -lag\n\n\nlist access rights for all files\n\n\n\n\n\n\nchmod [options] file\n\n\nchange access rights for named file\n\n\n\n\n\n\ncommand &\n\n\nrun command in background\n\n\n\n\n\n\n^C\n\n\nkill the job running in the foreground\n\n\n\n\n\n\n^Z\n\n\nsuspend the job running in the foreground\n\n\n\n\n\n\nbg\n\n\nbackground the suspended job\n\n\n\n\n\n\njobs\n\n\nlist current jobs\n\n\n\n\n\n\nfg %1\n\n\nforeground job number 1\n\n\n\n\n\n\nkill %1\n\n\nkill job number 1\n\n\n\n\n\n\nps\n\n\nlist current processes\n\n\n\n\n\n\nkill 26152\n\n\nkill process number 26152",
            "title": "CheatSheet"
        }
    ]
}