{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Office of Advanced Research Computing at Rutgers! \n\n\n\n\nOARC is university-wide initiative that is to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training.\n\n\n\n\nFor more information on OARC, including how to get access or become owners, please visit \nour web page\n\n\nThese pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read \nAmarel user guide\n as it has slurm tips and examples. \n\n\n\n\nFor users familiar with Linux but new to \nslurm\n, follow \nthis path\n\n\nFor users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets",
            "title": "Home"
        },
        {
            "location": "/howto/",
            "text": "Tunneling\n\n\nThis is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. \n\n\n\n\nrun jupyter notebook as a slurm job \n\n\nfind out on which compute node jupyter notebook ended up\n\n\nin another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\n\n\n\n\n# This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID\n\n\n\n\nVideo expaining the steps above: \n\n\n\nHow to launch Jupyter notebook on the cluster\n\n\nThere is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: \n\n\nmodule use /projects/community/modulefiles\nmodule load py-data-science-stack\n\n\n\n\nCopy this into a script file like \nstart_jupyter.sh\n\n\n#!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n\n\n\nThen run \nsbatch start_jupyter.sh\n on amarel login node. Now you need to find which node the jupyter notebook is running at. Do \nsqueue -u <your net id>\n to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port. \n\n\nYoutube video that explains this:",
            "title": "HowTo"
        },
        {
            "location": "/howto/#tunneling",
            "text": "This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster.    run jupyter notebook as a slurm job   find out on which compute node jupyter notebook ended up  in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)   # This procedure works not only with jupyter notebook, but any web app running on a compute node\n\n#command to run jupyter notebook \nsrun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889\n\n# to start jupyter succintly \nsbatch start_jupyter.sh      #start jupyter job on port 8889\nsqueue -u kp807              #find out on which node is the job running - say it's slepner009\n\n# in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here)\nssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu   # modify slepner009, the ports, the netID  Video expaining the steps above:",
            "title": "Tunneling"
        },
        {
            "location": "/howto/#how-to-launch-jupyter-notebook-on-the-cluster",
            "text": "There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands:   module use /projects/community/modulefiles\nmodule load py-data-science-stack  Copy this into a script file like  start_jupyter.sh  #!/bin/bash\n\n#SBATCH --partition=main             # Partition (job queue)\n#SBATCH --job-name=jupyter          # Assign an short name to your job\n#SBATCH --nodes=1                    # Number of nodes you require\n#SBATCH --ntasks=1                   # Total # of tasks across all nodes\n#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)\n#SBATCH --mem=4000                 # Real memory (RAM) required (MB)\n#SBATCH --time=01:00:00              # Total run time limit (HH:MM:SS)\n#SBATCH --output=slurm.%N.%j.out     # STDOUT output file\n#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional) \n\nexport XDG_RUNTIME_DIR=$HOME/tmp   ## needed for jupyter writting temporary files\n\nmodule use /projects/community/modulefiles \nmodule load py-data-science-stack         # loads anaconda\nsource activate pytorch-0.4.0\n\n#run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means \"any ip as interface\"\nsrun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889  Then run  sbatch start_jupyter.sh  on amarel login node. Now you need to find which node the jupyter notebook is running at. Do  squeue -u <your net id>  to see the slurm jobs you are running. \nYou should then do the port tunneling described in the previous section and open local browser at that port.   Youtube video that explains this:",
            "title": "How to launch Jupyter notebook on the cluster"
        },
        {
            "location": "/resources/",
            "text": "Here are some resources you might want to consult to learn more about how to use an HPC cluster\n\n\nSlurm\n\n\n\n\nOARC cluster user guide Amarel/Perceval\n - this is a must-read for any new users, even if you are an experienced Linux user \n\n\nOARC cluster community\n - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it\n\n\nintro videos by Kristina\n\n\nterse slurm tips\n\n\n\n\nLinux tutorials\n\n\n\n\nlinux tutorial by Galen\n\n\nto be continued\n\n\n\n\nGraphical user interface\n\n\n\n\nweb-based access to the cluster (still testing) - only from campus or VPN\n\n\n\n\nResources on the web\n\n\n\n\nmarkdown editor\n - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources"
        },
        {
            "location": "/resources/#slurm",
            "text": "OARC cluster user guide Amarel/Perceval  - this is a must-read for any new users, even if you are an experienced Linux user   OARC cluster community  - a lot of software is installed already by other users of the cluster, this describes how to use it and contribute to it  intro videos by Kristina  terse slurm tips",
            "title": "Slurm"
        },
        {
            "location": "/resources/#linux-tutorials",
            "text": "linux tutorial by Galen  to be continued",
            "title": "Linux tutorials"
        },
        {
            "location": "/resources/#graphical-user-interface",
            "text": "web-based access to the cluster (still testing) - only from campus or VPN",
            "title": "Graphical user interface"
        },
        {
            "location": "/resources/#resources-on-the-web",
            "text": "markdown editor  - lets you view the finished (rendered) markdown side by side with raw markdown",
            "title": "Resources on the web"
        }
    ]
}