{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Office of Advanced Research Computing at Rutgers! OARC is a university-wide initiative that aims to develop and implement a strategic vision for centralizing the advanced research computing and data cyberinfrastructure (ACI) ecosystem at Rutgers. OARC has the goal of providing Rutgers researchers with essential computing and data handling capabilities, and students with necessary exposure and training, through centralized resources, services and training. For more information on OARC, including how to get access or become owners , please visit our web page These pages are a collection of resources to help you to utilize the cluster more effectively. Even if you are a very experienced Linux user, you will want to read cluster user guide as it has slurm tips and examples. WARNING - READ! Do not run large computational jobs on the login node. Use slurm to allocate resources on the compute node. Failure to respect the golden rule can get your account suspended. Number of jobs to submit should not exceed 5,000 at a time - queue is currently 10,000 and slurm will go to sleep if that is exceeded. Right-size your jobs: jobs may run sooner if the resources requested are smaller and your future priority is degraded if you have asked for (even if not used) a lot of resources lately. use -n option in sbatch only if you know what you are doing. Multithreaded but not multi-node jobs should use -c option. TODO: add more complete list Learning paths If you understand material in lmod cheatsheet and slurm cheatsheet you are good to go. For users familiar with Linux but new to slurm , check out environment modules and intro to slurm . For users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets, or use OnDemand until you feel more comfortable with Linux. If you wish to learn about HPC in detail, check out HPC Carpentry course for beginners.","title":"Home"},{"location":"#warning-read","text":"Do not run large computational jobs on the login node. Use slurm to allocate resources on the compute node. Failure to respect the golden rule can get your account suspended. Number of jobs to submit should not exceed 5,000 at a time - queue is currently 10,000 and slurm will go to sleep if that is exceeded. Right-size your jobs: jobs may run sooner if the resources requested are smaller and your future priority is degraded if you have asked for (even if not used) a lot of resources lately. use -n option in sbatch only if you know what you are doing. Multithreaded but not multi-node jobs should use -c option. TODO: add more complete list","title":"WARNING - READ!"},{"location":"#learning-paths","text":"If you understand material in lmod cheatsheet and slurm cheatsheet you are good to go. For users familiar with Linux but new to slurm , check out environment modules and intro to slurm . For users not familiar with Linux, please familiarize yourself with Linux through tutorials and cheatsheets, or use OnDemand until you feel more comfortable with Linux. If you wish to learn about HPC in detail, check out HPC Carpentry course for beginners.","title":"Learning paths"},{"location":"CheatSheet/","text":"Bash cheatsheet (command line): command description usage example ls -ltra mydir list all files in mydir in descending order of creation, with permissions ls -ltra . du disk usage, e.g. how much space does your directory occupy, -h human-readable du -h mydir which see where command is installed which python pwd which directory I'm in pwd man manual page for command man cut grep filter for lines which fit pattern cat myfile grep GATK cut -d -f split line by delimiter and get field number 3 cat myfile cut -d'_' -f3 sort sort lines, often used with uniq sort myfile uniq uniq suppress repeated lines, works only if sorted see above example less paginated output less myfile redirect output (e.g. list files and save filenames in aaa.txt) ls aaa.txt append output to existing file echo \"blah\" aaa.txt find find files with some properties e.g. display all files recursively from current directory find . chmod change permissions on a file or directory, eg. make myscript.sh executable for user chmod u+x myscript.sh top display most intensive processes top ps auxw list processes ps auxw time myscript.sh measure how much time does myscript.sh take to finish execution echo $variable output the value of the variable echo $USER For a more complete cheatsheet including ifs, loops and functions, see this website Here is a sample: dirname /home/kp807/projects/cluster_reports/cluster.csv # everything but the last part basename /home/kp807/projects/cluster_reports/cluster.csv # last part of path CURRENT_DIR=`pwd` ; echo $CURRENT_DIR # backtick for execution of bash command echo 'projects_cluster_file.csv' | cut -d '_' -f1 # split name to retain a part of file echo $((1 + 2)) # double parenthesis for arithmetic expressions echo filename_fly{5..10}.csv # list comprehensions {start..end} # for-loop: for file in filename_fly{5..10}.csv; do echo $file ; done # if-statement - 2 examples if [ 1 -gt 2 ]; then echo '1 2' ; else echo '1 2' ; fi if [ -d newdir ]; then echo 'directory exists' ; else echo 'this directory doesnt exist' ; fi #variable assignment a=10; echo $a #good - no spaces b = 10; echo $b #bad - spaces around = Environment modules (lmod) cheatsheet Command Description module avail show a list of the modules available module load java loads the named software module i.e. java module use /projects/community/modulefiles Now if you do module avail, it also shows modules created by other users module spider shows a comprehensive list of all available modules or type name after spider to show details about specific module module keyword anaconda looks for the keyword anaconda in the description of a module module purge removes all loaded module ml lists modules loaded To learm more about environments and modules, see lmod documenation . Slurm cheatsheet Command Description sinfo info about utilization and resources sbatch my_slurm_script.sh submit a slurm script srun -N 1 -c 2 -t 1:00:00 --pty bash run interactive shell on a compute node for 1 hour scancel 123456 cancel job 123456 that's currently running or queued sacct -u kp807 --start=2018-05-02 show all jobs by user kp807 since some date scontrol show job 123456 -dd details about job that's running scontrol show node hal0001 details about node ha0001 scontrol --help generic way to invoke quick help on any command sacct -o MaxRSS,job,jobName,state keep track of how much memory you are using sacct -o Elapsed,Start,End,job,jobName estimate the wall time To learn more about slurm, see slurm documentation Check quota cheatsheet NOTE: /home and /scratch are two different filesystems, with different backup policies. Command Description of which file usage mmlsquota scratch --block-size=auto file usage on /scratch/netid for the user's netid mmlsquota cache --block-size=auto file usage on /home/netid for the user's netid mmlsquota -j foran projectsp --block-size=auto quota and usage of the whole fileset foran - you need to know if it is projectsp, projectsn or projectc - Piscataway, Newark or Camden du -hs /directory/to/query/* human-readable sizes of all 1st-level subdirectories of /directory/to/query/ Linux basic commands File tree Command Meaning ls list files and directories ls -a list all files and directories mkdir make a directory cd directory change to named directory cd change to home-directory cd ~ change to home-directory cd .. change to parent directory pwd display the path of the current directory Moving and viewing files Command Meaning cp file1 file2 copy file1 and call it file2 mv file1 file2 move or rename file1 to file2 rm file remove a file rmdir directory remove a directory (only if empty) cat file display a file less file display a file a page at a time head -19 file display the first 19 lines of a file tail -19 file display the last 19 lines of a file grep Finally myfile.txt search myfile.txt for word Finally wc file count number of lines/words/characters in file Piping Command Description command file redirect standard output to a file command file append standard output to a file command file redirect standard input from a file command1 command2 pipe the output of command1 to the input of command2 cat file1 file2 file0 concatenate file1 and file2 to file0 sort sort data who list users currently logged in * match any number of characters ? match one character man command read the online manual page for a command whatis command brief description of a command apropos keyword match commands with keyword in their man pages Permissions Symbol Description u user g group o other a all r read w write (and delete) x execute (and access directory) u+x add executing permission for user u-x take away executing permission Output of ls -ltra drwxr-x--x 12 kp807 kp807 4096 Jun 26 23:33 .rstudio d = it's a directory rwx = first group of permissions, readable, writeable and executable by user r-x = second group of permissions, readable and executable by group, but not writeable by group --x = third group of permissions, executable by others, but not readable and writeable by others .rstudio = hidden directory Linux on the web Galen's tutorial Excellent short lessons Alexei's course he runs for engineering students every semester Cornell virtual workshop - interactive if you have xsede login; many HPC topics available Software carpentry - list of introductory programming courses","title":"Cheat Sheets"},{"location":"CheatSheet/#bash-cheatsheet-command-line","text":"command description usage example ls -ltra mydir list all files in mydir in descending order of creation, with permissions ls -ltra . du disk usage, e.g. how much space does your directory occupy, -h human-readable du -h mydir which see where command is installed which python pwd which directory I'm in pwd man manual page for command man cut grep filter for lines which fit pattern cat myfile grep GATK cut -d -f split line by delimiter and get field number 3 cat myfile cut -d'_' -f3 sort sort lines, often used with uniq sort myfile uniq uniq suppress repeated lines, works only if sorted see above example less paginated output less myfile redirect output (e.g. list files and save filenames in aaa.txt) ls aaa.txt append output to existing file echo \"blah\" aaa.txt find find files with some properties e.g. display all files recursively from current directory find . chmod change permissions on a file or directory, eg. make myscript.sh executable for user chmod u+x myscript.sh top display most intensive processes top ps auxw list processes ps auxw time myscript.sh measure how much time does myscript.sh take to finish execution echo $variable output the value of the variable echo $USER For a more complete cheatsheet including ifs, loops and functions, see this website Here is a sample: dirname /home/kp807/projects/cluster_reports/cluster.csv # everything but the last part basename /home/kp807/projects/cluster_reports/cluster.csv # last part of path CURRENT_DIR=`pwd` ; echo $CURRENT_DIR # backtick for execution of bash command echo 'projects_cluster_file.csv' | cut -d '_' -f1 # split name to retain a part of file echo $((1 + 2)) # double parenthesis for arithmetic expressions echo filename_fly{5..10}.csv # list comprehensions {start..end} # for-loop: for file in filename_fly{5..10}.csv; do echo $file ; done # if-statement - 2 examples if [ 1 -gt 2 ]; then echo '1 2' ; else echo '1 2' ; fi if [ -d newdir ]; then echo 'directory exists' ; else echo 'this directory doesnt exist' ; fi #variable assignment a=10; echo $a #good - no spaces b = 10; echo $b #bad - spaces around =","title":"Bash cheatsheet (command line):"},{"location":"CheatSheet/#environment-modules-lmod-cheatsheet","text":"Command Description module avail show a list of the modules available module load java loads the named software module i.e. java module use /projects/community/modulefiles Now if you do module avail, it also shows modules created by other users module spider shows a comprehensive list of all available modules or type name after spider to show details about specific module module keyword anaconda looks for the keyword anaconda in the description of a module module purge removes all loaded module ml lists modules loaded To learm more about environments and modules, see lmod documenation .","title":"Environment modules (lmod) cheatsheet"},{"location":"CheatSheet/#slurm-cheatsheet","text":"Command Description sinfo info about utilization and resources sbatch my_slurm_script.sh submit a slurm script srun -N 1 -c 2 -t 1:00:00 --pty bash run interactive shell on a compute node for 1 hour scancel 123456 cancel job 123456 that's currently running or queued sacct -u kp807 --start=2018-05-02 show all jobs by user kp807 since some date scontrol show job 123456 -dd details about job that's running scontrol show node hal0001 details about node ha0001 scontrol --help generic way to invoke quick help on any command sacct -o MaxRSS,job,jobName,state keep track of how much memory you are using sacct -o Elapsed,Start,End,job,jobName estimate the wall time To learn more about slurm, see slurm documentation","title":"Slurm cheatsheet"},{"location":"CheatSheet/#check-quota-cheatsheet","text":"NOTE: /home and /scratch are two different filesystems, with different backup policies. Command Description of which file usage mmlsquota scratch --block-size=auto file usage on /scratch/netid for the user's netid mmlsquota cache --block-size=auto file usage on /home/netid for the user's netid mmlsquota -j foran projectsp --block-size=auto quota and usage of the whole fileset foran - you need to know if it is projectsp, projectsn or projectc - Piscataway, Newark or Camden du -hs /directory/to/query/* human-readable sizes of all 1st-level subdirectories of /directory/to/query/","title":"Check quota cheatsheet"},{"location":"CheatSheet/#linux-basic-commands","text":"","title":"Linux basic commands"},{"location":"CheatSheet/#file-tree","text":"Command Meaning ls list files and directories ls -a list all files and directories mkdir make a directory cd directory change to named directory cd change to home-directory cd ~ change to home-directory cd .. change to parent directory pwd display the path of the current directory","title":"File tree"},{"location":"CheatSheet/#moving-and-viewing-files","text":"Command Meaning cp file1 file2 copy file1 and call it file2 mv file1 file2 move or rename file1 to file2 rm file remove a file rmdir directory remove a directory (only if empty) cat file display a file less file display a file a page at a time head -19 file display the first 19 lines of a file tail -19 file display the last 19 lines of a file grep Finally myfile.txt search myfile.txt for word Finally wc file count number of lines/words/characters in file","title":"Moving and viewing files"},{"location":"CheatSheet/#piping","text":"Command Description command file redirect standard output to a file command file append standard output to a file command file redirect standard input from a file command1 command2 pipe the output of command1 to the input of command2 cat file1 file2 file0 concatenate file1 and file2 to file0 sort sort data who list users currently logged in * match any number of characters ? match one character man command read the online manual page for a command whatis command brief description of a command apropos keyword match commands with keyword in their man pages","title":"Piping"},{"location":"CheatSheet/#permissions","text":"Symbol Description u user g group o other a all r read w write (and delete) x execute (and access directory) u+x add executing permission for user u-x take away executing permission Output of ls -ltra drwxr-x--x 12 kp807 kp807 4096 Jun 26 23:33 .rstudio d = it's a directory rwx = first group of permissions, readable, writeable and executable by user r-x = second group of permissions, readable and executable by group, but not writeable by group --x = third group of permissions, executable by others, but not readable and writeable by others .rstudio = hidden directory","title":"Permissions"},{"location":"CheatSheet/#linux-on-the-web","text":"Galen's tutorial Excellent short lessons Alexei's course he runs for engineering students every semester Cornell virtual workshop - interactive if you have xsede login; many HPC topics available Software carpentry - list of introductory programming courses","title":"Linux on the web"},{"location":"cloud/","text":"Introduction This page explains how to apply for Google Resarch and Education Credits correctly, and in the process minimize pain for you and for us and OIT. Google credits for Education This is to enable you to run courses or workshops in the cloud. you do not need to provide a charging string or a credit card number your students will get a coupon to redeem if you go over allotted resources, the project access shuts down automatically Google credits for Research apply here: https://edu.google.com/programs/credits/research/?modal_active=none $5K GCP credits per project (or can ask for more, up to $25K) -PhD students and above can apply 6 months project (they will be extending it to a year based on feedback they got) mean time to hear back to be approved - 2 weeks after you have been awarded credits, you will need a rutgers gcp account, such as kp807@gcp.rutgers.edu. To apply for the account, please fill out the form here: https://oit.rutgers.edu/request-gcp-rutgers-edu-account you will also need to request an environment: https://oitforms.rutgers.edu/request-cloud-environment","title":"Introduction"},{"location":"cloud/#introduction","text":"This page explains how to apply for Google Resarch and Education Credits correctly, and in the process minimize pain for you and for us and OIT.","title":"Introduction"},{"location":"cloud/#google-credits-for-education","text":"This is to enable you to run courses or workshops in the cloud. you do not need to provide a charging string or a credit card number your students will get a coupon to redeem if you go over allotted resources, the project access shuts down automatically","title":"Google credits for Education"},{"location":"cloud/#google-credits-for-research","text":"apply here: https://edu.google.com/programs/credits/research/?modal_active=none $5K GCP credits per project (or can ask for more, up to $25K) -PhD students and above can apply 6 months project (they will be extending it to a year based on feedback they got) mean time to hear back to be approved - 2 weeks after you have been awarded credits, you will need a rutgers gcp account, such as kp807@gcp.rutgers.edu. To apply for the account, please fill out the form here: https://oit.rutgers.edu/request-gcp-rutgers-edu-account you will also need to request an environment: https://oitforms.rutgers.edu/request-cloud-environment","title":"Google credits for Research"},{"location":"faq/","text":"Usual FAQs What happened... tip to gather info If you see a problem with your job on a cluster, please DO NOT CANCEL your job and please report it to help@oarc.rutgers.edu. In this case we can troubleshoot and investigate why something is not working. We have no control and no information about why jobs got cancelled by user. I can't reach Amarel from outside the campus You can only connect to Amarel through a VPN to access the Rutgers network when off campus. Download the Pulse Secure VPN here Follow the instructions on this webpage and fill in the server URL: ssl-vpn.rutgers.edu Emailing for help who do I email for help? Please email help@oarc.rutgers.edu - this will reach all research scientists, but not system admins. Research scientists will include sysadmins if necessary. (We are trying to streamline communications.) Where are snapshots for /home stored? Normally, they are in /home/.snapshots . error and output files In the directory \"slurm.slepner009.3695956.err\" was created. Is this what should be happening? Yes, if one of your options to run a slurm job included a line like --error=slurm.%N.%j.err then any error messages during execution will be going to a file you can inspect afterwards to determine how to fix the job's failure. E.g. if your jobs's results are supposed to go to results directory, but the directory \"results\" was not created before running the job. Since the slurm script is accessing a nonexistent path, it aborts. You can't see the job with squeue because it's not running (it finished with \"exit 1\" i.e. failed). You can use the command sacct (see slurm cheatsheet) to see some details of finished jobs (but not as many details as with scontrol show job 12345 ). common errors and their fixes I'm getting this error. Why? What's happening? error explanation fix sbatch: error: Slurm temporarily unable to accept job, sleeping and retrying. The Slurm queue reached its maximum and is not accepting new jobs Wait until the queue clears up and resubmit Batch script contains DOS line breaks (\\r\\n)sbatch: error: instead of expected UNIX line breaks (\\n). If you created or edited the file in Windows, it can insert Windows line breaks, which are not the same as unix line breaks. convert between the two formats with dos2unix and unix2dos commands - see this explanation No such file or directory A path you gave is wrong or nonexistent Check the exact path; if file exists, check permissions numpy and other hidden modules How do I get numpy? You need to load libraries that are needed by numpy first. For example Intel Math Kernel Library: module load intel_mkl/17.0.2 python/3.5.2 R DLLpath When I install R package xxx, I get an error dyn.load(file, DLLpath = DLLpath, ...): A lot of R packages are actually written in other languages, very often C or C++. Depending on how the R package is packaged, it can rely on \"shared objects\", recognized by .so extension. These .so files are created by compilers, but different compiler versions will produce files which are sometimes not compatible. So either the .so is not found because its file path is missing, or it has been compiled for a different version of the operating system or different version of the software. Complicating this, sometimes software which is in your path (e.g. anaconda will put itself in your .bashrc so it will be available) has its own version of the library you want to load - e.g. R package may need a .so file that anaconda installation already contains, but is not compatible with your R package. Solutions can vary: remove paths to known other softeware (such as Anaconda) remove other R packages that might be interfering (e.g. by moving it out of your path, not necessarily deleting) email notifications How do I make sure that I get an email when my slurm job finally starts, finishes, or fails? It is possible to specify in the slurm batch script or srun command that you wish to receive an email notification. There are two options associated with this: mail-type and mail-user . For example, --mail-type=BEGIN and --mail-user=kp807@rutgers.edu . Without mail-type specified, it doesn't send an email If you have domain other than rutgers.edu , slurm doesn't properly send it (e.g. kp807@oarc.rutgers.edu won't work, but kp807@rutgers.edu will.) Other options for mail-type are: BEGIN (send email when job starts, useful for interactive jobs), END (send email when job finishes), FAIL (send email when job finishes), ALL(send email for all 3). file permissions One way to control the access to the dataset is via getfacl and setfacl commands (see https://www.computerhope.com/unix/usetfacl.htm). Here is an example of how to give permission to user user1, and only to this user, to read one of my subdirectories, sharedir : setfacl -m u:user1:rx /home/kp807/sharedir Here is how you would see what are the permissions to this directory: getfacl /home/kp807/sharedir -m option means modify permissions u means user rx means read and execute. You must give execute permissions to a directory if you want to execute a listing of the directory. You also have to give x permission to every directory that needs to be traversed during execution and reading. E.g. you'd also have to execute setfacl -m u:user1:x /home/kp807/ NOTE OF CAUTION: By default, Linux umask is set to 022. This means that by default, all directories and files are readable by group and other. The only thing stopping someone else reading your home directory is lack of x flag on the home directory. If you add this flag to a user, now this user will also be able to read any readable files in your home directory. So use x flag judiciously, or simply change all your permissions (and user's umask in .bashrc to something more restrictive). python and conda Backgroud: bash has some predefined variables that programs (or you) can use. You can get a list of those variables by typing env command in your bash terminal. By convention, the names are uppercase. to set variables and get variables use = (without spaces) and echo with ${NAME_OF_VARIABLE} . E.g. PWD=/home/kris to set the PWD variable, or echo $PATH to print the value of variable PATH PATH is a bash variable that tells the systems in which directories to look for the programs/commands you invoke from the bash terminal. You can see the current path by echo $PATH .bashrc file is the file that always gets called when you start up a terminal, and sets PATH variable to some default How to install anaconda with python 2, but not set it as default anaconda version install anaconda2 (https://www.anaconda.com/download/) - follow instructions for your OS anaconda puts itself automatically in your .bashrc file. You need to find the line that anaconda added and comment it out or delete from your .bashrc file so that anaconda2 is not your default every time you log in. The lines to comment out look like this: # added by Anaconda2 installer export PATH= /home/kris/anaconda2/bin:$PATH When you want to use anaconda2 instead of anaconda3, you reset your PATH variable manually, by issuing this command in your bash terminal: export PATH=\"/home/$USER/anaconda2/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin\" This will be valid ONLY in that terminal, so remember to launch any python2 scripts in that terminal, otherwise python will default to your other default Verify which conda and python version are you using by this command: which python When you are sure that you are using conda from anaconda2, create new conda environment with a descriptive name, e.g. conda create --name mytensorflow-1.5 Activate this environment: source activate mytensorflow-1.5 Install tensorflow-1.5 into this environment: pip install tensorflow==1.5.0 After having finished your work, source deactivate gets you out of the mytensorflow-1.5 environment. Now you have installed your environment. Next time, these are the commands to get you in the python2, mytensorflow-1.5 environment ready to do your python2 work: export PATH= /home/$USER/anaconda2/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin source activate mytensorflow-1.5 My /scratch disappeared after October 6,2018 outage. Where is it? The filesystem has been moved to new hardware. As a result, what used to be /scratch before the upgrade is now /oldscratch . Please copy any files you need to retain to new /scratch. /oldscratch will be retired soon. I get a message telling me someone may be spoofing, when I try to login to Perceval. Am I in danger? If there was a recent outage, you are not in danger. During the upgrade the fingerprint of the server can have changed. Go to your .ssh/authorized_hosts or .ssh/known_hosts file and delete the line containing perceval information. The message you got before, that looked like the following, will disappear next time you login. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for perceval.hpc.rutgers.edu has changed, and the key for the corresponding IP address 172.16.94.50 is unknown. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. What is the status of GPU nodes? To check the current status of GPU nodes with reason: check with command sinfo -R -l -p gpu . Here is some output: [kp807@perceval1 ~]$ sinfo -R -l -p gpu Mon Oct 15 10:06:40 2018 REASON USER TIMESTAMP STATE NODELIST NHC: check_cmd_outpu root(0) 2018-10-12T03:22:03 drain cuda[001-008] [kp807@amarel1 ~]$ sinfo -R -l -p gpu Mon Oct 15 10:06:19 2018 REASON USER TIMESTAMP STATE NODELIST no cnx ipmi root(0) 2018-10-13T11:51:15 drain* gpu[001-003] IP root(0) 2018-10-15T09:53:10 drng pascal[001-004] not part of mentat2 root(0) 2018-10-13T11:52:52 drain gpu[005-006] IP root(0) 2018-10-15T09:59:24 drain pascal005 IP root(0) 2018-10-15T09:53:10 drain pascal006 How can I troubleshoot my OnDemand session? Each OnDemand session leaves an output.log file in your home directory. The exact path depends on the ondemand app you are using (Jupyter,RStudio,Desktop...). So, if you find that your session terminates for some reason (and most common reasons are, either the job was preempted, or it ran out of memory), then you can after the fact, find the directory that contains the \"trail of evidence\" for that session. Each session is given a unique directory (a long hash string), and the best thing is to look at the time stamps to identify which session was problematic. Let's illustrate that with some linux commands. Go into the directory which contains the record of all ondemand sessions for Amarel Desktop app, and list the contents by time order (most recent listed last): [kp807@amarel2 ~]$ cd ondemand/data/sys/dashboard/batch_connect/sys/bc_desktop/amarel/output/ [kp807@amarel2 output]$ ls -ltra drwxr-xr-x. 3 kp807 kp807 32768 Oct 17 11:05 da10e852-4475-43e0-827f-9114e9116511 Having identified the session, look at the output.log file to see if there are any slurm messages (you can ignore warnings, they usually don't interfere with the functioning of the desktop). [kp807@amarel2 output]$ cat 25d6dec3-d32c-470e-a18e-861148cdcabb/output.log (.....) slurmstepd: error: Job 133540017 exceeded memory limit (52578904 52428800), being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 133540017 ON slepner015 CANCELLED AT 2019-10-17T10:59:32 *** In this case, the application you are using within the desktop requires more memory than has been allocated to you. You have several options: - Relaunch the session with more memory - use Advanced Desktop, where you can list verbatim slurm options, including memory - rewrite your job as a batch script - large memory usage is an indication it should really be a long-running computation, and batch submission is best suited for this.","title":"FAQ"},{"location":"faq/#usual-faqs","text":"","title":"Usual FAQs"},{"location":"faq/#what-happened-tip-to-gather-info","text":"If you see a problem with your job on a cluster, please DO NOT CANCEL your job and please report it to help@oarc.rutgers.edu. In this case we can troubleshoot and investigate why something is not working. We have no control and no information about why jobs got cancelled by user.","title":"What happened... tip to gather info"},{"location":"faq/#i-cant-reach-amarel-from-outside-the-campus","text":"You can only connect to Amarel through a VPN to access the Rutgers network when off campus. Download the Pulse Secure VPN here Follow the instructions on this webpage and fill in the server URL: ssl-vpn.rutgers.edu","title":"I can't reach Amarel from outside the campus"},{"location":"faq/#emailing-for-help","text":"who do I email for help? Please email help@oarc.rutgers.edu - this will reach all research scientists, but not system admins. Research scientists will include sysadmins if necessary. (We are trying to streamline communications.)","title":"Emailing for help"},{"location":"faq/#where-are-snapshots-for-home-stored","text":"Normally, they are in /home/.snapshots .","title":"Where are snapshots for /home stored?"},{"location":"faq/#error-and-output-files","text":"In the directory \"slurm.slepner009.3695956.err\" was created. Is this what should be happening? Yes, if one of your options to run a slurm job included a line like --error=slurm.%N.%j.err then any error messages during execution will be going to a file you can inspect afterwards to determine how to fix the job's failure. E.g. if your jobs's results are supposed to go to results directory, but the directory \"results\" was not created before running the job. Since the slurm script is accessing a nonexistent path, it aborts. You can't see the job with squeue because it's not running (it finished with \"exit 1\" i.e. failed). You can use the command sacct (see slurm cheatsheet) to see some details of finished jobs (but not as many details as with scontrol show job 12345 ).","title":"error and output files"},{"location":"faq/#common-errors-and-their-fixes","text":"I'm getting this error. Why? What's happening? error explanation fix sbatch: error: Slurm temporarily unable to accept job, sleeping and retrying. The Slurm queue reached its maximum and is not accepting new jobs Wait until the queue clears up and resubmit Batch script contains DOS line breaks (\\r\\n)sbatch: error: instead of expected UNIX line breaks (\\n). If you created or edited the file in Windows, it can insert Windows line breaks, which are not the same as unix line breaks. convert between the two formats with dos2unix and unix2dos commands - see this explanation No such file or directory A path you gave is wrong or nonexistent Check the exact path; if file exists, check permissions","title":"common errors and their fixes"},{"location":"faq/#numpy-and-other-hidden-modules","text":"How do I get numpy? You need to load libraries that are needed by numpy first. For example Intel Math Kernel Library: module load intel_mkl/17.0.2 python/3.5.2","title":"numpy and other hidden modules"},{"location":"faq/#r-dllpath","text":"When I install R package xxx, I get an error dyn.load(file, DLLpath = DLLpath, ...): A lot of R packages are actually written in other languages, very often C or C++. Depending on how the R package is packaged, it can rely on \"shared objects\", recognized by .so extension. These .so files are created by compilers, but different compiler versions will produce files which are sometimes not compatible. So either the .so is not found because its file path is missing, or it has been compiled for a different version of the operating system or different version of the software. Complicating this, sometimes software which is in your path (e.g. anaconda will put itself in your .bashrc so it will be available) has its own version of the library you want to load - e.g. R package may need a .so file that anaconda installation already contains, but is not compatible with your R package. Solutions can vary: remove paths to known other softeware (such as Anaconda) remove other R packages that might be interfering (e.g. by moving it out of your path, not necessarily deleting)","title":"R DLLpath"},{"location":"faq/#email-notifications","text":"How do I make sure that I get an email when my slurm job finally starts, finishes, or fails? It is possible to specify in the slurm batch script or srun command that you wish to receive an email notification. There are two options associated with this: mail-type and mail-user . For example, --mail-type=BEGIN and --mail-user=kp807@rutgers.edu . Without mail-type specified, it doesn't send an email If you have domain other than rutgers.edu , slurm doesn't properly send it (e.g. kp807@oarc.rutgers.edu won't work, but kp807@rutgers.edu will.) Other options for mail-type are: BEGIN (send email when job starts, useful for interactive jobs), END (send email when job finishes), FAIL (send email when job finishes), ALL(send email for all 3).","title":"email notifications"},{"location":"faq/#file-permissions","text":"One way to control the access to the dataset is via getfacl and setfacl commands (see https://www.computerhope.com/unix/usetfacl.htm). Here is an example of how to give permission to user user1, and only to this user, to read one of my subdirectories, sharedir : setfacl -m u:user1:rx /home/kp807/sharedir Here is how you would see what are the permissions to this directory: getfacl /home/kp807/sharedir -m option means modify permissions u means user rx means read and execute. You must give execute permissions to a directory if you want to execute a listing of the directory. You also have to give x permission to every directory that needs to be traversed during execution and reading. E.g. you'd also have to execute setfacl -m u:user1:x /home/kp807/ NOTE OF CAUTION: By default, Linux umask is set to 022. This means that by default, all directories and files are readable by group and other. The only thing stopping someone else reading your home directory is lack of x flag on the home directory. If you add this flag to a user, now this user will also be able to read any readable files in your home directory. So use x flag judiciously, or simply change all your permissions (and user's umask in .bashrc to something more restrictive).","title":"file permissions"},{"location":"faq/#python-and-conda","text":"Backgroud: bash has some predefined variables that programs (or you) can use. You can get a list of those variables by typing env command in your bash terminal. By convention, the names are uppercase. to set variables and get variables use = (without spaces) and echo with ${NAME_OF_VARIABLE} . E.g. PWD=/home/kris to set the PWD variable, or echo $PATH to print the value of variable PATH PATH is a bash variable that tells the systems in which directories to look for the programs/commands you invoke from the bash terminal. You can see the current path by echo $PATH .bashrc file is the file that always gets called when you start up a terminal, and sets PATH variable to some default How to install anaconda with python 2, but not set it as default anaconda version install anaconda2 (https://www.anaconda.com/download/) - follow instructions for your OS anaconda puts itself automatically in your .bashrc file. You need to find the line that anaconda added and comment it out or delete from your .bashrc file so that anaconda2 is not your default every time you log in. The lines to comment out look like this: # added by Anaconda2 installer export PATH= /home/kris/anaconda2/bin:$PATH When you want to use anaconda2 instead of anaconda3, you reset your PATH variable manually, by issuing this command in your bash terminal: export PATH=\"/home/$USER/anaconda2/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin\" This will be valid ONLY in that terminal, so remember to launch any python2 scripts in that terminal, otherwise python will default to your other default Verify which conda and python version are you using by this command: which python When you are sure that you are using conda from anaconda2, create new conda environment with a descriptive name, e.g. conda create --name mytensorflow-1.5 Activate this environment: source activate mytensorflow-1.5 Install tensorflow-1.5 into this environment: pip install tensorflow==1.5.0 After having finished your work, source deactivate gets you out of the mytensorflow-1.5 environment. Now you have installed your environment. Next time, these are the commands to get you in the python2, mytensorflow-1.5 environment ready to do your python2 work: export PATH= /home/$USER/anaconda2/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin source activate mytensorflow-1.5","title":"python and conda"},{"location":"faq/#my-scratch-disappeared-after-october-62018-outage-where-is-it","text":"The filesystem has been moved to new hardware. As a result, what used to be /scratch before the upgrade is now /oldscratch . Please copy any files you need to retain to new /scratch. /oldscratch will be retired soon.","title":"My /scratch disappeared after October 6,2018 outage. Where is it?"},{"location":"faq/#i-get-a-message-telling-me-someone-may-be-spoofing-when-i-try-to-login-to-perceval-am-i-in-danger","text":"If there was a recent outage, you are not in danger. During the upgrade the fingerprint of the server can have changed. Go to your .ssh/authorized_hosts or .ssh/known_hosts file and delete the line containing perceval information. The message you got before, that looked like the following, will disappear next time you login. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for perceval.hpc.rutgers.edu has changed, and the key for the corresponding IP address 172.16.94.50 is unknown. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed.","title":"I get a message telling me someone may be spoofing, when I try to login to Perceval. Am I in danger?"},{"location":"faq/#what-is-the-status-of-gpu-nodes","text":"To check the current status of GPU nodes with reason: check with command sinfo -R -l -p gpu . Here is some output: [kp807@perceval1 ~]$ sinfo -R -l -p gpu Mon Oct 15 10:06:40 2018 REASON USER TIMESTAMP STATE NODELIST NHC: check_cmd_outpu root(0) 2018-10-12T03:22:03 drain cuda[001-008] [kp807@amarel1 ~]$ sinfo -R -l -p gpu Mon Oct 15 10:06:19 2018 REASON USER TIMESTAMP STATE NODELIST no cnx ipmi root(0) 2018-10-13T11:51:15 drain* gpu[001-003] IP root(0) 2018-10-15T09:53:10 drng pascal[001-004] not part of mentat2 root(0) 2018-10-13T11:52:52 drain gpu[005-006] IP root(0) 2018-10-15T09:59:24 drain pascal005 IP root(0) 2018-10-15T09:53:10 drain pascal006","title":"What is the status of GPU nodes?"},{"location":"faq/#how-can-i-troubleshoot-my-ondemand-session","text":"Each OnDemand session leaves an output.log file in your home directory. The exact path depends on the ondemand app you are using (Jupyter,RStudio,Desktop...). So, if you find that your session terminates for some reason (and most common reasons are, either the job was preempted, or it ran out of memory), then you can after the fact, find the directory that contains the \"trail of evidence\" for that session. Each session is given a unique directory (a long hash string), and the best thing is to look at the time stamps to identify which session was problematic. Let's illustrate that with some linux commands. Go into the directory which contains the record of all ondemand sessions for Amarel Desktop app, and list the contents by time order (most recent listed last): [kp807@amarel2 ~]$ cd ondemand/data/sys/dashboard/batch_connect/sys/bc_desktop/amarel/output/ [kp807@amarel2 output]$ ls -ltra drwxr-xr-x. 3 kp807 kp807 32768 Oct 17 11:05 da10e852-4475-43e0-827f-9114e9116511 Having identified the session, look at the output.log file to see if there are any slurm messages (you can ignore warnings, they usually don't interfere with the functioning of the desktop). [kp807@amarel2 output]$ cat 25d6dec3-d32c-470e-a18e-861148cdcabb/output.log (.....) slurmstepd: error: Job 133540017 exceeded memory limit (52578904 52428800), being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 133540017 ON slepner015 CANCELLED AT 2019-10-17T10:59:32 *** In this case, the application you are using within the desktop requires more memory than has been allocated to you. You have several options: - Relaunch the session with more memory - use Advanced Desktop, where you can list verbatim slurm options, including memory - rewrite your job as a batch script - large memory usage is an indication it should really be a long-running computation, and batch submission is best suited for this.","title":"How can I troubleshoot my OnDemand session?"},{"location":"resources/","text":"Code Examples OARC-provided slurm examples - collection of slurm batch scripts that can serve as templates for your jobs Shiny examples - excellent collection of 200 Shiny apps (interactive R webapps, or how to publish your R code) Services Rutgers free web pages - WordPress-based websites hosting, free for Rutgers community Tools markdown editor - lets you view the finished (rendered) markdown side by side with raw markdown Education OARC events - OARC holds a number of workshops on Linux, HPC, genomics and others Cornell virtual workshops - A number of courses designed to get you up to speed using HPC and programming. Software Carpentry - Software Carpentry has a number of lectures and workshops on many computing subjects HPC Carpentry - HPC carpentry - course materials much like this training guide, especially suited for a novice Linux Linux Journey - slow and gentle introduction with side-by-side exercises EdX course - free course by Linux Foundation, option to buy certificate not needed Galen's tutorial Excellent short lessons Alexei's course he runs for engineering students every semester Book: How Linux Works Book: The Linux Command Line","title":"Resources"},{"location":"resources/#code-examples","text":"OARC-provided slurm examples - collection of slurm batch scripts that can serve as templates for your jobs Shiny examples - excellent collection of 200 Shiny apps (interactive R webapps, or how to publish your R code)","title":"Code Examples"},{"location":"resources/#services","text":"Rutgers free web pages - WordPress-based websites hosting, free for Rutgers community","title":"Services"},{"location":"resources/#tools","text":"markdown editor - lets you view the finished (rendered) markdown side by side with raw markdown","title":"Tools"},{"location":"resources/#education","text":"OARC events - OARC holds a number of workshops on Linux, HPC, genomics and others Cornell virtual workshops - A number of courses designed to get you up to speed using HPC and programming. Software Carpentry - Software Carpentry has a number of lectures and workshops on many computing subjects HPC Carpentry - HPC carpentry - course materials much like this training guide, especially suited for a novice","title":"Education"},{"location":"resources/#linux","text":"Linux Journey - slow and gentle introduction with side-by-side exercises EdX course - free course by Linux Foundation, option to buy certificate not needed Galen's tutorial Excellent short lessons Alexei's course he runs for engineering students every semester Book: How Linux Works Book: The Linux Command Line","title":"Linux"},{"location":"ressentials/","text":"To start R shell on the cluster See workshop for explanation of the various options of srun module load intel/17.0.4 module load R-Project/3.4.1 ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash ##start R on compute node now R To submit R job using sbatch: #!/bin/bash #SBATCH -n 1 #SBATCH -t 1:00 #SBATCH -p main #SBATCH --export=ALL #SBATCH -o example-%j.out #SBATCH -e example-%j.err Rscript yourRprogram.r Packages used from BioConductor If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: source( https://bioconductor.org/biocLite.R ) biocLite( ape ) biocLite( MKmisc ) biocLite( Heatplus ) biocLite( affycoretools ) biocLite( flashClust ) biocLite( affy ) Example: Calculate gene length Get some data from ENSEMBLE wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz In R shell, you can execute these commands to compute gene lengths: library(GenomicFeatures) gtfdb - makeTxDbFromGFF( Homo_sapiens.GRCh38.78.gtf ,format= gtf ) exons.list.per.gene - exonsBy(gtfdb,by= gene ) exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength') Some R essentials Arithmetic functions 2+2 3*3 3*8+2 log10(1000) log2(8) abs(-10) sqrt(81) Creating objects ls() #see what objects are in the workspace x - 4 x x = 3 #a single = is an assignment operator x x == 5 #a double == asks is the left side equivalent to the right side? x + 2 #objects can be used in equations y - anyname y class(x) class(y) ls() Vector and Matrix x1 - c(1,2,3,4,5) x1 class(x1) length(x1) x - cbind(x1, x1+1) #1 will be added to all the numbers in x1 x class(x) #what kind of object is x? dim(x) #the dimension of matrix x1[1:3] #use [] to get subsets of a vector x[1,] #use [,] to get subsets of a matrix (or dataframe) x[,1] x[,-1] x[c(1,2),] x[-c(1,3),] colnames(x) colnames(x) -c( A , B ) rownames(x) -c( C , D , E , F , G ) x Data Frames z - data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F) class(z) names(z) dim(z) class(z$A) class(z$B) class(z$C) class(z$D) z$B z$C More ways to subset dataframes z$B z[[2]] z[,2] #these first 3 give equivalent results z[,1:2] z[,c(1,3)] z[c(1,3:5),] Lists mylist - list(first=z,second=x,third=c( W , X , Y , Z )) class(mylist) mylist names(mylist) class(mylist$first) class(mylist$second) Functions my.add - function(a, b) {a - b} class(my.add) my.add(4,99) my.add(99,4) my.add(b = 99, a = 4) Various directory/file/library manipulations library(limma) #load the limma package #### Make sure the working directory is set to your file on the computer; getwd() #see what the current working directory is setwd( ???????????????? ) #change the working directory #### Output a single object as a comma separated value file write.csv(z, file= test.csv ) Save all the objects you have created to your workspace save.image() #creates a default file named .RData save.image( intro.Rdata ) #creates a named file Remove objects from your workspace ls() rm(x) #remove a single object by name ls() rm(z,x1) #remove multiple objects by name ls() load( intro.Rdata ) ls() rm(list=ls()) #remove all objects ls() Save a history of all the commands entered savehistory( introhistory.Rhistory )","title":"R tutorial"},{"location":"ressentials/#to-start-r-shell-on-the-cluster","text":"See workshop for explanation of the various options of srun module load intel/17.0.4 module load R-Project/3.4.1 ## run an interactive shell for 1 hr 40 min on 1 node, 2 cores; you will be placed on a compute node this way srun -p main -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash ##start R on compute node now R","title":"To start R shell on the cluster"},{"location":"ressentials/#to-submit-r-job-using-sbatch","text":"#!/bin/bash #SBATCH -n 1 #SBATCH -t 1:00 #SBATCH -p main #SBATCH --export=ALL #SBATCH -o example-%j.out #SBATCH -e example-%j.err Rscript yourRprogram.r","title":"To submit R job using sbatch:"},{"location":"ressentials/#packages-used-from-bioconductor","text":"If these packages are not installed, you can install them yourself. On login node, start R and inside R copy-paste the following commands: source( https://bioconductor.org/biocLite.R ) biocLite( ape ) biocLite( MKmisc ) biocLite( Heatplus ) biocLite( affycoretools ) biocLite( flashClust ) biocLite( affy )","title":"Packages used from BioConductor"},{"location":"ressentials/#example-calculate-gene-length","text":"Get some data from ENSEMBLE wget ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz In R shell, you can execute these commands to compute gene lengths: library(GenomicFeatures) gtfdb - makeTxDbFromGFF( Homo_sapiens.GRCh38.78.gtf ,format= gtf ) exons.list.per.gene - exonsBy(gtfdb,by= gene ) exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength')","title":"Example: Calculate gene length"},{"location":"ressentials/#some-r-essentials","text":"","title":"Some R essentials"},{"location":"ressentials/#arithmetic-functions","text":"2+2 3*3 3*8+2 log10(1000) log2(8) abs(-10) sqrt(81)","title":"Arithmetic functions"},{"location":"ressentials/#creating-objects","text":"ls() #see what objects are in the workspace x - 4 x x = 3 #a single = is an assignment operator x x == 5 #a double == asks is the left side equivalent to the right side? x + 2 #objects can be used in equations y - anyname y class(x) class(y) ls()","title":"Creating objects"},{"location":"ressentials/#vector-and-matrix","text":"x1 - c(1,2,3,4,5) x1 class(x1) length(x1) x - cbind(x1, x1+1) #1 will be added to all the numbers in x1 x class(x) #what kind of object is x? dim(x) #the dimension of matrix x1[1:3] #use [] to get subsets of a vector x[1,] #use [,] to get subsets of a matrix (or dataframe) x[,1] x[,-1] x[c(1,2),] x[-c(1,3),] colnames(x) colnames(x) -c( A , B ) rownames(x) -c( C , D , E , F , G ) x","title":"Vector and Matrix"},{"location":"ressentials/#data-frames","text":"z - data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F) class(z) names(z) dim(z) class(z$A) class(z$B) class(z$C) class(z$D) z$B z$C","title":"Data Frames"},{"location":"ressentials/#more-ways-to-subset-dataframes","text":"z$B z[[2]] z[,2] #these first 3 give equivalent results z[,1:2] z[,c(1,3)] z[c(1,3:5),]","title":"More ways to subset dataframes"},{"location":"ressentials/#lists","text":"mylist - list(first=z,second=x,third=c( W , X , Y , Z )) class(mylist) mylist names(mylist) class(mylist$first) class(mylist$second)","title":"Lists"},{"location":"ressentials/#functions","text":"my.add - function(a, b) {a - b} class(my.add) my.add(4,99) my.add(99,4) my.add(b = 99, a = 4)","title":"Functions"},{"location":"ressentials/#various-directoryfilelibrary-manipulations","text":"library(limma) #load the limma package #### Make sure the working directory is set to your file on the computer; getwd() #see what the current working directory is setwd( ???????????????? ) #change the working directory #### Output a single object as a comma separated value file write.csv(z, file= test.csv )","title":"Various directory/file/library manipulations"},{"location":"ressentials/#save-all-the-objects-you-have-created-to-your-workspace","text":"save.image() #creates a default file named .RData save.image( intro.Rdata ) #creates a named file","title":"Save all the objects you have created to your workspace"},{"location":"ressentials/#remove-objects-from-your-workspace","text":"ls() rm(x) #remove a single object by name ls() rm(z,x1) #remove multiple objects by name ls() load( intro.Rdata ) ls() rm(list=ls()) #remove all objects ls()","title":"Remove objects from your workspace"},{"location":"ressentials/#save-a-history-of-all-the-commands-entered","text":"savehistory( introhistory.Rhistory )","title":"Save a history of all the commands entered"},{"location":"guides/Cluster_Examples/","text":"Using R Generally, there are 2 approaches for accessing R on Amarel: 1. use one of the pre-installed R modules named R-Project/ version (these versions come bundled with a very broad range of common and useful tools). 2. install your own custom build of R in your /home directory or in a shared directory (e.g. /projects/[group] or /projects/community ). Quickstart module load intel/17.0.4 R-Project/3.4.1 #loads libraries for underlying c/c++/fortran code first Using pre-installed R modules Start by finding which module you wish to use with the 'module spider R-Project' command: module spider R-Project -------------------------------------------------- R-Project: -------------------------------------------------- Description: R: The R Project for Statistical Computing Versions: R-Project/3.2.2 R-Project/3.2.5 R-Project/3.3.3 R-Project/3.4.1 -------------------------------------------------- To find detailed information about R-Project please enter the full name. For example: $ module spider R-Project/3.4.1 -------------------------------------------------- Next, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?): module spider R-Project/3.4.1 -------------------------------------------------- R-Project: R-Project/3.4.1 -------------------------------------------------- Description: R: The R Project for Statistical Computing This module can only be loaded through the following modules: intel/17.0.4 Help: This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers. Load the R-Project module of your choice: module load intel/17.0.4 R-Project/3.4.1 which R /opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R What R packages are already installed? pkgs - installed.packages () pkgs[,c( Package , Version )] Package Version base base 3.4.4 BH BH 1.66.0-1 Biobase Biobase 2.38.0 BiocGenerics BiocGenerics 0.24.0 BiocInstaller BiocInstaller 1.28.0 BiocParallel BiocParallel 1.12.0 Biostrings Biostrings 2.46.0 bitops bitops 1.0-6 boot boot 1.3-20 BSgenome BSgenome 1.46.0 ... It's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account. First, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access: mkdir ~/my.R.libs Next, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file: R_LIBS=~/my.R.libs Now, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library. Some important notes about installing packages: There are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball ( .tar.gz or .tgz file) containing the source code for that version. For example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the website . At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command: install.packages( https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz , lib= ~/my.R.libs ) The other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows: install.packages( gplots , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz , lib= ~/my.R.libs ) install.packages(c( mvtnorm , modeltools , strucchange , coin , zoo , sandwich ), lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz , lib= ~/my.R.libs ) Once all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster. Installing your own build of R For some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below. Here are the commands to use for installing R-3.4.4 from source: wget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz tar -zxf R-3.4.4.tar.gz cd R-3.4.4 module load gcc/5.4 java/1.8.0_161 ./configure --prefix=/home/gc563/R/3.4.4 --enable-java make -j 4 make install cd .. rm -rf R-3.4.4* Here, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages. When I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference. At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up. Since I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory. Before using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist): module load gcc/5.4 java/1.8.0_161 export PATH=/home/gc563/R/3.4.4/bin:$PATH export LIBRARY_PATH=/home/gc563/R/3.4.4/lib64 export LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64 export MANPATH=/home/gc563/R/3.4.4/share/man If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working: $ module list Currently Loaded Modules: 1) gcc/5.4 2) java/1.8.0_161 $ which R ~/R/3.4.4/bin/R Now that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library .libPaths() [1] /home/gc563/R/3.4.4/lib64/R/library Install a package: install.packages( rJava ) library(rJava) Saving figures/ plots from R (without a display): Need to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example: $ R --no-save png('my-figure.png', type='cairo') plot(rnorm(10),rnorm(10)) dev.off() q() Using Python Generally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community). Quickstart module load intel_mkl/17.0.2 python/3.5.2 #loads libraries for underlying c/c++/fortran code first, needed for numpy. # OR module use /projects/community/modulefiles # community software module load py-data-science-stack/5.1.0-kp807 # anaconda 5.1.0; various conda environments Using pre-installed Python modules With the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package: module load python/3.5.2 pip install --user Django Note: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly: $ which pip /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip $ pip --version pip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5) $ pip install -U --user pip Successfully installed pip-10.0.1 $ which pip /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip $ pip --version Traceback (most recent call last): File /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip , line 7, in from pip import main ImportError: cannot import name 'main' $ .local/bin/pip --version pip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5) $ .local/bin/pip install --user Django Building your own Python installation Using this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference). At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up. wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz tar -zxf Python-3.6.5.tgz cd Python-3.6.5 ./configure --prefix=/home/gc563/python/3.6.5 make -j 4 make install cd .. rm -rf Python-3.6.5* Before using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist): export PATH=/home/gc563/python/3.6.5/bin:$PATH export LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib export MANPATH=/home/gc563/python/3.6.5/share/man If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working: which python3 ~/python/3.6.5/bin Running GROMACS Here is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel. Download a PDB file. wget https://files.rcsb.org/view/5EWT.pdb Load the GROMACS software module plus any needed prerequisites. module purge module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1 Import the PDB into GROMACS, while defining the force field and water model to be used for this system. gmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein. gmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5 Now add water molecules into the empty space in the unit cell to solvate the protein. gmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro Prepare your SLURM job script(s). The 2 mdrun commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2 mdrun commands below, simply replace the example mdrun command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate grompp step. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=gmdrun # Assign an 8-character name to your job #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks=16 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Threads per process (or per core) #SBATCH --mem=124000 # Memory per node (MB) #SBATCH --time=00:20:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # combined STDOUT and STDERR output file #SBATCH --export=ALL # Export you current env to the job env module purge module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1 srun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\ -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\ -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log Perform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions: integrator = steep nsteps = 200 cutoff-scheme = Verlet coulombtype = PME pbc = xyz emtol = 100 These are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization: gmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp gmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions: integrator = md dt = 0.002 nsteps = 5000 nstlog = 50 nstenergy = 50 nstxout = 50 continuation = yes constraints = all-bonds constraint-algorithm = lincs cutoff-scheme = Verlet coulombtype = PME rcoulomb = 1.0 vdwtype = Cut-off rvdw = 1.0 DispCorr = EnerPres tcoupl = V-rescale tc-grps = Protein SOL tau-t = 0.1 0.1 ref-t = 300 300 pcoupl = Parrinello-Rahman tau-p = 2.0 compressibility = 4.5e-5 ref-p = 1.0 These are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration: gmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp gmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log Running TensorFlow with a GPU TensorFlow has two versions of its python package: tensorflow and tensorflow-gpu , but confusingly the command to use it is the same in both cases: import tensorflow as tf (and not import tensorflow-gpu as tf in case of the GPU version). This means that it really matters which package is installed in your environment. you can control your environment using Singularity image (but the problem arises if you need a package not included in the prebuilt image, in which case you need to build the image yourself) you can control your environment using conda environments (or virtual-env). Using Singularity To do this, you can use the Singularity container manager and a Docker image containing the TensorFlow software. Running Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script TF_gpu.sh ) #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --no-requeue # Do not re-run job if preempted #SBATCH --job-name=TF_gpu # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=2 # Cores per task ( 1 if multithread tasks) #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --mem=16000 # Real memory (RAM) required (MB) #SBATCH --time=00:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env module purge module load singularity/.2.5.1 srun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python Once your job script is ready, submit it using the sbatch command: $ sbatch TF_gpu.sh Alternatively, you can run Singularity interactively: $ srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu Docker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu Cache folder set to /home/user/.singularity/docker Creating container runtime... Importing: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz Importing: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz Importing: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz Importing: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz Importing: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz Importing: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz Importing: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz Importing: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz Importing: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz Importing: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz Importing: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz Importing: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz Singularity: Invoking an interactive shell within container... Singularity tensorflow:latest-gpu:~ Now, you're ready to execute commands: Singularity tensorflow:latest-gpu:~ python -V Python 2.7.12 Singularity tensorflow:latest-gpu:~ python3 -V Python 3.5.2 Please remember to exit from your interactive job after you are finished with your calculations. There are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow:1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be python3 instead of python in your script. Using conda You can either install your own version of Anaconda in your home directory, or you can use a community module. module use /projects/community/modulefiles #loads community-contributed software packages module keyword anaconda #search packages with 'anaconda' in description output: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The following modules match your search criteria: anaconda --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- py-data-science-stack: py-data-science-stack/5.1.0-kp807 Sets up anaconda 5.1.0 in your environment So here are commands with which you can load tensorflow package: module load py-data-science-stack/5.1.0-kp807 conda env list # be patient source activate tensorflow-gpu-1.7.0 # Note that if you try to import tensorflow on a node without a gpu, the import will fail, because it will try to load cuda driver that is not installed (because there is no GPU on the machine). So you need to include this line in a slurm script, where you request a GPU resource in slurm: #SBATCH --gres=gpu:1 and --partition=gpu and perhaps --constraint=pascal . Here is an example of a slurm script that trains mnist: #!/bin/bash #SBATCH --partition=gpu # Partition (job queue) #SBATCH --no-requeue # Do not re-run job if preempted #SBATCH --job-name=mnist_p100 # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --constraint=pascal # will look for that architecture #SBATCH --mem=6000 # Real memory (RAM) required (MB) #SBATCH --time=03:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.p100_gpu1.out # STDOUT output file #SBATCH --error=slurm.%N.%j.p100_gpu1.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env module purge module use /projects/community/modulefiles module load py-data-science-stack/5.1.0-kp807 source activate tensorflow-gpu-1.7.0 export PYTHONPATH=$PYTHONPATH:/home/kp807/tf/models1.7 #mnist.py needs a package defined in the tensorflow/models, not a standard distribution of TF. srun python /home/kp807/tf/benchmarking/mnist.py","title":"Examples"},{"location":"guides/Cluster_Examples/#using-r","text":"Generally, there are 2 approaches for accessing R on Amarel: 1. use one of the pre-installed R modules named R-Project/ version (these versions come bundled with a very broad range of common and useful tools). 2. install your own custom build of R in your /home directory or in a shared directory (e.g. /projects/[group] or /projects/community ).","title":"Using R"},{"location":"guides/Cluster_Examples/#quickstart","text":"module load intel/17.0.4 R-Project/3.4.1 #loads libraries for underlying c/c++/fortran code first","title":"Quickstart"},{"location":"guides/Cluster_Examples/#using-pre-installed-r-modules","text":"Start by finding which module you wish to use with the 'module spider R-Project' command: module spider R-Project -------------------------------------------------- R-Project: -------------------------------------------------- Description: R: The R Project for Statistical Computing Versions: R-Project/3.2.2 R-Project/3.2.5 R-Project/3.3.3 R-Project/3.4.1 -------------------------------------------------- To find detailed information about R-Project please enter the full name. For example: $ module spider R-Project/3.4.1 -------------------------------------------------- Next, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?): module spider R-Project/3.4.1 -------------------------------------------------- R-Project: R-Project/3.4.1 -------------------------------------------------- Description: R: The R Project for Statistical Computing This module can only be loaded through the following modules: intel/17.0.4 Help: This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers. Load the R-Project module of your choice: module load intel/17.0.4 R-Project/3.4.1 which R /opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R What R packages are already installed? pkgs - installed.packages () pkgs[,c( Package , Version )] Package Version base base 3.4.4 BH BH 1.66.0-1 Biobase Biobase 2.38.0 BiocGenerics BiocGenerics 0.24.0 BiocInstaller BiocInstaller 1.28.0 BiocParallel BiocParallel 1.12.0 Biostrings Biostrings 2.46.0 bitops bitops 1.0-6 boot boot 1.3-20 BSgenome BSgenome 1.46.0 ... It's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account. First, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access: mkdir ~/my.R.libs Next, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading \".\" in that name) and I'll add the following line to that file: R_LIBS=~/my.R.libs Now, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library. Some important notes about installing packages: There are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball ( .tar.gz or .tgz file) containing the source code for that version. For example, I want to load the following list of packages, and I need the specifc versions listed here: 'kernlab' version 0.9-24 'ROCR' version 1.0.7 'class' version 7.3.14 'party' version 1.0.25 'e1071' version 1.6.7 'randomForest' version 4.6.12 I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for \"kernlab\" and find the website . At that site, I see that 0.9-25 is the current version (not what I want), but there is \"kernlab archive\" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command: install.packages( https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz , lib= ~/my.R.libs ) The other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows: install.packages( gplots , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz , lib= ~/my.R.libs ) install.packages(c( mvtnorm , modeltools , strucchange , coin , zoo , sandwich ), lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz , lib= ~/my.R.libs ) install.packages( https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz , lib= ~/my.R.libs ) Once all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.","title":"Using pre-installed R modules"},{"location":"guides/Cluster_Examples/#installing-your-own-build-of-r","text":"For some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below. Here are the commands to use for installing R-3.4.4 from source: wget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz tar -zxf R-3.4.4.tar.gz cd R-3.4.4 module load gcc/5.4 java/1.8.0_161 ./configure --prefix=/home/gc563/R/3.4.4 --enable-java make -j 4 make install cd .. rm -rf R-3.4.4* Here, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages. When I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference. At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up. Since I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory. Before using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist): module load gcc/5.4 java/1.8.0_161 export PATH=/home/gc563/R/3.4.4/bin:$PATH export LIBRARY_PATH=/home/gc563/R/3.4.4/lib64 export LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64 export MANPATH=/home/gc563/R/3.4.4/share/man If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working: $ module list Currently Loaded Modules: 1) gcc/5.4 2) java/1.8.0_161 $ which R ~/R/3.4.4/bin/R Now that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library .libPaths() [1] /home/gc563/R/3.4.4/lib64/R/library Install a package: install.packages( rJava ) library(rJava) Saving figures/ plots from R (without a display): Need to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example: $ R --no-save png('my-figure.png', type='cairo') plot(rnorm(10),rnorm(10)) dev.off() q()","title":"Installing your own build of R"},{"location":"guides/Cluster_Examples/#using-python","text":"Generally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).","title":"Using Python"},{"location":"guides/Cluster_Examples/#quickstart_1","text":"module load intel_mkl/17.0.2 python/3.5.2 #loads libraries for underlying c/c++/fortran code first, needed for numpy. # OR module use /projects/community/modulefiles # community software module load py-data-science-stack/5.1.0-kp807 # anaconda 5.1.0; various conda environments","title":"Quickstart"},{"location":"guides/Cluster_Examples/#using-pre-installed-python-modules","text":"With the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package: module load python/3.5.2 pip install --user Django Note: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly: $ which pip /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip $ pip --version pip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5) $ pip install -U --user pip Successfully installed pip-10.0.1 $ which pip /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip $ pip --version Traceback (most recent call last): File /opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip , line 7, in from pip import main ImportError: cannot import name 'main' $ .local/bin/pip --version pip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5) $ .local/bin/pip install --user Django","title":"Using pre-installed Python modules"},{"location":"guides/Cluster_Examples/#building-your-own-python-installation","text":"Using this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference). At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up. wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz tar -zxf Python-3.6.5.tgz cd Python-3.6.5 ./configure --prefix=/home/gc563/python/3.6.5 make -j 4 make install cd .. rm -rf Python-3.6.5* Before using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist): export PATH=/home/gc563/python/3.6.5/bin:$PATH export LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib export MANPATH=/home/gc563/python/3.6.5/share/man If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working: which python3 ~/python/3.6.5/bin","title":"Building your own Python installation"},{"location":"guides/Cluster_Examples/#running-gromacs","text":"Here is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we\u2019ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel. Download a PDB file. wget https://files.rcsb.org/view/5EWT.pdb Load the GROMACS software module plus any needed prerequisites. module purge module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1 Import the PDB into GROMACS, while defining the force field and water model to be used for this system. gmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein. gmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5 Now add water molecules into the empty space in the unit cell to solvate the protein. gmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro Prepare your SLURM job script(s). The 2 mdrun commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your mdrun commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here\u2019s an example job script for a GROMACS MD simulation. To run the 2 mdrun commands below, simply replace the example mdrun command in this script with one of the mdrun commands from the steps below and submit that job after preparing the simulation with the appropriate grompp step. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=gmdrun # Assign an 8-character name to your job #SBATCH --nodes=1 # Number of nodes #SBATCH --ntasks=16 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Threads per process (or per core) #SBATCH --mem=124000 # Memory per node (MB) #SBATCH --time=00:20:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # combined STDOUT and STDERR output file #SBATCH --export=ALL # Export you current env to the job env module purge module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1 srun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \\ -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \\ -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log Perform an inital, quick energy minimization. Here, we\u2019re using a customized MD parameters file named em.mdp, which contains these instructions: integrator = steep nsteps = 200 cutoff-scheme = Verlet coulombtype = PME pbc = xyz emtol = 100 These are the commands (both the grompp step and the mdrun step) used to prepare and run the minimization: gmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp gmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we\u2019re using a customized MD parameters file named equil.mdp, which contains these instructions: integrator = md dt = 0.002 nsteps = 5000 nstlog = 50 nstenergy = 50 nstxout = 50 continuation = yes constraints = all-bonds constraint-algorithm = lincs cutoff-scheme = Verlet coulombtype = PME rcoulomb = 1.0 vdwtype = Cut-off rvdw = 1.0 DispCorr = EnerPres tcoupl = V-rescale tc-grps = Protein SOL tau-t = 0.1 0.1 ref-t = 300 300 pcoupl = Parrinello-Rahman tau-p = 2.0 compressibility = 4.5e-5 ref-p = 1.0 These are the commands (both the grompp step and the mdrun step) used to prepare and run the equilibration: gmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp gmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log","title":"Running GROMACS"},{"location":"guides/Cluster_Examples/#running-tensorflow-with-a-gpu","text":"TensorFlow has two versions of its python package: tensorflow and tensorflow-gpu , but confusingly the command to use it is the same in both cases: import tensorflow as tf (and not import tensorflow-gpu as tf in case of the GPU version). This means that it really matters which package is installed in your environment. you can control your environment using Singularity image (but the problem arises if you need a package not included in the prebuilt image, in which case you need to build the image yourself) you can control your environment using conda environments (or virtual-env).","title":"Running TensorFlow with a GPU"},{"location":"guides/Cluster_Examples/#using-singularity","text":"To do this, you can use the Singularity container manager and a Docker image containing the TensorFlow software. Running Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script TF_gpu.sh ) #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --no-requeue # Do not re-run job if preempted #SBATCH --job-name=TF_gpu # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=2 # Cores per task ( 1 if multithread tasks) #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --mem=16000 # Real memory (RAM) required (MB) #SBATCH --time=00:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env module purge module load singularity/.2.5.1 srun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python Once your job script is ready, submit it using the sbatch command: $ sbatch TF_gpu.sh Alternatively, you can run Singularity interactively: $ srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu Docker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu Cache folder set to /home/user/.singularity/docker Creating container runtime... Importing: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz Importing: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz Importing: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz Importing: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz Importing: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz Importing: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz Importing: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz Importing: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz Importing: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz Importing: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz Importing: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz Importing: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz Singularity: Invoking an interactive shell within container... Singularity tensorflow:latest-gpu:~ Now, you're ready to execute commands: Singularity tensorflow:latest-gpu:~ python -V Python 2.7.12 Singularity tensorflow:latest-gpu:~ python3 -V Python 3.5.2 Please remember to exit from your interactive job after you are finished with your calculations. There are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow:1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be python3 instead of python in your script.","title":"Using Singularity"},{"location":"guides/Cluster_Examples/#using-conda","text":"You can either install your own version of Anaconda in your home directory, or you can use a community module. module use /projects/community/modulefiles #loads community-contributed software packages module keyword anaconda #search packages with 'anaconda' in description output: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The following modules match your search criteria: anaconda --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- py-data-science-stack: py-data-science-stack/5.1.0-kp807 Sets up anaconda 5.1.0 in your environment So here are commands with which you can load tensorflow package: module load py-data-science-stack/5.1.0-kp807 conda env list # be patient source activate tensorflow-gpu-1.7.0 # Note that if you try to import tensorflow on a node without a gpu, the import will fail, because it will try to load cuda driver that is not installed (because there is no GPU on the machine). So you need to include this line in a slurm script, where you request a GPU resource in slurm: #SBATCH --gres=gpu:1 and --partition=gpu and perhaps --constraint=pascal . Here is an example of a slurm script that trains mnist: #!/bin/bash #SBATCH --partition=gpu # Partition (job queue) #SBATCH --no-requeue # Do not re-run job if preempted #SBATCH --job-name=mnist_p100 # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --gres=gpu:1 # Number of GPUs #SBATCH --constraint=pascal # will look for that architecture #SBATCH --mem=6000 # Real memory (RAM) required (MB) #SBATCH --time=03:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.p100_gpu1.out # STDOUT output file #SBATCH --error=slurm.%N.%j.p100_gpu1.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env module purge module use /projects/community/modulefiles module load py-data-science-stack/5.1.0-kp807 source activate tensorflow-gpu-1.7.0 export PYTHONPATH=$PYTHONPATH:/home/kp807/tf/models1.7 #mnist.py needs a package defined in the tensorflow/models, not a standard distribution of TF. srun python /home/kp807/tf/benchmarking/mnist.py","title":"Using conda"},{"location":"guides/Cluster_User_Guide/","text":"General Information There are several clusters administered by OARC. Each cluster is using the resource scheduler Slurm to allocate users the requested resources. Amarel - a general access cluster. Request access or become an owner Perceval - similar to Amarel, but paid for, and to be used by exclusively by, NIH grants and grantees AmarelN - the Amarel system component located in Newark Sirius - a single big machine with no resource scheduler, acts as a big desktop with multiple concurrent users. This is being retired Feb 25, 2019 and no new accounts will be create on this machine. Didact - experimental - a small cluster dedicated to teaching classes. It was stood up in Jan 2019 and is in experimental stage (only friendly users allowed). Cluster Resources Amarel Amarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource. Below is a sample of the hardware found in the Amarel system (this list may already be outdated since the cluster is actively growing): 68 CPU-only nodes, each with 32 Xeon 6130 (Skylake) cores + 192 GB RAM 52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM 20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM 4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard 2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM 53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM 5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM 26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM 4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM 3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard 2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard 1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard Default run time = 2 minutes in the 'main' partition Maximum run time = 3 days in the 'main' partition Perceval Perceval has the same setup as Amarel. The same file system is mounted on both clusters and changes made to the filesystem on one cluster will be reflected in the other cluster as well. Perceval includes the following hardware (this list may already be outdated since the cluster is actively growing): 132 CPU-only nodes, each with 24 Intel Xeon E5-2680 cores + 128 GB RAM 8 GPU nodes with 24 Intel Xeon E5-2680 cores + 128 GB RAM 1 Large Memory node with 48 Intel Xeon E5-2680 cores + 1.5 TB RAM Default run time = 2 hours for all partitions Maximum run time = 7 days in the 'main' and 'perceval' partitions, 2 days in the 'gpu' partition Preemption: jobs running in the 'main' job partition are subject to preemption but jobs running in the 'perceval' partition are not subject to preemption. Basic operations - connecting and moving files Connecting to the cluster Amarel is currently accessed using a single hostname, amarel.rutgers.edu Perceval is currently accessed using a single hostname, perceval.rutgers.edu AmarelN is currently accessed using a single hostname, amareln.hpc.rutgers.edu For the example purposes, we will assume you are connecting to Amarel. When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc. So, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using. ssh [your NetID]@amarel.rutgers.edu If you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See here for details. Moving files There are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory, scp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID] To pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command), scp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt . If you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it: tar -czf my-directory.tar.gz my-directory After moving it, you can unpack that .tar.gz file to get your original directory and contents: tar -xzf my-directory.tar.gz A handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel: rsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir In this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data) To sync a local directory with updated data from Amarel: rsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir Here, we've simply reversed the order of the local and remote locations. For added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case): rsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir OnDemand - GUI for the cluster For users not familiar with Linux, there is an option to connect to the cluster via a web browser (either from campus or through VPN) until you get more confortable with Linux. Both connecting and moving files can be achieved through this interface . However, you are strongly encouraged to get comfortable with Linux, as your productivity will soar and a GUI is never as flexible as a command line interface. Functionalities of OnDemand: file upload file editor linux shell launch Jupyter notebook launch RStudio compose slurm job from a template submit slurm job view job queue Listing available resources Before requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources. Example of using the sinfo command: sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST main* up 3-00:00:00 4 drain* hal[0050-0051,0055,0093] main* up 3-00:00:00 5 down* slepner[084-088] main* up 3-00:00:00 4 drain hal[0023,0025-0027] main* up 3-00:00:00 86 mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082] main* up 3-00:00:00 84 alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083] main* up 3-00:00:00 2 down gpu[001-002] gpu up 3-00:00:00 8 mix gpu[003,006],pascal[001-006] gpu up 3-00:00:00 1 alloc gpu005 gpu up 3-00:00:00 2 down gpu[001-002] phi up 3-00:00:00 1 mix gpu004 mem up 3-00:00:00 1 mix mem002 mem up 3-00:00:00 1 alloc mem001 Understanding this output: There are 4 basic partitions, main (traditional compute nodes, CPUs only, jobs running here are preemptible), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors. Note: Perceval users will see an additional 'perceval' partition available and this partition is not subject to preemption. The upper limit for a job\u2019s run time is 3 days (72 hours). Term Meaning Allocated (alloc) nodes that are currently running jobs. Mixed (mix) nodes have jobs using some, but not all, CPU cores onboard. Idle nodes are currently available for new jobs. Drained (drain, drng) nodes are not available for use and may be offline for maintenance. Slepner, Norse Mythology \"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores). Hal Hal is a dependable member of the Discovery One crew who does an excellent job of following instructions. Pascal French mathematician and the name of one of NVIDIA's GPU architectures. CUDA This is the name of a parallel computing platform and application programming interface (API) model created by Nvidia Loading software modules When you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system. Lmod video walkthrough Commands used in the video: lmod = https://lmod.readthedocs.io = A New Environment Module System Solves the problem of setting environment variables so you can run different software easily Main commands: module avail # which modules are available module spider # find information about a software module load # load a particular software module use /projects/community/modulefiles # user contributed software, unsupported module purge # unload all modules module list # list currently loaded modules gotcha: will not show modules until you load up their dependencies - e.g. R-Project module avail # available system-installed modules which R # no results module spider R # too many results module spider R- # keyword search for specific module module load intel/17.0.4 # after load, we will see R-Project in the list of modules module avail module load R-Project/3.4.1# loading R module list which R # now R is in my path srun -N 1 -n 1 -c 1 -t 10:00 R --no-save module use /projects/community/modulefiles # user contributed software, unsupported module avail # which modules are available module load spark/2.3.0-kp807 # load a particular software which spark-shell # shows where the software is located Longer explanation The module avail command will show a list of the core (primary) modules available: module avail -------------------------------------------------- /opt/sw/modulefiles/Core -------------------------------------------------- ARACNE/20110228 blat/35 gcc/4.9.3 intel_mkl/16.0.3 (D) mvapich2/2.2 (D) HISAT2/2.0.4 bowtie2/2.2.6 gcc/4.9.4 intel_mkl/17.0.0 openmpi/2.1.1 HISAT2/2.1.0 (D) bowtie2/2.2.9 (D) gcc/5.3 intel_mkl/17.0.1 pgi/16.9 MATLAB/R2017a bwa/0.7.12 gcc/5.4 (D) intel_mkl/17.0.2 pgi/16.10 (D) MATLAB/R2017b (D) bwa/0.7.13 (D) hdf5/1.8.16 java/1.7.0_79 python/2.7.11 Mathematica/11.1 cuda/7.5 intel/16.0.1 java/1.8.0_66 python/2.7.12 OpenCV/2.3.1 cuda/8.0 intel/16.0.3 (D) java/1.8.0_73 python/3.5.0 STAR/2.5.2a cuda/9.0 (D) intel/16.0.4 java/1.8.0_121 python/3.5.2 (D) Trinotate/2.0.2 cudnn/7.0.3 intel/17.0.0 java/1.8.0_141 samtools/0.1.19 bamtools/2.4.0 cufflinks/2.2.1 intel/17.0.1 java/1.8.0_152 (D) samtools/1.2 bcftools/1.2 delly/0.7.6 intel/17.0.2 modeller/9.16 samtools/1.3.1 (D) bedtools2/2.25.0 gaussian/03revE01 intel/17.0.4 moe/2016.0802 trinityrnaseq/2.1.1 blast/2.6.0 gaussian/09revD01 (D) intel_mkl/16.0.1 mvapich2/2.1 Understanding this output: The packages with a (D) are the default versions for packages where multiple versions are available. To see a comprehensive list of all available modules (not just the core modules) use the module spider command, or module keyword command (see CheatSheat section). To be able to use community-contributed software, execute module use /projects/community/modulefiles before using the the above mentioned commands (see Community section). module spider --------------------------------------------------------------------------------------------------------------- The following is a list of the modules currently available: --------------------------------------------------------------------------------------------------------------- ARACNE: ARACNE/20110228 ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context HISAT2: HISAT2/2.0.4, HISAT2/2.1.0 HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes HMMER: HMMER/3.1b2 HMMER: biosequence analysis using profile hidden Markov models MATLAB: MATLAB/R2017a, MATLAB/R2017b MATLAB: The Language of Technical Computing Mathematica: Mathematica/11.1 Wolfram Mathematica: Modern Technical Computing NAMD: NAMD/2.10 NAMD: Scalable Molecular Dynamics ORCA: ORCA/3.0.3 ORCA: An ab initio, DFT and semiempirical SCF-MO package OpenCV: OpenCV/2.3.1 OpenCV: Open Source Computer Vision Loading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use. command explanation usage module load load a software module module load intel/16.0.3 module unload remove select modules module unload intel/16.0.3 module purge load the default version of any software package module load intel Below are some examples. module load intel/16.0.3 mvapich2/2.1 module list Currently Loaded Modules: 1) intel/16.0.3 2) mvapich2/2.1 module unload mvapich2/2.1 module list Currently Loaded Modules: 1) intel/16.0.3 module purge module list No modules loaded module load intel module list Currently Loaded Modules: 1) intel/16.0.3 If you always use the same software modules, your ~/.bashrc (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your ~/.bashrc file to change or remove those commands later. PLEASE NOTE: Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory. Running slurm jobs Video walkthrough Demoing sinfo, srun, squeue, scancel commands: Running a serial (single-core) job Here\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --requeue # Return job to the queue if preempted #SBATCH --job-name=zipx001a # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=2000 # Real memory (RAM) required (MB) #SBATCH --time=02:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env cd /scratch/[your NetID] module purge module load intel/16.0.3 fftw/3.3.1 srun /scratch/[your NetID]/zipper/2.4.1/bin/zipper my-input-file.in Understanding this job script: A job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job. The lines beginning with #SBATCH contain commands intended only for the workload manager. My job will be assigned to the \u201cmain\u201d partition (job queue). If this job is preempted, it will be returned to the job queue and will start again when required resources are available This job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that. My job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running. Any output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number. Be sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable. Here\u2019s how to run a serial batch job, loading modules and using the sbatch command: sbatch my-job-script.sh The sbatch command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running. Running a parallel (multicore MPI) job Here\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --requeue # Return job to the queue if preempted #SBATCH --job-name=zipx001a # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=16 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=124000 # Real memory (RAM) required (MB) #SBATCH --time=02:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env cd /scratch/[your NetID] module purge module load intel/16.0.3 fftw/3.3.1 mvapich2/2.1 srun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper my-input-file.in Understanding this job script: The srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request. This job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that. Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable. Here\u2019s how to run a parallel batch job, loading modules and using the sbatch command: sbatch my-job-script.sh Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable. Here\u2019s how to run a parallel batch job, loading modules and using the sbatch command: sbatch my-job-script.sh Running array of jobs Array job is an approach to handle multiple jobs with single job script. Here is an example to submit 500 jobs with single job script. #!/bin/bash #SBATCH --partition=main # Name of the partition #SBATCH --job-name=arrayjobs # Name of the job #SBATCH --ntasks=1 # Number of tasks #SBATCH --cpus-per-task=1 # Number of CPUs per task #SBATCH --mem=1GB # Requested memory #SBATCH --array=0-499 # Array job will submit 500 jobs #SBATCH --time=00:10:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT file #SBATCH --error=slurm.%N.%j.err # STDERR file echo -n Executing on the machine: hostname echo Array Task ID : $SLURM_ARRAY_TASK_ID echo Random number : $RANDOM In the above description, the line #SBATCH --array=0-499 submits 500 jobs. The \"%\" seperator is useful to limit the number of jobs in the queue at a any given time. For example, the following line would send a maximum of 100 jobs to the queue. #SBATCH --array=0-499%100 It is a good practice to populate the queue with less than 500 jobs at any given time. Running an interactive job An interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications. When submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job: [NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i srun: job 1365471 queued and waiting for resources srun: job 1365471 has been allocated resources [NetID@slepner045 ~]$ Notice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command. Monitoring the status of jobs _ The simplest way to quickly check on the status of active jobs is by using the squeue command: squeue -u [your NetID] JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1633383 main zipper xx345 R 1:15 1 slepner36 Here, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS). For summary accounting information (including jobs that have already completed), you can use the sacct command: sacct JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1633383 zipper main statx 16 RUNNING 0:0 Here, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED. For complete and detailed job info, you can use the scontrol show job [JobID] command: scontrol show job 244348 JobId=244348 JobName=XIoT22 UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A Priority=5050 Nice=0 Account=oarc QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19 StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=main AllocNode:Sid=amarel1:22391 ReqNodeList=(null) ExcNodeList=(null) NodeList=hal0053 BatchHost=hal0053 NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:* TRES=cpu=28,mem=124000M,node=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/gc563/run.STMV.CPU.slurm WorkDir=/scratch/gc563 StdErr=/scratch/gc563/slurm.%N.244348.out StdIn=/dev/null StdOut=/scratch/gc563/slurm.%N.244348.out Power= If your jobs have already completed (or have been terminated), you can see details about those jobs using the sacct command with your NetID and a start time for the list of jobs this command will produce. sacct --user=[NetID] --starttime=2018-07-03 --format=JobID,Partition,JobName,MaxRSS,NodeList,Elapsed,MaxDiskRead,MaxDiskWrite,State Killing/ cancelling/ terminating jobs To terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate: scancel 1633383 A job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this: slurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 *** Installing your own software Package management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]). As an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist: Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the wget utility: wget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files: tar -zxf zipper-4.1.5.tar.gz Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software. cd zipper-4.1.5 less README Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER: module load intel/16.0.3 mvapich2/2.1 Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the ./configure -\u2013help command. What you see below is just a typical example of special options that might be specified. ./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared make -j 4 make install Several packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation: - /home/[NetID]/zipper/4.1.5/bin where executables will be placed - /home/[NetID]/zipper/4.1.5/lib where library files will be placed - /home/[NetID]/zipper/4.1.5/include where header files will be placed - /home/[NetID]/zipper/4.1.5/share/man where documentation will be placed Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment: export PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH export C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH export MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH These export commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel. Singularity Singularity is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers. Docker is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine. Important differences between Docker and Singularity: Docker and Singularity have their own container formats. Docker containers can be imported and run using Singularity. Docker containers usually run as root, which means you cannot run Docker on a shared computing system (cluster). Singularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account. Importing a Docker image: If you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container. Here's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel. If you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories. Start your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel. $ sudo docker run -it ubuntu:latest bash root@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc Exporting your Docker image Find the name of your Docker image using the 'docker ps' command, $ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 11a87dkw8748 ubuntu:latest bash 2 minutes ago Up 2 minutes bendakaya_pakodi In this example the name of the images is bendakaya_pakodi. Export this image to a tarball, $ sudo docker export bendakaya_pakodi ubuntu.tar Converting to a Singularity image You will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel. Create an empty singularity image, and then import the exported docker image into it, $ sudo singularity create ubuntu.img Creating a sparse image with a maximum size of 1024MiB... Using given image size of 1024 Formatting image (/sbin/mkfs.ext3) Done. Image can be found at: ubuntu.img $ sudo singularity import ubuntu.img ubuntu.tar Using Singularity containers inside a SLURM job Transfer your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel. You can run any task/program inside the container by prefacing it with singularity exec [your image name] Here is a simple example job script that executes commands inside a container, #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=sing2me # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=4000 # Real memory (RAM) required (MB) #SBATCH --time=00:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file module purge module load singularity/.2.5.1 ## Where am I running? srun singularity exec ubuntu.img hostname ## What is the current time and date? srun singularity exec ubuntu.img date If you created directories for any Amarel filesystems, you should find they are mounted inside your container, mount | grep gpfs /dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime) /dev/projects/oarc on /projects/oarc type gpfs (rw,relatime) NOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution. Guidelines for managing data on scratch Table: Data management on OARC cluster Fileset Users quota Time Back up External storage options Transfer Tools scratch general 20 TB 90 days no - Personal devices: Laptop, Desktop, USB drives, etc. - Online drives: Box(unlimited for Rutgers), Google Drive, OneDrive, Dropbox, etc. - Public cloud storages: GCP, Azure, AWS, etc. -CLI tools: scp, rsync, rcloud - GUI tools: Filezilla, WinSCP, Open OnDemand, rclone-browser (via sirius3) home general 100GB Valid account yes project node owners varies varies yes Our /scratch fileset offers unlimited temporary storage. You can process large volumes of data in /scratch, but the data cannot be older than 90 days. Files older than 90 days are automatically removed, and there is no backup. So, it is a best practice to move the data from scratch as soon as possible. Internal and external storage options For long-term storage, you can utilize the internal storage at OARC's cluster or any external storage resource. OARC's internal storage includes /home and /projects file sets. If your data can be accommodated in your /home directory, it is usually best to keep important items there. Node owners may want to use their /projects storage space. If you don't have enough space within one of OARC's clusters, you should move the data to an external storage resource. Here, we point out a few options to consider for external storage. Personal devices such as laptop, desktop, workstations, or USB drives can serve as external storage. Third party vendors offer several storage options that are either free or paid services. Box offers unlimited free storage for the Rutgers community. Google offers unlimited free storage for the academic community by a service called Google Drive File Stream. One Drive and DropBox services are free to use up to a set quota. Public cloud providers such as AWS, Azure, and GCP charge a fee to move the data in and out of the cloud. Data transfer tools There are several tools and methods available to move the data that are based on command line interface (CLI) or Graphical User Interface (GUI). You can use command line tools such as scp, rsync, or rcloud to move the data from OARC cluster to any external storage. scp and rsync are suitable choices if you are moving data from OARC to your laptop or desktop that runs linux or MacOS. rcloud is an excellent choice to move your data to many storage services such as Box, DropBox, cloud, etc. GUI tools such as Filezilla, WinSCP, rclone-browser, and Open OnDemand are popular choices to move the data from variety of storage end points. Among GUI tools, FileZilla, rclone-browser, and Open OnDemand tools should work on any operating systems (Linux, MacOS, Windows) since they interact through a web-browser. Please refer our user guide for more details about how to use specific CLI tools such as scp, rsync, and rclone. Here is a simple command line example to move your data using scp. scp your-netid@amarel.hpc.rutgers.edu: file location on oarc cluster file location on your device where \u2018your-netid\u2019 is your Rutger\u2019s netid, which is same as your account name on OARC cluster. For example, to move data from scratch space located at /scratch/your-netid/your-data-file to a home directory on your laptop using scp command, type the following in your terminal shell prompt on your laptop. scp your-netid@amarel.hpc.rutgers.edu:/scratch/your-net-id/your-data-file ~/your-data-file Troubleshooting/ Common Problems Failure to load module dependencies/prerequisites: module load R-Project/3.4.1 Lmod has detected the following error: These module(s) exist but cannot be loaded as requested: R-Project/3.4.1 Try: module spider R-Project/3.4.1 to see how to load the module(s). This software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load: module spider R-Project/3.4.1 This module can only be loaded through the following modules: intel/17.0.4 Help: This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers. Ah-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1 Acknowledging Amarel Please reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include: Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey A suggested acknowledgement is: The authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu","title":"Cluster Guides"},{"location":"guides/Cluster_User_Guide/#general-information","text":"There are several clusters administered by OARC. Each cluster is using the resource scheduler Slurm to allocate users the requested resources. Amarel - a general access cluster. Request access or become an owner Perceval - similar to Amarel, but paid for, and to be used by exclusively by, NIH grants and grantees AmarelN - the Amarel system component located in Newark Sirius - a single big machine with no resource scheduler, acts as a big desktop with multiple concurrent users. This is being retired Feb 25, 2019 and no new accounts will be create on this machine. Didact - experimental - a small cluster dedicated to teaching classes. It was stood up in Jan 2019 and is in experimental stage (only friendly users allowed).","title":"General Information"},{"location":"guides/Cluster_User_Guide/#cluster-resources","text":"","title":"Cluster Resources"},{"location":"guides/Cluster_User_Guide/#amarel","text":"Amarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource. Below is a sample of the hardware found in the Amarel system (this list may already be outdated since the cluster is actively growing): 68 CPU-only nodes, each with 32 Xeon 6130 (Skylake) cores + 192 GB RAM 52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM 20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM 4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard 2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM 53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM 5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM 26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM 4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM 3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard 2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard 1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard Default run time = 2 minutes in the 'main' partition Maximum run time = 3 days in the 'main' partition","title":"Amarel"},{"location":"guides/Cluster_User_Guide/#perceval","text":"Perceval has the same setup as Amarel. The same file system is mounted on both clusters and changes made to the filesystem on one cluster will be reflected in the other cluster as well. Perceval includes the following hardware (this list may already be outdated since the cluster is actively growing): 132 CPU-only nodes, each with 24 Intel Xeon E5-2680 cores + 128 GB RAM 8 GPU nodes with 24 Intel Xeon E5-2680 cores + 128 GB RAM 1 Large Memory node with 48 Intel Xeon E5-2680 cores + 1.5 TB RAM Default run time = 2 hours for all partitions Maximum run time = 7 days in the 'main' and 'perceval' partitions, 2 days in the 'gpu' partition Preemption: jobs running in the 'main' job partition are subject to preemption but jobs running in the 'perceval' partition are not subject to preemption.","title":"Perceval"},{"location":"guides/Cluster_User_Guide/#basic-operations-connecting-and-moving-files","text":"","title":"Basic operations - connecting and moving files"},{"location":"guides/Cluster_User_Guide/#connecting-to-the-cluster","text":"Amarel is currently accessed using a single hostname, amarel.rutgers.edu Perceval is currently accessed using a single hostname, perceval.rutgers.edu AmarelN is currently accessed using a single hostname, amareln.hpc.rutgers.edu For the example purposes, we will assume you are connecting to Amarel. When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc. So, while you are logged-in to Amarel, you will see \"amarel1\" or \"amarel2\" as the name of the machine you are using. ssh [your NetID]@amarel.rutgers.edu If you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See here for details.","title":"Connecting to the cluster"},{"location":"guides/Cluster_User_Guide/#moving-files","text":"There are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let\u2019s assume you\u2019re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory, scp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID] To pull a file from your Amarel /home directory to your laptop (note the \u201c.\u201d at the end of this command), scp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt . If you want to copy an entire directory and its contents using scp, you\u2019ll need to \u201cpackage\u201d your directory into a single, compressed file before moving it: tar -czf my-directory.tar.gz my-directory After moving it, you can unpack that .tar.gz file to get your original directory and contents: tar -xzf my-directory.tar.gz A handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel: rsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir In this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data) To sync a local directory with updated data from Amarel: rsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir Here, we've simply reversed the order of the local and remote locations. For added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case): rsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir","title":"Moving files"},{"location":"guides/Cluster_User_Guide/#ondemand-gui-for-the-cluster","text":"For users not familiar with Linux, there is an option to connect to the cluster via a web browser (either from campus or through VPN) until you get more confortable with Linux. Both connecting and moving files can be achieved through this interface . However, you are strongly encouraged to get comfortable with Linux, as your productivity will soar and a GUI is never as flexible as a command line interface. Functionalities of OnDemand: file upload file editor linux shell launch Jupyter notebook launch RStudio compose slurm job from a template submit slurm job view job queue","title":"OnDemand - GUI for the cluster"},{"location":"guides/Cluster_User_Guide/#listing-available-resources","text":"Before requesting resources (compute nodes), it\u2019s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources. Example of using the sinfo command: sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST main* up 3-00:00:00 4 drain* hal[0050-0051,0055,0093] main* up 3-00:00:00 5 down* slepner[084-088] main* up 3-00:00:00 4 drain hal[0023,0025-0027] main* up 3-00:00:00 86 mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082] main* up 3-00:00:00 84 alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083] main* up 3-00:00:00 2 down gpu[001-002] gpu up 3-00:00:00 8 mix gpu[003,006],pascal[001-006] gpu up 3-00:00:00 1 alloc gpu005 gpu up 3-00:00:00 2 down gpu[001-002] phi up 3-00:00:00 1 mix gpu004 mem up 3-00:00:00 1 mix mem002 mem up 3-00:00:00 1 alloc mem001 Understanding this output: There are 4 basic partitions, main (traditional compute nodes, CPUs only, jobs running here are preemptible), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors. Note: Perceval users will see an additional 'perceval' partition available and this partition is not subject to preemption. The upper limit for a job\u2019s run time is 3 days (72 hours). Term Meaning Allocated (alloc) nodes that are currently running jobs. Mixed (mix) nodes have jobs using some, but not all, CPU cores onboard. Idle nodes are currently available for new jobs. Drained (drain, drng) nodes are not available for use and may be offline for maintenance. Slepner, Norse Mythology \"Sleipnir\" 8-legged war horse (this made more sense when CPUs had 8 cores). Hal Hal is a dependable member of the Discovery One crew who does an excellent job of following instructions. Pascal French mathematician and the name of one of NVIDIA's GPU architectures. CUDA This is the name of a parallel computing platform and application programming interface (API) model created by Nvidia","title":"Listing available resources"},{"location":"guides/Cluster_User_Guide/#loading-software-modules","text":"When you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.","title":"Loading software modules"},{"location":"guides/Cluster_User_Guide/#lmod-video-walkthrough","text":"Commands used in the video: lmod = https://lmod.readthedocs.io = A New Environment Module System Solves the problem of setting environment variables so you can run different software easily Main commands: module avail # which modules are available module spider # find information about a software module load # load a particular software module use /projects/community/modulefiles # user contributed software, unsupported module purge # unload all modules module list # list currently loaded modules gotcha: will not show modules until you load up their dependencies - e.g. R-Project module avail # available system-installed modules which R # no results module spider R # too many results module spider R- # keyword search for specific module module load intel/17.0.4 # after load, we will see R-Project in the list of modules module avail module load R-Project/3.4.1# loading R module list which R # now R is in my path srun -N 1 -n 1 -c 1 -t 10:00 R --no-save module use /projects/community/modulefiles # user contributed software, unsupported module avail # which modules are available module load spark/2.3.0-kp807 # load a particular software which spark-shell # shows where the software is located","title":"Lmod video walkthrough"},{"location":"guides/Cluster_User_Guide/#longer-explanation","text":"The module avail command will show a list of the core (primary) modules available: module avail -------------------------------------------------- /opt/sw/modulefiles/Core -------------------------------------------------- ARACNE/20110228 blat/35 gcc/4.9.3 intel_mkl/16.0.3 (D) mvapich2/2.2 (D) HISAT2/2.0.4 bowtie2/2.2.6 gcc/4.9.4 intel_mkl/17.0.0 openmpi/2.1.1 HISAT2/2.1.0 (D) bowtie2/2.2.9 (D) gcc/5.3 intel_mkl/17.0.1 pgi/16.9 MATLAB/R2017a bwa/0.7.12 gcc/5.4 (D) intel_mkl/17.0.2 pgi/16.10 (D) MATLAB/R2017b (D) bwa/0.7.13 (D) hdf5/1.8.16 java/1.7.0_79 python/2.7.11 Mathematica/11.1 cuda/7.5 intel/16.0.1 java/1.8.0_66 python/2.7.12 OpenCV/2.3.1 cuda/8.0 intel/16.0.3 (D) java/1.8.0_73 python/3.5.0 STAR/2.5.2a cuda/9.0 (D) intel/16.0.4 java/1.8.0_121 python/3.5.2 (D) Trinotate/2.0.2 cudnn/7.0.3 intel/17.0.0 java/1.8.0_141 samtools/0.1.19 bamtools/2.4.0 cufflinks/2.2.1 intel/17.0.1 java/1.8.0_152 (D) samtools/1.2 bcftools/1.2 delly/0.7.6 intel/17.0.2 modeller/9.16 samtools/1.3.1 (D) bedtools2/2.25.0 gaussian/03revE01 intel/17.0.4 moe/2016.0802 trinityrnaseq/2.1.1 blast/2.6.0 gaussian/09revD01 (D) intel_mkl/16.0.1 mvapich2/2.1 Understanding this output: The packages with a (D) are the default versions for packages where multiple versions are available. To see a comprehensive list of all available modules (not just the core modules) use the module spider command, or module keyword command (see CheatSheat section). To be able to use community-contributed software, execute module use /projects/community/modulefiles before using the the above mentioned commands (see Community section). module spider --------------------------------------------------------------------------------------------------------------- The following is a list of the modules currently available: --------------------------------------------------------------------------------------------------------------- ARACNE: ARACNE/20110228 ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context HISAT2: HISAT2/2.0.4, HISAT2/2.1.0 HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes HMMER: HMMER/3.1b2 HMMER: biosequence analysis using profile hidden Markov models MATLAB: MATLAB/R2017a, MATLAB/R2017b MATLAB: The Language of Technical Computing Mathematica: Mathematica/11.1 Wolfram Mathematica: Modern Technical Computing NAMD: NAMD/2.10 NAMD: Scalable Molecular Dynamics ORCA: ORCA/3.0.3 ORCA: An ab initio, DFT and semiempirical SCF-MO package OpenCV: OpenCV/2.3.1 OpenCV: Open Source Computer Vision Loading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use. command explanation usage module load load a software module module load intel/16.0.3 module unload remove select modules module unload intel/16.0.3 module purge load the default version of any software package module load intel Below are some examples. module load intel/16.0.3 mvapich2/2.1 module list Currently Loaded Modules: 1) intel/16.0.3 2) mvapich2/2.1 module unload mvapich2/2.1 module list Currently Loaded Modules: 1) intel/16.0.3 module purge module list No modules loaded module load intel module list Currently Loaded Modules: 1) intel/16.0.3 If you always use the same software modules, your ~/.bashrc (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired module load command(s) to the end of that file. You can always edit your ~/.bashrc file to change or remove those commands later. PLEASE NOTE: Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.","title":"Longer explanation"},{"location":"guides/Cluster_User_Guide/#running-slurm-jobs","text":"","title":"Running slurm jobs"},{"location":"guides/Cluster_User_Guide/#video-walkthrough","text":"Demoing sinfo, srun, squeue, scancel commands:","title":"Video walkthrough"},{"location":"guides/Cluster_User_Guide/#running-a-serial-single-core-job","text":"Here\u2019s an example of a SLURM job script for a serial job. I\u2019m running a program called \u201czipper\u201d which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --requeue # Return job to the queue if preempted #SBATCH --job-name=zipx001a # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=2000 # Real memory (RAM) required (MB) #SBATCH --time=02:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env cd /scratch/[your NetID] module purge module load intel/16.0.3 fftw/3.3.1 srun /scratch/[your NetID]/zipper/2.4.1/bin/zipper my-input-file.in Understanding this job script: A job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job. The lines beginning with #SBATCH contain commands intended only for the workload manager. My job will be assigned to the \u201cmain\u201d partition (job queue). If this job is preempted, it will be returned to the job queue and will start again when required resources are available This job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that. My job will be terminated when the run time limit has been reached, even if the program I\u2019m running is not finished. It is not possible to extend this time after a job starts running. Any output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number. Be sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable. Here\u2019s how to run a serial batch job, loading modules and using the sbatch command: sbatch my-job-script.sh The sbatch command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.","title":"Running a serial (single-core) job"},{"location":"guides/Cluster_User_Guide/#running-a-parallel-multicore-mpi-job","text":"Here\u2019s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here. #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --requeue # Return job to the queue if preempted #SBATCH --job-name=zipx001a # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=16 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=124000 # Real memory (RAM) required (MB) #SBATCH --time=02:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) #SBATCH --export=ALL # Export you current env to the job env cd /scratch/[your NetID] module purge module load intel/16.0.3 fftw/3.3.1 mvapich2/2.1 srun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper my-input-file.in Understanding this job script: The srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the \u2013ntasks value in your job\u2019s hardware allocation request. This job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM \u2014 it\u2019s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that. Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable. Here\u2019s how to run a parallel batch job, loading modules and using the sbatch command: sbatch my-job-script.sh Note here that I\u2019m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable. Here\u2019s how to run a parallel batch job, loading modules and using the sbatch command: sbatch my-job-script.sh","title":"Running a parallel (multicore MPI) job"},{"location":"guides/Cluster_User_Guide/#running-array-of-jobs","text":"Array job is an approach to handle multiple jobs with single job script. Here is an example to submit 500 jobs with single job script. #!/bin/bash #SBATCH --partition=main # Name of the partition #SBATCH --job-name=arrayjobs # Name of the job #SBATCH --ntasks=1 # Number of tasks #SBATCH --cpus-per-task=1 # Number of CPUs per task #SBATCH --mem=1GB # Requested memory #SBATCH --array=0-499 # Array job will submit 500 jobs #SBATCH --time=00:10:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT file #SBATCH --error=slurm.%N.%j.err # STDERR file echo -n Executing on the machine: hostname echo Array Task ID : $SLURM_ARRAY_TASK_ID echo Random number : $RANDOM In the above description, the line #SBATCH --array=0-499 submits 500 jobs. The \"%\" seperator is useful to limit the number of jobs in the queue at a any given time. For example, the following line would send a maximum of 100 jobs to the queue. #SBATCH --array=0-499%100 It is a good practice to populate the queue with less than 500 jobs at any given time.","title":"Running array of jobs"},{"location":"guides/Cluster_User_Guide/#running-an-interactive-job","text":"An interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications. When submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job: [NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i srun: job 1365471 queued and waiting for resources srun: job 1365471 has been allocated resources [NetID@slepner045 ~]$ Notice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I\u2019ve been automatically logged-in to slepner045 and I\u2019m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the exit command.","title":"Running an interactive job"},{"location":"guides/Cluster_User_Guide/#monitoring-the-status-of-jobs","text":"_ The simplest way to quickly check on the status of active jobs is by using the squeue command: squeue -u [your NetID] JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1633383 main zipper xx345 R 1:15 1 slepner36 Here, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS). For summary accounting information (including jobs that have already completed), you can use the sacct command: sacct JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1633383 zipper main statx 16 RUNNING 0:0 Here, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED. For complete and detailed job info, you can use the scontrol show job [JobID] command: scontrol show job 244348 JobId=244348 JobName=XIoT22 UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A Priority=5050 Nice=0 Account=oarc QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19 StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=main AllocNode:Sid=amarel1:22391 ReqNodeList=(null) ExcNodeList=(null) NodeList=hal0053 BatchHost=hal0053 NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:* TRES=cpu=28,mem=124000M,node=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/gc563/run.STMV.CPU.slurm WorkDir=/scratch/gc563 StdErr=/scratch/gc563/slurm.%N.244348.out StdIn=/dev/null StdOut=/scratch/gc563/slurm.%N.244348.out Power= If your jobs have already completed (or have been terminated), you can see details about those jobs using the sacct command with your NetID and a start time for the list of jobs this command will produce. sacct --user=[NetID] --starttime=2018-07-03 --format=JobID,Partition,JobName,MaxRSS,NodeList,Elapsed,MaxDiskRead,MaxDiskWrite,State","title":"Monitoring the status of jobs"},{"location":"guides/Cluster_User_Guide/#killing-cancelling-terminating-jobs","text":"To terminate a job, regardless of whether it is running or just waiting in the job queue, use the scancel command and specify the JobID number of the job you wish to terminate: scancel 1633383 A job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this: slurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***","title":"Killing/ cancelling/ terminating jobs"},{"location":"guides/Cluster_User_Guide/#installing-your-own-software","text":"Package management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]). As an example, here are the steps for installing ZIPPER, a generic example package that doesn\u2019t actually exist: Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the wget utility: wget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the tar utility to unpack the contents of these files: tar -zxf zipper-4.1.5.tar.gz Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software. cd zipper-4.1.5 less README Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER: module load intel/16.0.3 mvapich2/2.1 Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the ./configure -\u2013help command. What you see below is just a typical example of special options that might be specified. ./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared make -j 4 make install Several packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script \u2013 these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation: - /home/[NetID]/zipper/4.1.5/bin where executables will be placed - /home/[NetID]/zipper/4.1.5/lib where library files will be placed - /home/[NetID]/zipper/4.1.5/include where header files will be placed - /home/[NetID]/zipper/4.1.5/share/man where documentation will be placed Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don\u2019t \u201cknow\u201d about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment: export PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH export C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH export MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH These export commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.","title":"Installing your own software"},{"location":"guides/Cluster_User_Guide/#singularity","text":"Singularity is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers. Docker is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.","title":"Singularity"},{"location":"guides/Cluster_User_Guide/#important-differences-between-docker-and-singularity","text":"Docker and Singularity have their own container formats. Docker containers can be imported and run using Singularity. Docker containers usually run as root, which means you cannot run Docker on a shared computing system (cluster). Singularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.","title":"Important differences between Docker and Singularity:"},{"location":"guides/Cluster_User_Guide/#importing-a-docker-image","text":"If you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container. Here's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel. If you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories. Start your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel. $ sudo docker run -it ubuntu:latest bash root@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc","title":"Importing a Docker image:"},{"location":"guides/Cluster_User_Guide/#exporting-your-docker-image","text":"Find the name of your Docker image using the 'docker ps' command, $ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS NAMES 11a87dkw8748 ubuntu:latest bash 2 minutes ago Up 2 minutes bendakaya_pakodi In this example the name of the images is bendakaya_pakodi. Export this image to a tarball, $ sudo docker export bendakaya_pakodi ubuntu.tar","title":"Exporting your Docker image"},{"location":"guides/Cluster_User_Guide/#converting-to-a-singularity-image","text":"You will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel. Create an empty singularity image, and then import the exported docker image into it, $ sudo singularity create ubuntu.img Creating a sparse image with a maximum size of 1024MiB... Using given image size of 1024 Formatting image (/sbin/mkfs.ext3) Done. Image can be found at: ubuntu.img $ sudo singularity import ubuntu.img ubuntu.tar","title":"Converting to a Singularity image"},{"location":"guides/Cluster_User_Guide/#using-singularity-containers-inside-a-slurm-job","text":"Transfer your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel. You can run any task/program inside the container by prefacing it with singularity exec [your image name] Here is a simple example job script that executes commands inside a container, #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=sing2me # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=4000 # Real memory (RAM) required (MB) #SBATCH --time=00:30:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file module purge module load singularity/.2.5.1 ## Where am I running? srun singularity exec ubuntu.img hostname ## What is the current time and date? srun singularity exec ubuntu.img date If you created directories for any Amarel filesystems, you should find they are mounted inside your container, mount | grep gpfs /dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime) /dev/projects/oarc on /projects/oarc type gpfs (rw,relatime) NOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.","title":"Using Singularity containers inside a SLURM job"},{"location":"guides/Cluster_User_Guide/#guidelines-for-managing-data-on-scratch","text":"","title":"Guidelines for managing data on scratch"},{"location":"guides/Cluster_User_Guide/#table-data-management-on-oarc-cluster","text":"Fileset Users quota Time Back up External storage options Transfer Tools scratch general 20 TB 90 days no - Personal devices: Laptop, Desktop, USB drives, etc. - Online drives: Box(unlimited for Rutgers), Google Drive, OneDrive, Dropbox, etc. - Public cloud storages: GCP, Azure, AWS, etc. -CLI tools: scp, rsync, rcloud - GUI tools: Filezilla, WinSCP, Open OnDemand, rclone-browser (via sirius3) home general 100GB Valid account yes project node owners varies varies yes Our /scratch fileset offers unlimited temporary storage. You can process large volumes of data in /scratch, but the data cannot be older than 90 days. Files older than 90 days are automatically removed, and there is no backup. So, it is a best practice to move the data from scratch as soon as possible.","title":"Table: Data management on OARC cluster"},{"location":"guides/Cluster_User_Guide/#internal-and-external-storage-options","text":"For long-term storage, you can utilize the internal storage at OARC's cluster or any external storage resource. OARC's internal storage includes /home and /projects file sets. If your data can be accommodated in your /home directory, it is usually best to keep important items there. Node owners may want to use their /projects storage space. If you don't have enough space within one of OARC's clusters, you should move the data to an external storage resource. Here, we point out a few options to consider for external storage. Personal devices such as laptop, desktop, workstations, or USB drives can serve as external storage. Third party vendors offer several storage options that are either free or paid services. Box offers unlimited free storage for the Rutgers community. Google offers unlimited free storage for the academic community by a service called Google Drive File Stream. One Drive and DropBox services are free to use up to a set quota. Public cloud providers such as AWS, Azure, and GCP charge a fee to move the data in and out of the cloud.","title":"Internal and external storage options"},{"location":"guides/Cluster_User_Guide/#data-transfer-tools","text":"There are several tools and methods available to move the data that are based on command line interface (CLI) or Graphical User Interface (GUI). You can use command line tools such as scp, rsync, or rcloud to move the data from OARC cluster to any external storage. scp and rsync are suitable choices if you are moving data from OARC to your laptop or desktop that runs linux or MacOS. rcloud is an excellent choice to move your data to many storage services such as Box, DropBox, cloud, etc. GUI tools such as Filezilla, WinSCP, rclone-browser, and Open OnDemand are popular choices to move the data from variety of storage end points. Among GUI tools, FileZilla, rclone-browser, and Open OnDemand tools should work on any operating systems (Linux, MacOS, Windows) since they interact through a web-browser. Please refer our user guide for more details about how to use specific CLI tools such as scp, rsync, and rclone. Here is a simple command line example to move your data using scp. scp your-netid@amarel.hpc.rutgers.edu: file location on oarc cluster file location on your device where \u2018your-netid\u2019 is your Rutger\u2019s netid, which is same as your account name on OARC cluster. For example, to move data from scratch space located at /scratch/your-netid/your-data-file to a home directory on your laptop using scp command, type the following in your terminal shell prompt on your laptop. scp your-netid@amarel.hpc.rutgers.edu:/scratch/your-net-id/your-data-file ~/your-data-file","title":"Data transfer tools"},{"location":"guides/Cluster_User_Guide/#troubleshooting-common-problems","text":"Failure to load module dependencies/prerequisites: module load R-Project/3.4.1 Lmod has detected the following error: These module(s) exist but cannot be loaded as requested: R-Project/3.4.1 Try: module spider R-Project/3.4.1 to see how to load the module(s). This software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load: module spider R-Project/3.4.1 This module can only be loaded through the following modules: intel/17.0.4 Help: This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers. Ah-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1","title":"Troubleshooting/ Common Problems"},{"location":"guides/Cluster_User_Guide/#acknowledging-amarel","text":"Please reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include: Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey A suggested acknowledgement is: The authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu","title":"Acknowledging Amarel"},{"location":"guides/classes/","text":"Didact for classes Didact is a small cluster dedicated to teaching. It can be accessed at this didact URL . It is equipped with R, Jupyter notebooks, and a Linux desktop which can be used to start other graphical applications like stata and SPSS. Info for instructors we have an instructor packet for you - please contact us at help@oarc.rutgers.edu Please fill in access request for Amarel before the start of class and say you want to teach a class we will create accounts in one go for your students and notify you. We need a list of student netids and a list of student email addresses in a pasteable form (csv or xlsx, not pdf) you need to be able to troubleshoot students connection problems to some extent - please read VERY carefully VPN access problems section - this is the most common pitfall students face you should communicate to students when their accounts are ready, expectations on OARC support, and information about expected account removal Info for students OARC can't provide help for issues outside the control of our office - most frequently VPN access. For this, you need to contact OIT helpdesk . You should submit access requests through course registration form even if your account already exists as a result of instructor's request. This is for our accounting purposes. Every didact account requires also an Amarel account, which will be automatically provided to you. You will be able to continue using the software for your research after the course ends. Your login and password are your Rutgers netid and the corresponding password. We do not manage that information, we use the central IT authentication service. Steps to run stata/spss or other programs Go the the form to launch an interactive desktop on didact or on Amarel and click blue button \"Launch\". Screenshot If you have trouble with this step, see the \"VPN access\" and \"Debugging\" sections. A new tab will open, where the session is being created. After about 30 seconds when the session is ready, a blue button will appear \"Launch noVNC in New Tab\" Screenshot - click on it, and your desktop is ready to be used. Screenshot icons should appear on your desktop. If you see no icons, please see this question - you need to copy the files from /projects/community/desktops to Desktop folder in your home folder. You can do this eitherby using a terminal linux command, or by using the file explorer. User forum Please find answers to some common questions and help each other by asking and answering questions at our user forum VPN access To access any OARC services from off-campus, you need a VPN. This is for Rutgers security reasons, and is a campus-wide service administered outside our office. Please direct any questions to OIT helpdesk . For your convenience, here is a brief collection of links and directions: you can download VPN client at the Rutgers software page , from the \"Download\" tab. Please DO NOT USE web-based VPN access , as it interferes with the OpenOnDemand application (see Known Issues below). This is how the web-based VPN access looks like - Screenshot instructions to install VPN client on a Mac from School of Arts and Sciences if you have a newer Windows laptop, you need 64bit version, and not 32bit version Known issues this page can't be reached ( screenshot )- this means you connected from off campus and you are not connected through VPN. If you can't see this page , you are not on campus and not connected through VPN. Please download the VPN client. failed to connect to VNC server - one of two things is possible: you connected through web-based VPN, which doesn't work well with ondemand. You will recognize this problem by looking at the URL and it starts with https://ssl-vpn.rutgers.edu/pun/sys/dashboard and your screenshot looks like this you are using Safari as your browser. Please try Firefox instead. Debugging questions If you have trouble connecting to didact, please try these steps to isolate the problem: please read about VPN access - this trips users most often. Either you didn't connect through VPN, or you didn't download the VPN client, and instead used this webpage to login to VPN. try a different browser (e.g. Safari doesn't work for some people) try a different computer try a different operating system ask your peers if they are successful in connecting raise the problem with your instructor","title":"Classes"},{"location":"guides/classes/#didact-for-classes","text":"Didact is a small cluster dedicated to teaching. It can be accessed at this didact URL . It is equipped with R, Jupyter notebooks, and a Linux desktop which can be used to start other graphical applications like stata and SPSS.","title":"Didact for classes"},{"location":"guides/classes/#info-for-instructors","text":"we have an instructor packet for you - please contact us at help@oarc.rutgers.edu Please fill in access request for Amarel before the start of class and say you want to teach a class we will create accounts in one go for your students and notify you. We need a list of student netids and a list of student email addresses in a pasteable form (csv or xlsx, not pdf) you need to be able to troubleshoot students connection problems to some extent - please read VERY carefully VPN access problems section - this is the most common pitfall students face you should communicate to students when their accounts are ready, expectations on OARC support, and information about expected account removal","title":"Info for instructors"},{"location":"guides/classes/#info-for-students","text":"OARC can't provide help for issues outside the control of our office - most frequently VPN access. For this, you need to contact OIT helpdesk . You should submit access requests through course registration form even if your account already exists as a result of instructor's request. This is for our accounting purposes. Every didact account requires also an Amarel account, which will be automatically provided to you. You will be able to continue using the software for your research after the course ends. Your login and password are your Rutgers netid and the corresponding password. We do not manage that information, we use the central IT authentication service.","title":"Info for students"},{"location":"guides/classes/#steps-to-run-stataspss-or-other-programs","text":"Go the the form to launch an interactive desktop on didact or on Amarel and click blue button \"Launch\". Screenshot If you have trouble with this step, see the \"VPN access\" and \"Debugging\" sections. A new tab will open, where the session is being created. After about 30 seconds when the session is ready, a blue button will appear \"Launch noVNC in New Tab\" Screenshot - click on it, and your desktop is ready to be used. Screenshot icons should appear on your desktop. If you see no icons, please see this question - you need to copy the files from /projects/community/desktops to Desktop folder in your home folder. You can do this eitherby using a terminal linux command, or by using the file explorer.","title":"Steps to run stata/spss or other programs"},{"location":"guides/classes/#user-forum","text":"Please find answers to some common questions and help each other by asking and answering questions at our user forum","title":"User forum"},{"location":"guides/classes/#vpn-access","text":"To access any OARC services from off-campus, you need a VPN. This is for Rutgers security reasons, and is a campus-wide service administered outside our office. Please direct any questions to OIT helpdesk . For your convenience, here is a brief collection of links and directions: you can download VPN client at the Rutgers software page , from the \"Download\" tab. Please DO NOT USE web-based VPN access , as it interferes with the OpenOnDemand application (see Known Issues below). This is how the web-based VPN access looks like - Screenshot instructions to install VPN client on a Mac from School of Arts and Sciences if you have a newer Windows laptop, you need 64bit version, and not 32bit version","title":"VPN access"},{"location":"guides/classes/#known-issues","text":"this page can't be reached ( screenshot )- this means you connected from off campus and you are not connected through VPN. If you can't see this page , you are not on campus and not connected through VPN. Please download the VPN client. failed to connect to VNC server - one of two things is possible: you connected through web-based VPN, which doesn't work well with ondemand. You will recognize this problem by looking at the URL and it starts with https://ssl-vpn.rutgers.edu/pun/sys/dashboard and your screenshot looks like this you are using Safari as your browser. Please try Firefox instead.","title":"Known issues"},{"location":"guides/classes/#debugging-questions","text":"If you have trouble connecting to didact, please try these steps to isolate the problem: please read about VPN access - this trips users most often. Either you didn't connect through VPN, or you didn't download the VPN client, and instead used this webpage to login to VPN. try a different browser (e.g. Safari doesn't work for some people) try a different computer try a different operating system ask your peers if they are successful in connecting raise the problem with your instructor","title":"Debugging questions"},{"location":"guides/community_resources/","text":"Useful resources contributed or setup by members of the Amarel Research Computing Community for the benefit of fellow members Community-Contributed Software Modules Users who wish to share software with the Amarel community of researchers can do so using the guidelines and suggestions presented here. Log-in to Amarel or Perceval and check out /projects/community Have you created a particularly fast build of your favorite simulation tool? Have you compiled a packaged that's notoriously hard to compile? Want to share some software with collaborators within the Amarel user community? OARC has setup a dedicated repository for community-contributed software and associated modulefiles in /projects/community. All Amarel and Perceval users can contribute software to this repository. We have established some basic guidelines to help keep things organized. If you would like to see the community-contributed software packages when you run the 'module avail' or 'module spider' command, add /projects/community/modulefiles to your MODULEPATH, export MODULEPATH=$MODULEPATH:/projects/community/modulefiles or add that line to your ~/.bashrc file for that setting to persist. A Note About Governance Please follow the guidelines presented here. The OARC research support team will remove or edit contributions that do not conform to these guidelines. If you find any problems or if you have questions, please let us know by sending a message to User Support Contributed software is officially unsupported. However, individual users who setup contributed software may be willing to answer basic questions about usability, performance, or selected build options. We certainly want to encourage users to contribute software, but at the same time, we do not want to burden busy researchers with support expectations. Be considerate if you contact a user about contributed software. Permissions We've adjusted permissions (and set the sticky bit) so Amarel and Perceval users can write to /projects/community and its subdirectories. # chmod 777 /projects/community # chmod o+t /projects/community This means that only a file's owner, a directory's owner, or root can rename or delete those files or directories, thus preventing others from modifying your contributions. You can adjust permissions of the files and directories you create, which may be necessary to enable others to use the software you place there. Naming Conventions New software packages should be added using the following conventions. Software Packages Directory Structure Modulefiles /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID Core packages /projects/community/pkg-name/version/NetID Compiler-dependent packages /projects/community/compiler-name/version/pkg-name/version/NetID Compiler-dependent packages using CUDA /projects/community/compiler-name/version/cuda/version/pkg-name/version/NetID MPI-Compiler-dependent packages /projects/community/compiler-name/version/mpi-name/version/pkg-name/version/NetID MPI-Compiler-dependent packages using CUDA /projects/community/compiler-name/version/mpi-name/version/cuda/version/pkg-name/version/NetID Good Housekeeping and Reproducibility in Research (1) All contributed software packages MUST have a README.OARC file created and stored in the directory containing the installed software. This file must include your contact info and any useful information about the software. This file should include all commands used to build the package since this information is important for anyone wishing to use contributed software for research purposes. There is a long but useful example README.OARC file in /projects/community/gcc/7.3.0/gc563 (2) All contributed software packages MUST have an associated Lmod modulefile so users can easily access the software. There are details for doing this and examples below. An Example: GCC-7.3.0 March 23, 2018: I want to build GCC, The GNU Compiler Collection, version 7.3.0 since a project I'm working on requires it, but GCC-7.3.0 is not already setup on Amarel. I'll begin by \"bootstrapping\" using the core/default tools that come with the version of CentOS installed on Amarel. That version (at the time of this writing) is $ cat /etc/*release | head -n 1 CentOS Linux release 7.4.1708 (Core) plus the additional components and tools that come from the Enterprise Linux repository. This version of CentOS is based on the following kernel and GCC version: $ cat /proc/version Linux version 3.10.0-693.21.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Wed Mar 7 19:03:37 UTC 2018 Installing GCC requires a few prerequisite packages: these must be installed and configured for use before installing GCC. Those prerequisites are: GNU Multiple Precision Library (GMP) version 4.3.2 (or later) MPFR Library version 2.4.2 (or later) MPC Library version 0.8.1 (or later) I'll download, test, and install a recent version of each of these software packages. Since I'm just trying to get setup for installing GCC here, I'll skip the description of these steps because I'll discuss those steps in detail later (below). wget https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2 tar -jxf gmp-6.1.2.tar.bz2 ./configure --prefix=/projects/community/gmp/6.1.2/gc563 --enable-cxx --enable-fft make -j 4 make check make install cd .. ; rm -rf gmp-6.1.2* Note: the following environment settings are needed for installing the next package. These environment settings have not yet been set for my shell session, so that's why I'm not prepending them to existing path settings here: export C_INCLUDE_PATH=/home/gc563/gmp/6.1.2/include export CPLUS_INCLUDE_PATH=/projects/community/gmp/6.1.2/gc563/include export LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib export LD_LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib wget http://www.mpfr.org/mpfr-current/mpfr-4.0.1.tar.bz2 tar -jxf mpfr-4.0.1.tar.bz2 cd mpfr-4.0.1 ./configure --prefix=/projects/community/mpfr/4.0.1/gc563 --enable-thread-safe make -j 4 make install cd .. ; rm -rf mpfr-4.0.1* Note: the following environment settings have already been set, so that's why I'm prepending additional segments to existing path settings here: export C_INCLUDE_PATH=/home/gc563/mpfr/4.0.1/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/projects/community/mpfr/4.0.1/gc563/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LD_LIBRARY_PATH wget https://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz tar -zxf mpc-1.1.0.tar.gz cd mpc-1.1.0 ./configure --prefix=/projects/community/mpc/1.1.0/gc563 make make install cd .. ; rm -rf mpc-1.1.0* export C_INCLUDE_PATH=/home/gc563/mpc/1.1.0/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/projects/community/mpc/1.1.0/gc563/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LD_LIBRARY_PATH Now, I can finally start building GCC. I'll download the latest stable supported release of GCC, open the tarball, and get ready to install it: wget http://mirrors.concertpass.com/gcc/releases/gcc-7.3.0/gcc-7.3.0.tar.gz tar -zxf gcc-7.3.0.tar.gz cd gcc-7.3.0 Like the 3 prerequisite software packages I just installed, the GCC compiler suite uses the traditional configure/make/make-install installation procedure. The first step is to run the Bash script named 'configure'. The configure script inspects the existing hardware and/or software configuration of the machine where you are about to install this new software. If something is needed and missing (e.g., a needed library or access to specific tools), the configure script should let you know. The result of running this script is the creation of a file named 'Makefile' that contains the instructions for compiling your new software based on the findings from the configure script. In addition, when you run the configure script, you have the opportunity to select important options for how your new software will be setup. Most importantly, you can select where the software will be installed. This is accomplished using the '--prefix=[some location]' option. In my case, I'll specify a location in my /home directory. There are many other options and those options vary depending on what you're installing. To see a summary of the available options, use './configure --help' and for details about what those options mean, see the documentation for your new software. $ ./configure --prefix=/projects/community/gcc/7.3.0/gc563--with- mpc=/projects/community/mpc/1.1.0/gc563 --with- mpfr=/projects/community/mpfr/4.0.1/gc563 --with- gmp=/projects/community/6.1.2/gc563 --disable-multilib Next, the 'make' utility will be used to compile your software. It requires the file named 'Makefile' that you created with the configure step. The Makefile indicates the sequence that an options to be used for building various components of your new software. The Makefile uses labels (i.e., names for different sections of the procedures contained therein), so entire sections of the Makefile can be skipped or used in a particular order. Running 'make' to compile your code can take a long time for some packages. You can parallelize this step to some extent using the '-j [n]' option where n is the number of tasks you wish to run simultaneously. This isn't quite the same as running a parallel or multithreaded program, but it can help get a large number of compiling tasks done in a shorter time. make -j 8 Finally, I need to copy my newly created executables and/or libraries to their final destinations (the location I specified with '--prefix=' in the configure step, above). One of the labels present in the Makefile is named 'install' and I can instruct make to run the commands under that label as follows: make install cd .. ; rm -rf gcc-7.3.0* Modulefiles for Community-Contributed Packages All contributed software packages MUST have an associated Lmod modulefile so users can easily access the software. Directory structure for modulefiles (omit parts that aren't used, like /CUDA/version or /mpi-name/version): /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID Here's an example Lmod modulefile for the GCC-7.3.0 example (above): help( [[ This module loads the GNU Compiler Collection version 7.3.0. The GNU Compiler Collection includes front ends for C, C++, Fortran, as well as libraries for these languages. ]]) whatis( Description: GCC: the GNU Compiler Collection ) whatis( URL: https://gcc.gnu.org ) conflict( gcc ) load( gmp/6.1.2-gc563 ) load( mpfr/4.0.1-gc563 ) load( mpc/1.1.0-gc563 ) local base = pathJoin( /projects/community , myModuleName(), 7.3.0 , gc563 ) prepend_path( PATH , pathJoin(base, bin )) prepend_path( C_INCLUDE_PATH , pathJoin(base, include )) prepend_path( CPLUS_INCLUDE_PATH , pathJoin(base, include )) prepend_path( LIBRARY_PATH , pathJoin(base, lib64 )) prepend_path( LD_LIBRARY_PATH , pathJoin(base, lib64 )) prepend_path( MANPATH , pathJoin(base, share/man )) Once created, this file should be named \"7.3.0-gc563.lua\" because that's the name that will appear in the list of modules when a user runs the 'module avail' or 'module spider' command. Understanding this file: A 'conflict' can be specified to prevent loading a potentially conflicting module (e.g., loading 2 different versions of GCC at the same time) In this example, 3 prerequisite modules are automatically loaded when this GCC module is loaded. Alternatively, you can simply specify a prerequisite using 'prereq(\"module/version\")' to notify a user of a prerequisite without automatically trying to load it. The 'local base' statement establishes the general path for your software's various subdirectories The 'prepend_path' statements define the specific path additions for those subdirectories","title":"Community"},{"location":"guides/community_resources/#community-contributed-software-modules","text":"Users who wish to share software with the Amarel community of researchers can do so using the guidelines and suggestions presented here.","title":"Community-Contributed Software Modules"},{"location":"guides/community_resources/#log-in-to-amarel-or-perceval-and-check-out-projectscommunity","text":"Have you created a particularly fast build of your favorite simulation tool? Have you compiled a packaged that's notoriously hard to compile? Want to share some software with collaborators within the Amarel user community? OARC has setup a dedicated repository for community-contributed software and associated modulefiles in /projects/community. All Amarel and Perceval users can contribute software to this repository. We have established some basic guidelines to help keep things organized. If you would like to see the community-contributed software packages when you run the 'module avail' or 'module spider' command, add /projects/community/modulefiles to your MODULEPATH, export MODULEPATH=$MODULEPATH:/projects/community/modulefiles or add that line to your ~/.bashrc file for that setting to persist.","title":"Log-in to Amarel or Perceval and check out /projects/community"},{"location":"guides/community_resources/#a-note-about-governance","text":"Please follow the guidelines presented here. The OARC research support team will remove or edit contributions that do not conform to these guidelines. If you find any problems or if you have questions, please let us know by sending a message to","title":"A Note About Governance"},{"location":"guides/community_resources/#user-support","text":"Contributed software is officially unsupported. However, individual users who setup contributed software may be willing to answer basic questions about usability, performance, or selected build options. We certainly want to encourage users to contribute software, but at the same time, we do not want to burden busy researchers with support expectations. Be considerate if you contact a user about contributed software.","title":"User Support"},{"location":"guides/community_resources/#permissions","text":"We've adjusted permissions (and set the sticky bit) so Amarel and Perceval users can write to /projects/community and its subdirectories. # chmod 777 /projects/community # chmod o+t /projects/community This means that only a file's owner, a directory's owner, or root can rename or delete those files or directories, thus preventing others from modifying your contributions. You can adjust permissions of the files and directories you create, which may be necessary to enable others to use the software you place there.","title":"Permissions"},{"location":"guides/community_resources/#naming-conventions","text":"New software packages should be added using the following conventions. Software Packages Directory Structure Modulefiles /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID Core packages /projects/community/pkg-name/version/NetID Compiler-dependent packages /projects/community/compiler-name/version/pkg-name/version/NetID Compiler-dependent packages using CUDA /projects/community/compiler-name/version/cuda/version/pkg-name/version/NetID MPI-Compiler-dependent packages /projects/community/compiler-name/version/mpi-name/version/pkg-name/version/NetID MPI-Compiler-dependent packages using CUDA /projects/community/compiler-name/version/mpi-name/version/cuda/version/pkg-name/version/NetID","title":"Naming Conventions"},{"location":"guides/community_resources/#good-housekeeping-and-reproducibility-in-research","text":"(1) All contributed software packages MUST have a README.OARC file created and stored in the directory containing the installed software. This file must include your contact info and any useful information about the software. This file should include all commands used to build the package since this information is important for anyone wishing to use contributed software for research purposes. There is a long but useful example README.OARC file in /projects/community/gcc/7.3.0/gc563 (2) All contributed software packages MUST have an associated Lmod modulefile so users can easily access the software. There are details for doing this and examples below.","title":"Good Housekeeping and Reproducibility in Research"},{"location":"guides/community_resources/#an-example-gcc-730","text":"March 23, 2018: I want to build GCC, The GNU Compiler Collection, version 7.3.0 since a project I'm working on requires it, but GCC-7.3.0 is not already setup on Amarel. I'll begin by \"bootstrapping\" using the core/default tools that come with the version of CentOS installed on Amarel. That version (at the time of this writing) is $ cat /etc/*release | head -n 1 CentOS Linux release 7.4.1708 (Core) plus the additional components and tools that come from the Enterprise Linux repository. This version of CentOS is based on the following kernel and GCC version: $ cat /proc/version Linux version 3.10.0-693.21.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Wed Mar 7 19:03:37 UTC 2018 Installing GCC requires a few prerequisite packages: these must be installed and configured for use before installing GCC. Those prerequisites are: GNU Multiple Precision Library (GMP) version 4.3.2 (or later) MPFR Library version 2.4.2 (or later) MPC Library version 0.8.1 (or later) I'll download, test, and install a recent version of each of these software packages. Since I'm just trying to get setup for installing GCC here, I'll skip the description of these steps because I'll discuss those steps in detail later (below). wget https://gmplib.org/download/gmp/gmp-6.1.2.tar.bz2 tar -jxf gmp-6.1.2.tar.bz2 ./configure --prefix=/projects/community/gmp/6.1.2/gc563 --enable-cxx --enable-fft make -j 4 make check make install cd .. ; rm -rf gmp-6.1.2* Note: the following environment settings are needed for installing the next package. These environment settings have not yet been set for my shell session, so that's why I'm not prepending them to existing path settings here: export C_INCLUDE_PATH=/home/gc563/gmp/6.1.2/include export CPLUS_INCLUDE_PATH=/projects/community/gmp/6.1.2/gc563/include export LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib export LD_LIBRARY_PATH=/projects/community/gmp/6.1.2/gc563/lib wget http://www.mpfr.org/mpfr-current/mpfr-4.0.1.tar.bz2 tar -jxf mpfr-4.0.1.tar.bz2 cd mpfr-4.0.1 ./configure --prefix=/projects/community/mpfr/4.0.1/gc563 --enable-thread-safe make -j 4 make install cd .. ; rm -rf mpfr-4.0.1* Note: the following environment settings have already been set, so that's why I'm prepending additional segments to existing path settings here: export C_INCLUDE_PATH=/home/gc563/mpfr/4.0.1/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/projects/community/mpfr/4.0.1/gc563/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/projects/community/mpfr/4.0.1/gc563/lib:$LD_LIBRARY_PATH wget https://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz tar -zxf mpc-1.1.0.tar.gz cd mpc-1.1.0 ./configure --prefix=/projects/community/mpc/1.1.0/gc563 make make install cd .. ; rm -rf mpc-1.1.0* export C_INCLUDE_PATH=/home/gc563/mpc/1.1.0/include:$C_INCLUDE_PATH export CPLUS_INCLUDE_PATH=/projects/community/mpc/1.1.0/gc563/include:$CPLUS_INCLUDE_PATH export LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LIBRARY_PATH export LD_LIBRARY_PATH=/projects/community/mpc/1.1.0/gc563/lib:$LD_LIBRARY_PATH Now, I can finally start building GCC. I'll download the latest stable supported release of GCC, open the tarball, and get ready to install it: wget http://mirrors.concertpass.com/gcc/releases/gcc-7.3.0/gcc-7.3.0.tar.gz tar -zxf gcc-7.3.0.tar.gz cd gcc-7.3.0 Like the 3 prerequisite software packages I just installed, the GCC compiler suite uses the traditional configure/make/make-install installation procedure. The first step is to run the Bash script named 'configure'. The configure script inspects the existing hardware and/or software configuration of the machine where you are about to install this new software. If something is needed and missing (e.g., a needed library or access to specific tools), the configure script should let you know. The result of running this script is the creation of a file named 'Makefile' that contains the instructions for compiling your new software based on the findings from the configure script. In addition, when you run the configure script, you have the opportunity to select important options for how your new software will be setup. Most importantly, you can select where the software will be installed. This is accomplished using the '--prefix=[some location]' option. In my case, I'll specify a location in my /home directory. There are many other options and those options vary depending on what you're installing. To see a summary of the available options, use './configure --help' and for details about what those options mean, see the documentation for your new software. $ ./configure --prefix=/projects/community/gcc/7.3.0/gc563--with- mpc=/projects/community/mpc/1.1.0/gc563 --with- mpfr=/projects/community/mpfr/4.0.1/gc563 --with- gmp=/projects/community/6.1.2/gc563 --disable-multilib Next, the 'make' utility will be used to compile your software. It requires the file named 'Makefile' that you created with the configure step. The Makefile indicates the sequence that an options to be used for building various components of your new software. The Makefile uses labels (i.e., names for different sections of the procedures contained therein), so entire sections of the Makefile can be skipped or used in a particular order. Running 'make' to compile your code can take a long time for some packages. You can parallelize this step to some extent using the '-j [n]' option where n is the number of tasks you wish to run simultaneously. This isn't quite the same as running a parallel or multithreaded program, but it can help get a large number of compiling tasks done in a shorter time. make -j 8 Finally, I need to copy my newly created executables and/or libraries to their final destinations (the location I specified with '--prefix=' in the configure step, above). One of the labels present in the Makefile is named 'install' and I can instruct make to run the commands under that label as follows: make install cd .. ; rm -rf gcc-7.3.0*","title":"An Example: GCC-7.3.0"},{"location":"guides/community_resources/#modulefiles-for-community-contributed-packages","text":"All contributed software packages MUST have an associated Lmod modulefile so users can easily access the software. Directory structure for modulefiles (omit parts that aren't used, like /CUDA/version or /mpi-name/version): /projects/community/modulefiles/compiler-name/version/mpi-name/version/CUDA/version/pkg-name/version/NetID Here's an example Lmod modulefile for the GCC-7.3.0 example (above): help( [[ This module loads the GNU Compiler Collection version 7.3.0. The GNU Compiler Collection includes front ends for C, C++, Fortran, as well as libraries for these languages. ]]) whatis( Description: GCC: the GNU Compiler Collection ) whatis( URL: https://gcc.gnu.org ) conflict( gcc ) load( gmp/6.1.2-gc563 ) load( mpfr/4.0.1-gc563 ) load( mpc/1.1.0-gc563 ) local base = pathJoin( /projects/community , myModuleName(), 7.3.0 , gc563 ) prepend_path( PATH , pathJoin(base, bin )) prepend_path( C_INCLUDE_PATH , pathJoin(base, include )) prepend_path( CPLUS_INCLUDE_PATH , pathJoin(base, include )) prepend_path( LIBRARY_PATH , pathJoin(base, lib64 )) prepend_path( LD_LIBRARY_PATH , pathJoin(base, lib64 )) prepend_path( MANPATH , pathJoin(base, share/man )) Once created, this file should be named \"7.3.0-gc563.lua\" because that's the name that will appear in the list of modules when a user runs the 'module avail' or 'module spider' command. Understanding this file: A 'conflict' can be specified to prevent loading a potentially conflicting module (e.g., loading 2 different versions of GCC at the same time) In this example, 3 prerequisite modules are automatically loaded when this GCC module is loaded. Alternatively, you can simply specify a prerequisite using 'prereq(\"module/version\")' to notify a user of a prerequisite without automatically trying to load it. The 'local base' statement establishes the general path for your software's various subdirectories The 'prepend_path' statements define the specific path additions for those subdirectories","title":"Modulefiles for Community-Contributed Packages"},{"location":"guides/didact/","text":"Didact - teaching cluster didact is a small cluster dedicated to teaching classes. There will be no maintenances during the semesters and all upgrades and updates will be done depending on academic schedule so as not to interfere with classes. didact consists of 8 machines with 16 cores each. It has a separate filesystem from the general Amarel cluster. (See also https://ask.oarc.rutgers.edu/question/35/what-are-the-differences-between-didact-and-amarel/) How to connect to didact? To connect to didact, just open your browser and this link: https://didact.oarc.rutgers.edu (you must be connected to VPN, otherwise it will show up as \"Page not found\"). Use your netid and password to login (note: you must have an Amarel account. If you are part of a class teaching on didact, this will be provided to you.) OARC does not administer netids and passwords - we get the authentication from the central authentication service at Rutgers. How to launch a desktop on didact? If you are used to apps.rutgers.edu, you will find Didact Desktop familiar. Here is the series of instructions how to launch a desktop: login to https://didact.oarc.rutgers.edu Go to Interactive Sessions Click Didact Desktop Click \"Launch\" (wait up to 30 seconds until your environment is ready and the blue button appears on the bottom) Click \"Launch noVNC in New Tab\" What else can you launch on didact? Jupyter notebook (with python 3.6) RStudio How is didact different from apps.rutgers.edu? It uses somewhat different interface to launch a desktop. It has a graphical way to upload and download files Can an instructor distribute class materials directly? Yes! The instructor doesn't have to post materials on Canvas or other places - the instructor can deposit files directly in their student's home directories.","title":"Didact - teaching cluster"},{"location":"guides/didact/#didact-teaching-cluster","text":"didact is a small cluster dedicated to teaching classes. There will be no maintenances during the semesters and all upgrades and updates will be done depending on academic schedule so as not to interfere with classes. didact consists of 8 machines with 16 cores each. It has a separate filesystem from the general Amarel cluster. (See also https://ask.oarc.rutgers.edu/question/35/what-are-the-differences-between-didact-and-amarel/)","title":"Didact - teaching cluster"},{"location":"guides/didact/#how-to-connect-to-didact","text":"To connect to didact, just open your browser and this link: https://didact.oarc.rutgers.edu (you must be connected to VPN, otherwise it will show up as \"Page not found\"). Use your netid and password to login (note: you must have an Amarel account. If you are part of a class teaching on didact, this will be provided to you.) OARC does not administer netids and passwords - we get the authentication from the central authentication service at Rutgers.","title":"How to connect to didact?"},{"location":"guides/didact/#how-to-launch-a-desktop-on-didact","text":"If you are used to apps.rutgers.edu, you will find Didact Desktop familiar. Here is the series of instructions how to launch a desktop: login to https://didact.oarc.rutgers.edu Go to Interactive Sessions Click Didact Desktop Click \"Launch\" (wait up to 30 seconds until your environment is ready and the blue button appears on the bottom) Click \"Launch noVNC in New Tab\"","title":"How to launch a desktop on didact?"},{"location":"guides/didact/#what-else-can-you-launch-on-didact","text":"Jupyter notebook (with python 3.6) RStudio","title":"What else can you launch on didact?"},{"location":"guides/didact/#how-is-didact-different-from-appsrutgersedu","text":"It uses somewhat different interface to launch a desktop. It has a graphical way to upload and download files","title":"How is didact different from apps.rutgers.edu?"},{"location":"guides/didact/#can-an-instructor-distribute-class-materials-directly","text":"Yes! The instructor doesn't have to post materials on Canvas or other places - the instructor can deposit files directly in their student's home directories.","title":"Can an instructor distribute class materials directly?"},{"location":"guides/graphical/","text":"Transition from sirius On Feb 25, 2019, sirius machine was retired. This section documents information for users who are transitionining from sirius to Amarel. Similarities most of the same software is available as on the sirius machine - and many more. If there is a special program you use, and it is not available, it may be installed upon request. to see what software is there, try module use /projects/community/modulefiles and module avail (as on sirius). A comparison is provided in the last section similar graphical interface based on the Linux desktop is available as the interface to the cluster Differences sirius3 is not a single isolated machine - it is one \"compute\" node on the computer cluster of some 400+ machines there is a scheduler called slurm, which makes sharing of the resources fairer between users - making it impossible to monopolize resources sirius3 is intended to be used for graphical applications and for interactive code development. As such, code for more intense computations may be developed and tested on small scale on this machine. However, to perform a more intense computation, you should prepare a \"batch\" script (see cluster guide and video walktroughs for OpenOnDemand job templates). To ensure that there are enough resources for many users at once, each user on sirius3 is limited to 2 cores and 4Gb of memory for a maximum of 24 hours . you can submit sbatch jobs, from the sirius3 desktop, to the wider Amarel cluster when off campus, you need to connect to it using VPN, otherwise it will not connect Instructions to connect to OpenOnDemand - Sirius3 Desktop Open your browser at https://ondemand.hpc.rutgers.edu/ (make sure you are connected through VPN) Go to Interactive Apps - Sirius3 Deskop Choose number of hours that your session will be active (max 24) and number of cores (max 2) and click Launch In maximum 30 seconds, a blue button \"Launch VNC in New Tab\" will appear; clicking it will open Linux Desktop on sirius3 To get the Linux terminal (shell), click on the little terminal icon in the top left corner of the Linux Mate desktop (the systems bar) Use module load command to load appropriate version of your software to your software path, such as module load SAS Video walkthroughs We have prepared some videos to illustrate the functionalities of OpenOnDemand graphical interface to the Amarel cluster: https://youtu.be/D2C-yYzQ2uQ - 3:18 mins - lauching Sirius3 desktop (NOTE: 2 cores, not 1 as in video) https://youtu.be/GPpDG1eAiKo - 4:24 mins - Amarel ondemand intro - available functionalities https://youtu.be/zNrpS5CUa-8 - 4:24 mins - launching Amarel desktop https://youtu.be/znmnbli0d5Y - 6:33 mins - using the Job Composer to keep track of slurm jobs, create new slurm jobs from templates, and create new templates. FastX users IMPORTANT NOTE : When you login through FastX on Amarel, you are on the \"login\" node. This is a shared machine between tens of users at a time. It is essential that you don't run any intensive computations on this machine (like you did on sirius). Instead, you need to either ssh to sirius3, or submit a computational job through slurm (the scheduler). TODO: rest - how to connect from login node to sirius3. Software comparison between sirius and Amarel Please let us know if any of this software is not available on Amarel, but is essential to your work. We will consider adjusting our priorities to your needs: Software that exists on Amarel: AmberTools Bcl2fastq Bcl2fastq2 Bedtools2 Bowtie2 Cufflinks Clc-workbench Eclipse FastQC IGV Ncbi-blast - blast/2.6.0 on Amarel. This version 2.6, it is called ncbi-blast, and it should be called ncbi-blast gdb Glibc intel Java MATLAB Moe openbabel python RStudio - available through OnDemand; R available on command line Rna-star - this is called STAR/2.5.2a on Amarel Samtools SAS Trimmomatic Trinityrnaseq Trinotate Tophat In question, but important software and is a work in progress: Goldsuite - Vlad in communication with CCDC vendor Will install on request: Weka Knime Coot -- crystalographic package Phenix - crystallographic packag Mgltools - autodock related Autodock - community Autodock_vina -- community Chimera -- installed in community, needs module Vmd -- community Installation will progress as time permits Blasr Emboss HMMER Ngsplot Qiime PhyloCSF PLINK SNP2HLA RSEM These will not be considered for installation unless there are objections. CHANCE MACS MACS2 MEME MISO pandaseq abyss Transabyss WaveCNV Definitely not installing: Sybyl-x FASTX-Toolkit Llvm Spartan binutils","title":"Transition from sirius"},{"location":"guides/graphical/#transition-from-sirius","text":"On Feb 25, 2019, sirius machine was retired. This section documents information for users who are transitionining from sirius to Amarel.","title":"Transition from sirius"},{"location":"guides/graphical/#similarities","text":"most of the same software is available as on the sirius machine - and many more. If there is a special program you use, and it is not available, it may be installed upon request. to see what software is there, try module use /projects/community/modulefiles and module avail (as on sirius). A comparison is provided in the last section similar graphical interface based on the Linux desktop is available as the interface to the cluster","title":"Similarities"},{"location":"guides/graphical/#differences","text":"sirius3 is not a single isolated machine - it is one \"compute\" node on the computer cluster of some 400+ machines there is a scheduler called slurm, which makes sharing of the resources fairer between users - making it impossible to monopolize resources sirius3 is intended to be used for graphical applications and for interactive code development. As such, code for more intense computations may be developed and tested on small scale on this machine. However, to perform a more intense computation, you should prepare a \"batch\" script (see cluster guide and video walktroughs for OpenOnDemand job templates). To ensure that there are enough resources for many users at once, each user on sirius3 is limited to 2 cores and 4Gb of memory for a maximum of 24 hours . you can submit sbatch jobs, from the sirius3 desktop, to the wider Amarel cluster when off campus, you need to connect to it using VPN, otherwise it will not connect","title":"Differences"},{"location":"guides/graphical/#instructions-to-connect-to-openondemand-sirius3-desktop","text":"Open your browser at https://ondemand.hpc.rutgers.edu/ (make sure you are connected through VPN) Go to Interactive Apps - Sirius3 Deskop Choose number of hours that your session will be active (max 24) and number of cores (max 2) and click Launch In maximum 30 seconds, a blue button \"Launch VNC in New Tab\" will appear; clicking it will open Linux Desktop on sirius3 To get the Linux terminal (shell), click on the little terminal icon in the top left corner of the Linux Mate desktop (the systems bar) Use module load command to load appropriate version of your software to your software path, such as module load SAS","title":"Instructions to connect to OpenOnDemand - Sirius3 Desktop"},{"location":"guides/graphical/#video-walkthroughs","text":"We have prepared some videos to illustrate the functionalities of OpenOnDemand graphical interface to the Amarel cluster: https://youtu.be/D2C-yYzQ2uQ - 3:18 mins - lauching Sirius3 desktop (NOTE: 2 cores, not 1 as in video) https://youtu.be/GPpDG1eAiKo - 4:24 mins - Amarel ondemand intro - available functionalities https://youtu.be/zNrpS5CUa-8 - 4:24 mins - launching Amarel desktop https://youtu.be/znmnbli0d5Y - 6:33 mins - using the Job Composer to keep track of slurm jobs, create new slurm jobs from templates, and create new templates.","title":"Video walkthroughs"},{"location":"guides/graphical/#fastx-users","text":"IMPORTANT NOTE : When you login through FastX on Amarel, you are on the \"login\" node. This is a shared machine between tens of users at a time. It is essential that you don't run any intensive computations on this machine (like you did on sirius). Instead, you need to either ssh to sirius3, or submit a computational job through slurm (the scheduler). TODO: rest - how to connect from login node to sirius3.","title":"FastX users"},{"location":"guides/graphical/#software-comparison-between-sirius-and-amarel","text":"Please let us know if any of this software is not available on Amarel, but is essential to your work. We will consider adjusting our priorities to your needs:","title":"Software comparison between sirius and Amarel"},{"location":"guides/graphical/#software-that-exists-on-amarel","text":"AmberTools Bcl2fastq Bcl2fastq2 Bedtools2 Bowtie2 Cufflinks Clc-workbench Eclipse FastQC IGV Ncbi-blast - blast/2.6.0 on Amarel. This version 2.6, it is called ncbi-blast, and it should be called ncbi-blast gdb Glibc intel Java MATLAB Moe openbabel python RStudio - available through OnDemand; R available on command line Rna-star - this is called STAR/2.5.2a on Amarel Samtools SAS Trimmomatic Trinityrnaseq Trinotate Tophat","title":"Software that exists on Amarel:"},{"location":"guides/graphical/#in-question-but-important-software-and-is-a-work-in-progress","text":"Goldsuite - Vlad in communication with CCDC vendor","title":"In question, but important software and is a work in progress:"},{"location":"guides/graphical/#will-install-on-request","text":"Weka Knime Coot -- crystalographic package Phenix - crystallographic packag Mgltools - autodock related Autodock - community Autodock_vina -- community Chimera -- installed in community, needs module Vmd -- community","title":"Will install on request:"},{"location":"guides/graphical/#installation-will-progress-as-time-permits","text":"Blasr Emboss HMMER Ngsplot Qiime PhyloCSF PLINK SNP2HLA RSEM","title":"Installation will progress as time permits"},{"location":"guides/graphical/#these-will-not-be-considered-for-installation-unless-there-are-objections","text":"CHANCE MACS MACS2 MEME MISO pandaseq abyss Transabyss WaveCNV","title":"These will not be considered for installation unless there are objections."},{"location":"guides/graphical/#definitely-not-installing","text":"Sybyl-x FASTX-Toolkit Llvm Spartan binutils","title":"Definitely not installing:"},{"location":"guides/scratch-datamanagement/","text":"Guidelines for managing data on scratch Table: Data management on OARC cluster Fileset Users quota Time Back up External storage options Transfer Tools scratch general No Quota 90 days no - Personal devices: Laptop, Desktop, USB drives, etc. - Online drives: Box(unlimited for Rutgers), Google Drive, One Drive, Dropbox, etc. -Public cloud storages: GCP, Azure, AWS, etc. -CLI tools: scp, rsync, rcloud -GUI tools: Filezilla, WinSCP, Open OnDemand, rclone-browser (via sirius3) home general 100GB Valid account yes project node owners varies varies yes Scratch offers unlimited temporary storage. You can processing large volumes of data in scratch, but the data cannot be older than 90 days. The files older than 90 days are automatically removed, and there is no backup. If you exceed the disk quota, your jobs would fail with a message that you cannot write anymore. So, it is a best practice to move the data from scratch as soon as possible. Internal and external storage options For long term storage, you can utilize the internal storage at OARC's cluster or any of the external storage. OARC's internal storage includes home and project file sets. If your data can be accommodated in your home directory, it is okay to keep the final results in the home. Node owners may want to use their project space. If you don't have enough space in the OARC cluster, you need to move the data to an external storage. Here we point out a few options to consider for external storage. Personal devices such as laptop, desktop, workstations, or USB drives can serve as external storage. Third part vendors offer several storage options that are either free or paid services. Box offers unlimited free storage for Rutgers community. Google offers unlimited free storage for the academic community by a service called Google Drive File Stream. One Drive and DropBox services are free to use up to a set quota. Public cloud providers such as AWS, Azure, and GCP charge a fee to move the data in and out of the cloud. Data transfer tools There are several tools and methods available to move the data that are based on command line interface (CLI) or Graphical User Interface (GUI). You can use command line tools such as scp, rsync, or rcloud to move the data from OARC cluster to any external storage. scp and rsync are suitable choices if you are moving data from OARC to your laptop or desktop that runs linux or MacOS. rcloud is an excellent choice to move your data to many storage services such as Box, DropBox, cloud, etc. GUI tools such as Filezilla, WinSCP, rclone-browser, and Open OnDemand are popular choices to move the data from variety of storage end points. Among GUI tools, FIlezilla, rclone-browser, and Open OnDemand tools should work on any operating systems (Linux, MacOS, Windows) since they interact through a web-browser. Please refer our user guide for more details about how to use specific CLI tools such as scp, rsync, and rclone. Here is a simple command line example to move your data using scp. scp your-netid@amarel.hpc.rutgers.edu: file location on oarc cluster file location on your device where \u2018your-netid\u2019 is your Rutger\u2019s netid, which is same as your account name on OARC cluster. For example, to move data from scratch space located at /scratch/your-netid/your-data-file to a home directory on your laptop using scp command, type the following in your terminal shell prompt on your laptop. scp your-netid@amarel.hpc.rutgers.edu:/scratch/your-net-id/your-data-file ~/your-data-file","title":"Guidelines for managing data on scratch"},{"location":"guides/scratch-datamanagement/#guidelines-for-managing-data-on-scratch","text":"","title":"Guidelines for managing data on scratch"},{"location":"guides/scratch-datamanagement/#table-data-management-on-oarc-cluster","text":"Fileset Users quota Time Back up External storage options Transfer Tools scratch general No Quota 90 days no - Personal devices: Laptop, Desktop, USB drives, etc. - Online drives: Box(unlimited for Rutgers), Google Drive, One Drive, Dropbox, etc. -Public cloud storages: GCP, Azure, AWS, etc. -CLI tools: scp, rsync, rcloud -GUI tools: Filezilla, WinSCP, Open OnDemand, rclone-browser (via sirius3) home general 100GB Valid account yes project node owners varies varies yes Scratch offers unlimited temporary storage. You can processing large volumes of data in scratch, but the data cannot be older than 90 days. The files older than 90 days are automatically removed, and there is no backup. If you exceed the disk quota, your jobs would fail with a message that you cannot write anymore. So, it is a best practice to move the data from scratch as soon as possible.","title":"Table: Data management on OARC cluster"},{"location":"guides/scratch-datamanagement/#internal-and-external-storage-options","text":"For long term storage, you can utilize the internal storage at OARC's cluster or any of the external storage. OARC's internal storage includes home and project file sets. If your data can be accommodated in your home directory, it is okay to keep the final results in the home. Node owners may want to use their project space. If you don't have enough space in the OARC cluster, you need to move the data to an external storage. Here we point out a few options to consider for external storage. Personal devices such as laptop, desktop, workstations, or USB drives can serve as external storage. Third part vendors offer several storage options that are either free or paid services. Box offers unlimited free storage for Rutgers community. Google offers unlimited free storage for the academic community by a service called Google Drive File Stream. One Drive and DropBox services are free to use up to a set quota. Public cloud providers such as AWS, Azure, and GCP charge a fee to move the data in and out of the cloud.","title":"Internal and external storage options"},{"location":"guides/scratch-datamanagement/#data-transfer-tools","text":"There are several tools and methods available to move the data that are based on command line interface (CLI) or Graphical User Interface (GUI). You can use command line tools such as scp, rsync, or rcloud to move the data from OARC cluster to any external storage. scp and rsync are suitable choices if you are moving data from OARC to your laptop or desktop that runs linux or MacOS. rcloud is an excellent choice to move your data to many storage services such as Box, DropBox, cloud, etc. GUI tools such as Filezilla, WinSCP, rclone-browser, and Open OnDemand are popular choices to move the data from variety of storage end points. Among GUI tools, FIlezilla, rclone-browser, and Open OnDemand tools should work on any operating systems (Linux, MacOS, Windows) since they interact through a web-browser. Please refer our user guide for more details about how to use specific CLI tools such as scp, rsync, and rclone. Here is a simple command line example to move your data using scp. scp your-netid@amarel.hpc.rutgers.edu: file location on oarc cluster file location on your device where \u2018your-netid\u2019 is your Rutger\u2019s netid, which is same as your account name on OARC cluster. For example, to move data from scratch space located at /scratch/your-netid/your-data-file to a home directory on your laptop using scp command, type the following in your terminal shell prompt on your laptop. scp your-netid@amarel.hpc.rutgers.edu:/scratch/your-net-id/your-data-file ~/your-data-file","title":"Data transfer tools"},{"location":"howtos/fastx/","text":"Connecting to the remote Linux cluster makes running graphical programs remotely tricky, because the graphical program runs on the remote computer, yet, it must be displayed on the local machine such as laptop. In general, command line interaction with the cluster is normally preferred and even more efficient than a GUI. However, there are times when running a graphical program cannot be avoided, for example, running a debugger for a code running on the cluster. There is a convenient way to run a graphical program remotely, for example, using FastX. Here is the procedure: Go to https://amarel.hpc.rutgers.edu:3443 and log in (you must be either on campus, or connected through VPN) Click on Launch session Click on xterm Run this command that will ask the resource scheduler to put you on a compute node, rather than a login node (where you shouldn't be running intensive computations) srun -p main -N 1 -c 2 --mem=4Gb -t 1:00:00 --pty /bin/bash This command puts you on main partition, asks for 2 cores on 1 node, asks for 4 Gb or memory and time of 1 hour and runs interactive shell. After executing this, you should notice that the name of the node changed from amarel to slepner036 or some such. You can request whatever resources you deem necessary for your work, but keep in mind that bigger requests are typically placed further down in the queue. Start the program with graphical interface from the terminal window. Note: to disconnect from and then return to your active FastX session on Amarel, you must connect using the same login node you were using previously (i.e., amarel1 or amarel2). So, consistently connecting via amarel1.hpc.rutgers.edu:3443 would help ensure that you don't accidently \"lose\" a FastX session. Here is the video walking through these steps: Troubleshooting If you get an error on xfce desktop, and you installed anaconda, it may be interfering with your fastX: close all FastX sessions delete .fastx_server directory (it will be hidden, do ls -la ) comment out path to anaconda in your .bash_profile and/or .bashrc files (also hidden)","title":"FastX"},{"location":"howtos/fastx/#troubleshooting","text":"If you get an error on xfce desktop, and you installed anaconda, it may be interfering with your fastX: close all FastX sessions delete .fastx_server directory (it will be hidden, do ls -la ) comment out path to anaconda in your .bash_profile and/or .bashrc files (also hidden)","title":"Troubleshooting"},{"location":"howtos/group_permissions/","text":"Suppose I have a directory that I want to share with a collaborator, /home/kp807/myproject , which I want to make readable for the collaborator and a subdirectory /home/kp807/myproject/code where I will also give the collaborator the permission to write. I do not want the collaborator to see everything in my home directory /home/kp807 . Using granular FACL controls FACL (File Access Control List) has two important commands, setfacl and getfacl . getfacl works just like ls . Example: [kp807@amarel1 ~]$ getfacl /home/kp807 getfacl: Removing leading '/' from absolute path names # file: home/kp807 # owner: kp807 # group: kp807 user::rwx user:dm1084:--x group::--- other::--- So to setup the correct permissions, we'd need to do: setfacl -m u:kholodvl:x /home/kp807/ #modify permissions (-m) for user kholodvl and make /home/kp807/ executable for his commands setfacl -m u:kholodvl:rx /home/kp807/myproject setfacl -m u:kholodvl:rwx /home/kp807/myproject Important Note : Check defaults with which your files and directories are created. Using group membership Use commands chmod (or chown as appropriate) to modify permissions and ownership, respectively, of directories or files: chmod g+x /home/kp807/ #make the directory executable so other person's command can tranverse the file tree to myproject chmod g+rx /home/kp807/myproject/ #make the directory readable and executable for the group I belong to chmod g+rwx /home/kp807/myproject/code #make the directory readable, writable and executable for the group I belong to [kp807@amarel1 ~]$ mkdir myproject [kp807@amarel1 ~]$ mkdir myproject/code [kp807@amarel1 ~]$ setfacl -m u:kholodvl:x /home/kp807/ #modify permissions (-m) to user kholodvl and make /home/kp807/ executable for his commands [kp807@amarel1 ~]$ setfacl -m u:kholodvl:rx /home/kp807/myproject [kp807@amarel1 ~]$ setfacl -m u:kholodvl:rwx /home/kp807/myproject [kp807@amarel1 ~]$ touch /home/kp807/myproject/readme.txt [kp807@amarel1 ~]$ touch /home/kp807/myproject/code/writeme.txt [kp807@amarel1 ~]$ ls /home/kp807/myproject/code/ rm -rf /home/kp807/myproject mkdir /home/kp807/myproject mkdir /home/kp807/myproject/code ls -la /home/kp807/myproject getfacl /home/kp807/myproject ls -la /home/kp807/myproject/code getfacl /home/kp807/myproject/code","title":"Group permissions"},{"location":"howtos/group_permissions/#using-granular-facl-controls","text":"FACL (File Access Control List) has two important commands, setfacl and getfacl . getfacl works just like ls . Example: [kp807@amarel1 ~]$ getfacl /home/kp807 getfacl: Removing leading '/' from absolute path names # file: home/kp807 # owner: kp807 # group: kp807 user::rwx user:dm1084:--x group::--- other::--- So to setup the correct permissions, we'd need to do: setfacl -m u:kholodvl:x /home/kp807/ #modify permissions (-m) for user kholodvl and make /home/kp807/ executable for his commands setfacl -m u:kholodvl:rx /home/kp807/myproject setfacl -m u:kholodvl:rwx /home/kp807/myproject Important Note : Check defaults with which your files and directories are created.","title":"Using granular FACL controls"},{"location":"howtos/group_permissions/#using-group-membership","text":"Use commands chmod (or chown as appropriate) to modify permissions and ownership, respectively, of directories or files: chmod g+x /home/kp807/ #make the directory executable so other person's command can tranverse the file tree to myproject chmod g+rx /home/kp807/myproject/ #make the directory readable and executable for the group I belong to chmod g+rwx /home/kp807/myproject/code #make the directory readable, writable and executable for the group I belong to [kp807@amarel1 ~]$ mkdir myproject [kp807@amarel1 ~]$ mkdir myproject/code [kp807@amarel1 ~]$ setfacl -m u:kholodvl:x /home/kp807/ #modify permissions (-m) to user kholodvl and make /home/kp807/ executable for his commands [kp807@amarel1 ~]$ setfacl -m u:kholodvl:rx /home/kp807/myproject [kp807@amarel1 ~]$ setfacl -m u:kholodvl:rwx /home/kp807/myproject [kp807@amarel1 ~]$ touch /home/kp807/myproject/readme.txt [kp807@amarel1 ~]$ touch /home/kp807/myproject/code/writeme.txt [kp807@amarel1 ~]$ ls /home/kp807/myproject/code/ rm -rf /home/kp807/myproject mkdir /home/kp807/myproject mkdir /home/kp807/myproject/code ls -la /home/kp807/myproject getfacl /home/kp807/myproject ls -la /home/kp807/myproject/code getfacl /home/kp807/myproject/code","title":"Using group membership"},{"location":"howtos/jupyter/","text":"Tunneling This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. run jupyter notebook as a slurm job find out on which compute node jupyter notebook ended up in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here) # This procedure works not only with jupyter notebook, but any web app running on a compute node #command to run jupyter notebook srun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889 # to start jupyter succintly sbatch start_jupyter.sh #start jupyter job on port 8889 squeue -u kp807 #find out on which node is the job running - say it's slepner009 # in another terminal establish port forwarding - from local port 9999 to port 8889 on the specific node (slepner009 here) ssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu # modify slepner009, the ports, the netID Video expaining the steps above: How to launch Jupyter notebook on the cluster There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: module use /projects/community/modulefiles module load py-data-science-stack Copy this into a script file like start_jupyter.sh #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=jupyter # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=4000 # Real memory (RAM) required (MB) #SBATCH --time=01:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) export XDG_RUNTIME_DIR=$HOME/tmp ## needed for jupyter writting temporary files module use /projects/community/modulefiles module load py-data-science-stack # loads anaconda source activate pytorch-0.4.0 #run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means any ip as interface srun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889 Then run sbatch start_jupyter.sh on amarel login node. Now you need to find which node the jupyter notebook is running at. Do squeue -u your net id to see the slurm jobs you are running. You should then do the port tunneling described in the previous section and open local browser at that port. Youtube video that explains this: NOTE : to find which node your jupyter notebook landed on, you can look in the slurm job output file (or error file). For example, if the error file is slurm.nm319_84010016.err , then you landed on node nm319 and your jupyter notebook is slurm job number 84010016. NOTE : You will see the url of the notebook which will have a line like this: Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://0.0.0.0:8889/?token=e68b5ad15574911b812acdfe3d9441ff217282685c8f7e29 the long hash is the token. If you by any chance get logged out of the notebook and you get asked for a password, you can copy-paste your token in the jupyter login screen. See this: https://ask.oarc.rutgers.edu/question/69/my-jupyter-notebook-asks-me-for-a-password/ NOTE : Recently, our systems have been federated. If you end up on a node like haln or nm3 , you will need to tunnel through amareln , not amarel login node.","title":"Jupyter"},{"location":"howtos/jupyter/#tunneling","text":"This is a technique for connecting from your local machine to a remote web server, such as Jupyter notebook running on the compute node of a cluster. run jupyter notebook as a slurm job find out on which compute node jupyter notebook ended up in another terminal establish \"port forwarding\" - from local port 9999 to port 8889 on the specific node (slepner009 here) # This procedure works not only with jupyter notebook, but any web app running on a compute node #command to run jupyter notebook srun -p main -c 1 -t 10:00 --error=slurm.%N_%j.err --output=slurm.%N_%j.out jupyter notebook --no-browser --ip=0.0.0.0 --port=8889 # to start jupyter succintly sbatch start_jupyter.sh #start jupyter job on port 8889 squeue -u kp807 #find out on which node is the job running - say it's slepner009 # in another terminal establish port forwarding - from local port 9999 to port 8889 on the specific node (slepner009 here) ssh -L 9999:slepner009:8889 kp807@amarel.hpc.rutgers.edu # modify slepner009, the ports, the netID Video expaining the steps above:","title":"Tunneling"},{"location":"howtos/jupyter/#how-to-launch-jupyter-notebook-on-the-cluster","text":"There is an already available installation of Anaconda 5.1.0 with Python 3.6.4 on the cluster, which contains both Jupyter notebook and Jupyter lab. To load the necessary module execute these commands: module use /projects/community/modulefiles module load py-data-science-stack Copy this into a script file like start_jupyter.sh #!/bin/bash #SBATCH --partition=main # Partition (job queue) #SBATCH --job-name=jupyter # Assign an short name to your job #SBATCH --nodes=1 # Number of nodes you require #SBATCH --ntasks=1 # Total # of tasks across all nodes #SBATCH --cpus-per-task=1 # Cores per task ( 1 if multithread tasks) #SBATCH --mem=4000 # Real memory (RAM) required (MB) #SBATCH --time=01:00:00 # Total run time limit (HH:MM:SS) #SBATCH --output=slurm.%N.%j.out # STDOUT output file #SBATCH --error=slurm.%N.%j.err # STDERR output file (optional) export XDG_RUNTIME_DIR=$HOME/tmp ## needed for jupyter writting temporary files module use /projects/community/modulefiles module load py-data-science-stack # loads anaconda source activate pytorch-0.4.0 #run system-installed jupyter notebook on port 8889. ip 0.0.0.0 means any ip as interface srun jupyter notebook --no-browser --ip=0.0.0.0 --port=8889 Then run sbatch start_jupyter.sh on amarel login node. Now you need to find which node the jupyter notebook is running at. Do squeue -u your net id to see the slurm jobs you are running. You should then do the port tunneling described in the previous section and open local browser at that port. Youtube video that explains this: NOTE : to find which node your jupyter notebook landed on, you can look in the slurm job output file (or error file). For example, if the error file is slurm.nm319_84010016.err , then you landed on node nm319 and your jupyter notebook is slurm job number 84010016. NOTE : You will see the url of the notebook which will have a line like this: Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://0.0.0.0:8889/?token=e68b5ad15574911b812acdfe3d9441ff217282685c8f7e29 the long hash is the token. If you by any chance get logged out of the notebook and you get asked for a password, you can copy-paste your token in the jupyter login screen. See this: https://ask.oarc.rutgers.edu/question/69/my-jupyter-notebook-asks-me-for-a-password/ NOTE : Recently, our systems have been federated. If you end up on a node like haln or nm3 , you will need to tunnel through amareln , not amarel login node.","title":"How to launch Jupyter notebook on the cluster"},{"location":"howtos/spark/","text":"Using Spark Apache Spark is a good way to process tabular or unstructured data at scale. Spark runs usually with one of the several schedulers: - Spark Standalone - YARN - Mesos - Kubernetes We will describe how to spin up a virtual Spark cluster inside the cluster than runs slurm. Easy - use Spark Standalone local (single node) Allocate resources, using Slurm, that Spark cluster will use for its cluster. This means allocate cpu cores, memory, and maxtime for your job. Number of nodes will be 1 if you use Standalone local. load Spark module (see environment modules ), which will put environment variables that Spark uses in your path Run spark-submit or spark-shell with --master local[4] (don't use local[*] , as it might confuse the Spark Standalone if it derives its information from node spec rather than what you have been allocated) In code: srun -N 1 -n 1 -c 4 -t 01:00:00 --mem=10Gb --pty bash # to ask for single node and 4 cores for 1 hour. This has the effect of placing you on a compute node module use /projects/community/modulefiles module load spark/2.3.0-kp807 # finds spark installed in community software spark-shell --master local[4] # starts spark-shell. Use the same number of cores (4 in this case) that you asked in srun To look at Spark GUI, see how to do tunnelling . Advanced - spin up multinode cluster TODO Submit a batch script Check out the slurm_examples directory in our github repo for an example. Start Spark notebook srun -N 1 -n 1 -c 4 -t 01:00:00 --mem=10Gb --pty bash # to ask for single node and 4 cores for 1 hour. This has the effect of placing you on a compute node module use /projects/community/modulefiles module load spark-notebook/2.2.0-kp807 spark-notebook # starts spark notebook on port 9000, now you have to tunnel to reach it See spark notebook demo for more information and options. To tunnel, in another terminal establish tunnelling with this command (supposing your compute node is slepner022, and your netid is kp807: ssh -L 9000:slepner022:9000 kp807@amarel.hpc.rutgers.edu Then open your browser on laptop at localhost:9000 to use the notebook.","title":"Using Spark"},{"location":"howtos/spark/#using-spark","text":"Apache Spark is a good way to process tabular or unstructured data at scale. Spark runs usually with one of the several schedulers: - Spark Standalone - YARN - Mesos - Kubernetes We will describe how to spin up a virtual Spark cluster inside the cluster than runs slurm.","title":"Using Spark"},{"location":"howtos/spark/#easy-use-spark-standalone-local-single-node","text":"Allocate resources, using Slurm, that Spark cluster will use for its cluster. This means allocate cpu cores, memory, and maxtime for your job. Number of nodes will be 1 if you use Standalone local. load Spark module (see environment modules ), which will put environment variables that Spark uses in your path Run spark-submit or spark-shell with --master local[4] (don't use local[*] , as it might confuse the Spark Standalone if it derives its information from node spec rather than what you have been allocated) In code: srun -N 1 -n 1 -c 4 -t 01:00:00 --mem=10Gb --pty bash # to ask for single node and 4 cores for 1 hour. This has the effect of placing you on a compute node module use /projects/community/modulefiles module load spark/2.3.0-kp807 # finds spark installed in community software spark-shell --master local[4] # starts spark-shell. Use the same number of cores (4 in this case) that you asked in srun To look at Spark GUI, see how to do tunnelling .","title":"Easy - use Spark Standalone local (single node)"},{"location":"howtos/spark/#advanced-spin-up-multinode-cluster","text":"TODO","title":"Advanced - spin up multinode cluster"},{"location":"howtos/spark/#submit-a-batch-script","text":"Check out the slurm_examples directory in our github repo for an example.","title":"Submit a batch script"},{"location":"howtos/spark/#start-spark-notebook","text":"srun -N 1 -n 1 -c 4 -t 01:00:00 --mem=10Gb --pty bash # to ask for single node and 4 cores for 1 hour. This has the effect of placing you on a compute node module use /projects/community/modulefiles module load spark-notebook/2.2.0-kp807 spark-notebook # starts spark notebook on port 9000, now you have to tunnel to reach it See spark notebook demo for more information and options. To tunnel, in another terminal establish tunnelling with this command (supposing your compute node is slepner022, and your netid is kp807: ssh -L 9000:slepner022:9000 kp807@amarel.hpc.rutgers.edu Then open your browser on laptop at localhost:9000 to use the notebook.","title":"Start Spark notebook"},{"location":"workshops/Labs/","text":"LAB 1: Visualization using IGV IGV --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 module load samtools samtools sort accepted_hits.bam -o accepted_hits.sorted.bam ##this step takes about 10 minutes to complete samtools index accepted_hits.sorted.bam ## it takes about 30 seconds The resulting files: accepted_hits.sorted.bam accepted_hits.sorted.bam.bai are the files to be uploaded to IGV You need to repeat these steps for every sample Shortcut Lab 1 We have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at: /projects/oarc/Genomics_Workshop/Bam_for_IGV/ . Make a soft link (see the following command), or copy them into your scratch folder, then we use IGV to analyze them. cd /scratch/$USER/Genomics_Workshop/ ln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV Bam_for_IGV ## this step was done when you ran lab_PartII.sh #start IGV module load java /projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh Practice and get familiar with: How to Load genome and data track How to navigate How and what to visualize: Examine coverage Low mapping quality Mis-alignment Translocation Novel genes/transcript Alternative splicing Inversion Look for SNPs CNV, ChipSeq, RNASeq, WGS alignmentSNP More detailed explanation here The following is a sample snap shot of the above two files loaded to IGV. CLOSE your interactive session on a node when done with IGV by typing exit LAB 2: Data processing and expression analysis using edgeR All the htseq-count output files should be present in one folder. Here we created the folder read_counts. read_counts/dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt Go to this folder containing all above six counts output files cd /scratch/$USER/Genomics_Workshop/read_counts We need to combine all counts into one file, which will be imported into R for further analysis. The following shows how this can be done with bash commands on linux ( you may do it in excel too) paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt merged_counts.txt ## merge files by columns cut -f1,2,4,6,8,10,13 merged_counts.txt merged_counts_clean.txt ## extract the relevant columns head -n -5 merged_counts_clean.txt merged_counts_clean2.txt ##remove the last 5 line stats summary We also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file. You may construct in excel. The file should contain the following info label sample group treatment 1 dex09 dex dex_treated 2 dex13 dex dex_treated 3 dex17 dex dex_treated 4 untreated08 control untreated 5 untreated12 control untreated 6 untreated16 control untreated This file has been prepared for you. You will need to copy it into your folder, see later Lab2.2: Lab2.2: Pre-processing the data in R cd /scratch/$USER/Genomics_Workshop/ mkdir DE_analysis ##set working directory to run differential expression analysis cd DE_analysis ## Copy the needed files here cp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD ##This file denotes the experimental group cp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD ##This file contains read counts on genes for all samples cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD ##annotation file needed to calculate exonic gene length -- needed for FPKM calculation HOMEWORK CATCH-UP From your homework assignment you should have the following packages to be installed already :). If you didn't, install them now. ##Open a terminal for amarel2 login node. ssh -X amarel2.hpc.rutgers.edu ##On the login node start R module load intel/17.0.4 module load R-Project/3.4.1 R ##then in the R workspace do the following: source( https://bioconductor.org/biocLite.R ) biocLite( MKmisc ) Would you like to use a personal library instead? (y/n) y Would you like to create a personal library ~/R/x86_64-pc-linux-gnu-library/3.4 to install packages into? (y/n) y ##Wait till it finishes. biocLite( Heatplus ) biocLite( affycoretools ) biocLite( flashClust ) biocLite( affy ) biocLite( GenomicFeatures ) quit() ###quit R, no save Save workspace image? [y/n/c]: n Starting the Job Now,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i Go to your working directory cd /scratch/$USER/Genomics_Workshop/DE_analysis/ Then, start R module load intel/17.0.4 module load R-Project/3.4.1 getwd() Check which directory you are in You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory : setwd(\"/scratch/ your_netID /Genomics_Workshop/DE_analysis/\") . Set the working directory Now load up the libraries needed for the analysis library(edgeR) library(MKmisc) library(affy) library(flashClust) library(affycoretools) library(Heatplus) library(GenomicFeatures) raw.data - read.delim( merged_counts_clean2.txt , header=F, row.names=1) ##import count data to R head(raw.data) ## check the beginning portion of the imported file, now an object dim(raw.data) ##check the dimention of this object class(raw.data) ## check what class of this object apply(raw.data, 2, summary) ## check the range of counts per sample range(rowSums(raw.data)) ## check the range of counts per gene colnames(raw.data) - c( dex09 , dex13 , dex17 , untreated08 , untreated12 , untreated16 ) ##add column header raw.data2 - raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples dim(raw.data2) cpm.values - cpm(raw.data2) #calculate counts per million mapped reads without any other normalization above1cpm - apply(cpm.values, 1, function(x) sum(x =1)) ##How many samples/genes had at least 1 cpm counts.use - raw.data2[above1cpm = 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group) dim(counts.use) colSums(counts.use) / colSums(raw.data) ##the % of total counts kept after filtering nrow(counts.use) / nrow(raw.data) ##the % of genes kept after filtering targets - readTargets() ##import targets file that contains experiment group info. targets$GpF - factor(targets$group) ##change character to factor targets$GpN - as.numeric(targets$GpF) ##change factor to numeric (optional) ls() #see that objects have been loaded save.image( RNASeqDemo.RData ) ##save the work workspace savehistory( RNASeqDemo.Rhistory ) ##save command history Lab 2.3: To calculate expression values as fpkm First, compute the gene length as described in Lab6--partI library(GenomicFeatures) ##may skip, because we already loaded at start gtfdb - makeTxDbFromGFF( Homo_sapiens.GRCh38.78.gtf ,format= gtf ) exons.list.per.gene - exonsBy(gtfdb,by= gene ) exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength') Hg20_geneLength2 - data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain the same number genes in the same order as in the filtered read counts file colnames(Hg20_geneLength2) - paste('geneLength') ## to change column name, make it neat fpkm.data - cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm min.count - apply(fpkm.data,1,min) write.csv(fpkm.data,file= fpkm_values.csv ) #### To output FPKM data Lab 2.4: Analysis QC ---or sample diagnosis ## density distribution plotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2) legend( topright , legend=levels(targets$GpF),fill=1:4) Change data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization Clustering hc.raw - flashClust(dist(t(log2(raw.data2+0.1))),method= average ) plot(hc.raw,hang = -1, main = RNASeqDemo, raw values , sub = , xlab = ,cex=0.9, labels=targets$sample) ##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization #####PCA####### plotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main= PCA on FPKM ) ###do it after edgeR analysis, otherwise some values not existing yet### plotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main= PCA on logCPM ) #####heatmap#### test - topTags(eR.dex_Ctl,n=Inf,sort.by= PValue )$table ####sort gene list according to P values test2 - test[1:500,] ###Take the top 500 significant genes logCPM2 - logCPM[rownames(test2),] ###Extract logCPM values of these 500 genes meanFC - logCPM2 - rowMeans(logCPM2) color.meanFC - heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c( blue , white , red ))(128)) heatmap_2(meanFC, col=color.meanFC, legend=3, scale= none ) #####Volcano plot#### with(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main= Volcano plot , xlim=c(-2.5,2),ylim=c(0,25))) with(subset(eR.dex_Ctl.detailed,FDR 0.05), points(logFC, -log10(PValue), pch=20, col= red )) with(subset(eR.dex_Ctl.detailed, abs(logFC) 1), points(logFC, -log10(PValue), pch=20, col= orange )) with(subset(eR.dex_Ctl.detailed, FDR 0.05 abs(logFC) 1), points(logFC, -log10(PValue), pch=20, col= green )) Lab 2.5: Differential expression analysis with edgeR Create the design matrix groups - factor(targets$group) design - model.matrix(~0+groups) colnames(design) - levels(groups) rownames(design) - targets$sample design control dex dex09 0 1 dex13 0 1 dex17 0 1 untreated08 1 0 untreated12 1 0 untreated16 1 0 attr(, assign ) [1] 1 1 attr(, contrasts ) attr(, contrasts )$groups [1] contr.treatment Create contrast matrix cont.matrix - makeContrasts(dex_Ctl= dex - control, levels=design) Create DGEList object d - DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF) class(d) names(d) ## the names of the items in the list d$counts[1:5,] ## The counts are stored in the $counts: d$samples ## The group info and library sizes stored in $samples d - calcNormFactors(d) ## an additional normalization factor using a TMM method d$samples ## now the norm.factors are no longer 1 ###this can be useful when diagnose problem)### apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100 ##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. logCPM - cpm(d$counts, log = TRUE) ## modified logCPM values in edgeR, can be used for clustering, heatmap Now,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among replicates. d - estimateGLMCommonDisp(d, design, verbose=TRUE) Disp = 0.06037 , BCV = 0.2457 d - estimateGLMTrendedDisp(d, design) d - estimateGLMTagwiseDisp(d, design) plotBCV(d) ## the relationship between the overall abundance and the tagwise dispersion estimates fit.edgeR - glmFit(d, design) ## Estimate model coefficients from count data and design matrix names(fit.edgeR) Specify contrasts of interest, do empirical Bayes \"shrinkage\" of variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models) eR.dex_Ctl - glmLRT(fit.edgeR, contrast=cont.matrix[,1]) summary(decideTestsDGE(eR.dex_Ctl)) [,1] ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS) eR.dex_Ctl.detailed - topTags(eR.dex_Ctl,n=Inf,sort.by= none )$table ## Get detailed output for a single contrast eR.dex_Ctl.detailed[1:5,] logFC logCPM LR PValue FDR ENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740 ENSG00000000419 0.18626588 4.674100 0.79016266 0.3740509 0.8571700 ENSG00000000457 0.04304803 3.838702 0.03317218 0.8554789 0.9905954 ENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274 ENSG00000000971 0.35401120 8.007586 1.55551391 0.2123233 0.7070242 ## Fold change is simply group A-B (if on the log scale), or A/B (if on raw scale). ## logCPM = the average log2-counts-per-million ## LR = likelihood ratio statistics ##PValue = the two-sided p-value ## FDR = adjusted p-value #### To back-translate logFC to regular FC with down-reg as -FC eR.dex_Ctl.detailed$FC - 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC) write.csv(eR.dex_Ctl.detailed,file= Demo_eR.dex_Ctl_results.csv ) ##output the file Lab 3 Running enrichment analysis using GSEA The command to start the gsea: srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i ##get onto a reserved compute node module load java java -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar Prepare files required to run GSEA For detailed file format, see here Files Format Expression data files Gene cluster text file format (.gct) Gene set files gene matrix file format(.gmx), gene matrix transposed format(.gmt) Phenotype data files Categorical class file (.cls) (defining experimental group) A set of following sample files are prepared for GSEA analysis practice, which are located at /projects/oarc/Genomics_Workshop/GSEA/ fpkm_values.ready.gct gct file (expression fpkm values) fpkm_values.ready.cls cls file (defining experimental group) Mouse_Human_NCI_Nature_November_01_2017_symbol.gmt gmt file (gene set file biological function set) In practice analysis, use online broad C2 geneset instead of the above .gmt file Results are located at /home/Net_ID/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnnnn/ To view your result: cd ~/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnnnn/ ## (same as: cd /home/Net_ID/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnn/) firefox index.html Additional gene set database downloading source: http://software.broadinstitute.org/gsea/msigdb/index.jsp http://download.baderlab.org/EM_Genesets/ http://www.go2msig.org/cgi-bin/prebuilt.cgi or build your own Lab 4 Running the GO term analysis Open this link Gene list from de-analysis of our downloaded data (selected based on FDR and FC): FDR 0.05, FC -2.5 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927 ENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B ENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904 ENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244 ENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218 ENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513 ENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498 ENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175 ENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924 ENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139 ENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812 ENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813 ENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383 ENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869 ENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626 ENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591 ENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386 ENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442 ENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490 ENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571 ENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675 ENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793 ENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437 ENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171 ENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260 ENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647 ENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265 ENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878 ENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431 ENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959 ENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979 FDR 0.05, FC -2.5 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493 ENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772 ENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675 ENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930 ENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569 ENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244 ENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738 ENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448 ENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697 ENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214 ENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194 ENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637 ENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715 ENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138 ENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218 ENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950 ENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672 ENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829 ENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741 ENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661 ENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368 ENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480 ENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072 ENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899 ENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928 ENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798 ENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614 ENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869 ENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251 ENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017 ENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907 ENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381 ENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364 ENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549 ENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718 ENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554 ENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401 ENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167 ENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390 ENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000164761 ENSG00000177283 ENSG00000268913 ENSG00000149633 ENSG00000230018 ENSG00000103196 ENSG00000012048 ENSG00000261468 ENSG00000162630 ENSG00000126860 ENSG00000197301 ENSG00000247311 ENSG00000092621 ENSG00000154734 ENSG00000046653 ENSG00000154263 ENSG00000169271 ENSG00000167641 ENSG00000079462 ENSG00000124440 ENSG00000135821 ENSG00000182580 ENSG00000099998 ENSG00000136237 ENSG00000167771 ENSG00000120162 ENSG00000099337 ENSG00000205208 ENSG00000126803 ENSG00000120129 ENSG00000172986 ENSG00000068831 ENSG00000004799 ENSG00000272341 ENSG00000123685 ENSG00000221866 ENSG00000128045 ENSG00000157150 ENSG00000101342 ENSG00000102760 ENSG00000096060 ENSG00000198624 ENSG00000128917 ENSG00000179094 ENSG00000163083 ENSG00000179300 ENSG00000173838 ENSG00000136383 ENSG00000143127 ENSG00000189221 ENSG00000163884 ENSG00000174697 ENSG00000168309 ENSG00000112936 ENSG00000152583 ENSG00000165995 ENSG00000127954 ENSG00000157514 ENSG00000250978 ENSG00000233117 ENSG00000109906 ENSG00000157152 ENSG00000179593 ENSG00000187193 ENSG00000101347 ENSG00000152779 ENSG00000211445 ENSG00000170214 Lab 5 ID mapping and conversion Learn about gene identifiers, g:profiler, Synergizer and BioMart Use the above gene list: 1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler Explore more functions, what the site can do for you 2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart 3. Do it again with your own gene list","title":"LAB 1: Visualization using IGV"},{"location":"workshops/Labs/#lab-1-visualization-using-igv","text":"IGV --- Focus on visualization, best for validation and confirmation of the analysis result, Not good for primary analysis The mapping file is in bam format, located under the folder of tophat_out, they shall be sorted and indexed using the following command cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 module load samtools samtools sort accepted_hits.bam -o accepted_hits.sorted.bam ##this step takes about 10 minutes to complete samtools index accepted_hits.sorted.bam ## it takes about 30 seconds The resulting files: accepted_hits.sorted.bam accepted_hits.sorted.bam.bai are the files to be uploaded to IGV You need to repeat these steps for every sample","title":"LAB 1: Visualization using IGV"},{"location":"workshops/Labs/#shortcut-lab-1","text":"We have prepared 4 sets of such files (dex09, dex13, untreated08 and untreated12), located at: /projects/oarc/Genomics_Workshop/Bam_for_IGV/ . Make a soft link (see the following command), or copy them into your scratch folder, then we use IGV to analyze them. cd /scratch/$USER/Genomics_Workshop/ ln -s /projects/oarc/Genomics_Workshop/Bam_for_IGV Bam_for_IGV ## this step was done when you ran lab_PartII.sh #start IGV module load java /projects/oarc/Genomics_Workshop/IGV_2.4.6/igv.sh Practice and get familiar with: How to Load genome and data track How to navigate How and what to visualize: Examine coverage Low mapping quality Mis-alignment Translocation Novel genes/transcript Alternative splicing Inversion Look for SNPs CNV, ChipSeq, RNASeq, WGS alignmentSNP More detailed explanation here The following is a sample snap shot of the above two files loaded to IGV. CLOSE your interactive session on a node when done with IGV by typing exit","title":"Shortcut Lab 1"},{"location":"workshops/Labs/#lab-2-data-processing-and-expression-analysis-using-edger","text":"All the htseq-count output files should be present in one folder. Here we created the folder read_counts. read_counts/dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt Go to this folder containing all above six counts output files cd /scratch/$USER/Genomics_Workshop/read_counts We need to combine all counts into one file, which will be imported into R for further analysis. The following shows how this can be done with bash commands on linux ( you may do it in excel too) paste dex09.txt dex13.txt dex17.txt untreated08.txt untreated12.txt untreated16.txt merged_counts.txt ## merge files by columns cut -f1,2,4,6,8,10,13 merged_counts.txt merged_counts_clean.txt ## extract the relevant columns head -n -5 merged_counts_clean.txt merged_counts_clean2.txt ##remove the last 5 line stats summary We also need to prepare a file containing group/treatment information. This file Targets.txt is a tab delimited text file. You may construct in excel. The file should contain the following info label sample group treatment 1 dex09 dex dex_treated 2 dex13 dex dex_treated 3 dex17 dex dex_treated 4 untreated08 control untreated 5 untreated12 control untreated 6 untreated16 control untreated This file has been prepared for you. You will need to copy it into your folder, see later Lab2.2:","title":"LAB 2:  Data processing and expression analysis using edgeR"},{"location":"workshops/Labs/#lab22-pre-processing-the-data-in-r","text":"cd /scratch/$USER/Genomics_Workshop/ mkdir DE_analysis ##set working directory to run differential expression analysis cd DE_analysis ## Copy the needed files here cp /projects/oarc/Genomics_Workshop/SRA_data/DE_analysis/Targets.txt $PWD ##This file denotes the experimental group cp /projects/oarc/Genomics_Workshop/SRA_data/read_counts/merged_counts_clean2.txt $PWD ##This file contains read counts on genes for all samples cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf $PWD ##annotation file needed to calculate exonic gene length -- needed for FPKM calculation","title":"Lab2.2: Pre-processing the data in R"},{"location":"workshops/Labs/#homework-catch-up","text":"From your homework assignment you should have the following packages to be installed already :). If you didn't, install them now. ##Open a terminal for amarel2 login node. ssh -X amarel2.hpc.rutgers.edu ##On the login node start R module load intel/17.0.4 module load R-Project/3.4.1 R ##then in the R workspace do the following: source( https://bioconductor.org/biocLite.R ) biocLite( MKmisc ) Would you like to use a personal library instead? (y/n) y Would you like to create a personal library ~/R/x86_64-pc-linux-gnu-library/3.4 to install packages into? (y/n) y ##Wait till it finishes. biocLite( Heatplus ) biocLite( affycoretools ) biocLite( flashClust ) biocLite( affy ) biocLite( GenomicFeatures ) quit() ###quit R, no save Save workspace image? [y/n/c]: n","title":"HOMEWORK CATCH-UP"},{"location":"workshops/Labs/#starting-the-job","text":"Now,start a new interactive job on the compute node or switch to another terminal if you still have an interactive job running srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 02:00:00 --pty /bin/bash -i Go to your working directory cd /scratch/$USER/Genomics_Workshop/DE_analysis/ Then, start R module load intel/17.0.4 module load R-Project/3.4.1 getwd() Check which directory you are in You should be in the directory where the files merged_counts_clean2.txt and \"Targets.txt\", annotation gtf file are. If not there, set your working directory : setwd(\"/scratch/ your_netID /Genomics_Workshop/DE_analysis/\") . Set the working directory Now load up the libraries needed for the analysis library(edgeR) library(MKmisc) library(affy) library(flashClust) library(affycoretools) library(Heatplus) library(GenomicFeatures) raw.data - read.delim( merged_counts_clean2.txt , header=F, row.names=1) ##import count data to R head(raw.data) ## check the beginning portion of the imported file, now an object dim(raw.data) ##check the dimention of this object class(raw.data) ## check what class of this object apply(raw.data, 2, summary) ## check the range of counts per sample range(rowSums(raw.data)) ## check the range of counts per gene colnames(raw.data) - c( dex09 , dex13 , dex17 , untreated08 , untreated12 , untreated16 ) ##add column header raw.data2 - raw.data[rowSums(raw.data) != 0 ,] ##remove genes with zero counts for all samples dim(raw.data2) cpm.values - cpm(raw.data2) #calculate counts per million mapped reads without any other normalization above1cpm - apply(cpm.values, 1, function(x) sum(x =1)) ##How many samples/genes had at least 1 cpm counts.use - raw.data2[above1cpm = 3,] ##we have three replicates in each group. If a gene can be reliably detected, it should be detected in all 3 replicates. So, a gene to be included for further analysis shall have 1 cpm in at least 3 samples. (The 3 samples are irrespective of group) dim(counts.use) colSums(counts.use) / colSums(raw.data) ##the % of total counts kept after filtering nrow(counts.use) / nrow(raw.data) ##the % of genes kept after filtering targets - readTargets() ##import targets file that contains experiment group info. targets$GpF - factor(targets$group) ##change character to factor targets$GpN - as.numeric(targets$GpF) ##change factor to numeric (optional) ls() #see that objects have been loaded save.image( RNASeqDemo.RData ) ##save the work workspace savehistory( RNASeqDemo.Rhistory ) ##save command history","title":"Starting the Job"},{"location":"workshops/Labs/#lab-23-to-calculate-expression-values-as-fpkm","text":"First, compute the gene length as described in Lab6--partI library(GenomicFeatures) ##may skip, because we already loaded at start gtfdb - makeTxDbFromGFF( Homo_sapiens.GRCh38.78.gtf ,format= gtf ) exons.list.per.gene - exonsBy(gtfdb,by= gene ) exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength') Hg20_geneLength2 - data.frame(Hg20_geneLength[rownames(counts.use),]) ## to extract the gene length file to contain the same number genes in the same order as in the filtered read counts file colnames(Hg20_geneLength2) - paste('geneLength') ## to change column name, make it neat fpkm.data - cpm(counts.use) / (Hg20_geneLength2$geneLength/1000) ## compute fpkm min.count - apply(fpkm.data,1,min) write.csv(fpkm.data,file= fpkm_values.csv ) #### To output FPKM data","title":"Lab 2.3: To calculate expression values as fpkm"},{"location":"workshops/Labs/#lab-24-analysis-qc-or-sample-diagnosis","text":"## density distribution plotDensity(log2(raw.data+0.1),col=targets$GpF,lty=1,lwd=2) legend( topright , legend=levels(targets$GpF),fill=1:4) Change data from raw.data to raw.data2, to CPM, FPKM,.. to see the effect of filtering and normalization","title":"Lab 2.4:  Analysis QC ---or sample diagnosis"},{"location":"workshops/Labs/#clustering","text":"hc.raw - flashClust(dist(t(log2(raw.data2+0.1))),method= average ) plot(hc.raw,hang = -1, main = RNASeqDemo, raw values , sub = , xlab = ,cex=0.9, labels=targets$sample) ##change data from raw.data to raw.data2, to CPM, logCPM, FPKM,.. to see the effect of filtering and normalization #####PCA####### plotPCA(fpkm.data, pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main= PCA on FPKM ) ###do it after edgeR analysis, otherwise some values not existing yet### plotPCA(logCPM), pch=16, col=targets$GpF,groupnames=levels(targets$GpF), addtext=rep(1:3,4),main= PCA on logCPM ) #####heatmap#### test - topTags(eR.dex_Ctl,n=Inf,sort.by= PValue )$table ####sort gene list according to P values test2 - test[1:500,] ###Take the top 500 significant genes logCPM2 - logCPM[rownames(test2),] ###Extract logCPM values of these 500 genes meanFC - logCPM2 - rowMeans(logCPM2) color.meanFC - heatmapCol(data = meanFC, lim = 3, col =colorRampPalette(c( blue , white , red ))(128)) heatmap_2(meanFC, col=color.meanFC, legend=3, scale= none ) #####Volcano plot#### with(eR.dex_Ctl.detailed, plot(logFC, -log10(PValue), pch=20, main= Volcano plot , xlim=c(-2.5,2),ylim=c(0,25))) with(subset(eR.dex_Ctl.detailed,FDR 0.05), points(logFC, -log10(PValue), pch=20, col= red )) with(subset(eR.dex_Ctl.detailed, abs(logFC) 1), points(logFC, -log10(PValue), pch=20, col= orange )) with(subset(eR.dex_Ctl.detailed, FDR 0.05 abs(logFC) 1), points(logFC, -log10(PValue), pch=20, col= green ))","title":"Clustering"},{"location":"workshops/Labs/#lab-25-differential-expression-analysis-with-edger","text":"Create the design matrix groups - factor(targets$group) design - model.matrix(~0+groups) colnames(design) - levels(groups) rownames(design) - targets$sample design control dex dex09 0 1 dex13 0 1 dex17 0 1 untreated08 1 0 untreated12 1 0 untreated16 1 0 attr(, assign ) [1] 1 1 attr(, contrasts ) attr(, contrasts )$groups [1] contr.treatment","title":"Lab 2.5: Differential expression analysis  with edgeR"},{"location":"workshops/Labs/#create-contrast-matrix","text":"cont.matrix - makeContrasts(dex_Ctl= dex - control, levels=design)","title":"Create contrast matrix"},{"location":"workshops/Labs/#create-dgelist-object","text":"d - DGEList(counts=counts.use, lib.size=colSums(counts.use), group=targets$GpF) class(d) names(d) ## the names of the items in the list d$counts[1:5,] ## The counts are stored in the $counts: d$samples ## The group info and library sizes stored in $samples d - calcNormFactors(d) ## an additional normalization factor using a TMM method d$samples ## now the norm.factors are no longer 1 ###this can be useful when diagnose problem)### apply(d$counts,2,function(x) sum(sort(x/sum(x),decreasing=T)[1:20])) * 100 ##the proportions of total counts for the top 20 genes in each sample, 10-20% is OK. logCPM - cpm(d$counts, log = TRUE) ## modified logCPM values in edgeR, can be used for clustering, heatmap Now,the DE test! The term \"estimating dispersions\" in edgeR describes a method to account for the variance among replicates. d - estimateGLMCommonDisp(d, design, verbose=TRUE) Disp = 0.06037 , BCV = 0.2457 d - estimateGLMTrendedDisp(d, design) d - estimateGLMTagwiseDisp(d, design) plotBCV(d) ## the relationship between the overall abundance and the tagwise dispersion estimates fit.edgeR - glmFit(d, design) ## Estimate model coefficients from count data and design matrix names(fit.edgeR) Specify contrasts of interest, do empirical Bayes \"shrinkage\" of variances and calculate test statistics. Both of these are performed with same function glmLRT in edgeR (Genewise Negative Binomial Generalized Linear Models) eR.dex_Ctl - glmLRT(fit.edgeR, contrast=cont.matrix[,1]) summary(decideTestsDGE(eR.dex_Ctl)) [,1] ## Correct for multiple tests and extract relvant data (1 means sig up, -1 means sig down, and 0 means NS) eR.dex_Ctl.detailed - topTags(eR.dex_Ctl,n=Inf,sort.by= none )$table ## Get detailed output for a single contrast eR.dex_Ctl.detailed[1:5,] logFC logCPM LR PValue FDR ENSG00000000003 -0.36354885 5.227841 2.37143785 0.1235732 0.5501740 ENSG00000000419 0.18626588 4.674100 0.79016266 0.3740509 0.8571700 ENSG00000000457 0.04304803 3.838702 0.03317218 0.8554789 0.9905954 ENSG00000000460 -0.05998067 1.609803 0.02021703 0.8869326 0.9916274 ENSG00000000971 0.35401120 8.007586 1.55551391 0.2123233 0.7070242 ## Fold change is simply group A-B (if on the log scale), or A/B (if on raw scale). ## logCPM = the average log2-counts-per-million ## LR = likelihood ratio statistics ##PValue = the two-sided p-value ## FDR = adjusted p-value #### To back-translate logFC to regular FC with down-reg as -FC eR.dex_Ctl.detailed$FC - 2^abs(eR.dex_Ctl.detailed$logFC) * sign(eR.dex_Ctl.detailed$logFC) write.csv(eR.dex_Ctl.detailed,file= Demo_eR.dex_Ctl_results.csv ) ##output the file","title":"Create DGEList object"},{"location":"workshops/Labs/#lab-3-running-enrichment-analysis-using-gsea","text":"The command to start the gsea: srun --x11 -p main --reservation=genomics_2 -N 1 -c 2 -n 1 -t 2:00:00 --pty /bin/bash -i ##get onto a reserved compute node module load java java -jar /scratch/$USER/Genomics_Workshop/gsea-3.0.jar Prepare files required to run GSEA For detailed file format, see here Files Format Expression data files Gene cluster text file format (.gct) Gene set files gene matrix file format(.gmx), gene matrix transposed format(.gmt) Phenotype data files Categorical class file (.cls) (defining experimental group) A set of following sample files are prepared for GSEA analysis practice, which are located at /projects/oarc/Genomics_Workshop/GSEA/ fpkm_values.ready.gct gct file (expression fpkm values) fpkm_values.ready.cls cls file (defining experimental group) Mouse_Human_NCI_Nature_November_01_2017_symbol.gmt gmt file (gene set file biological function set) In practice analysis, use online broad C2 geneset instead of the above .gmt file Results are located at /home/Net_ID/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnnnn/ To view your result: cd ~/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnnnn/ ## (same as: cd /home/Net_ID/gsea_home/output/ jun27 /my_analysis.Gsea.nnnnnnnnnnn/) firefox index.html","title":"Lab 3  Running enrichment analysis using GSEA"},{"location":"workshops/Labs/#additional-gene-set-database-downloading-source","text":"http://software.broadinstitute.org/gsea/msigdb/index.jsp http://download.baderlab.org/EM_Genesets/ http://www.go2msig.org/cgi-bin/prebuilt.cgi or build your own","title":"Additional gene set database downloading source:"},{"location":"workshops/Labs/#lab-4-running-the-go-term-analysis","text":"Open this link Gene list from de-analysis of our downloaded data (selected based on FDR and FC): FDR 0.05, FC -2.5 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000146006 ENSG00000123610 ENSG00000139220 ENSG00000235927 ENSG00000108700 ENSG00000124766 ENSG00000136478 ENSG00000099860B ENSG00000162692 ENSG00000176771 ENSG00000112218 ENSG00000153904 ENSG00000105989 ENSG00000196517 ENSG00000157510 ENSG00000243244 ENSG00000188176 ENSG00000132622 ENSG00000071282 ENSG00000169218 ENSG00000141469 ENSG00000126016 ENSG00000171385 ENSG00000163513 ENSG00000116991 ENSG00000128342 ENSG00000108960 ENSG00000187498 ENSG00000119714 ENSG00000116711 ENSG00000116962 ENSG00000148175 ENSG00000214814 ENSG00000025423 ENSG00000111859 ENSG00000108924 ENSG00000126878 ENSG00000125848 ENSG00000145675 ENSG00000180139 ENSG00000172061 ENSG00000163394 ENSG00000140511 ENSG00000245812 ENSG00000184564 ENSG00000272841 ENSG00000110756 ENSG00000158813 ENSG00000122877 ENSG00000181634 ENSG00000162616 ENSG00000068383 ENSG00000131771 ENSG00000243742 ENSG00000278621 ENSG00000221869 ENSG00000165272 ENSG00000103742 ENSG00000135678 ENSG00000213626 ENSG00000145777 ENSG00000172497 ENSG00000241399 ENSG00000149591 ENSG00000013293 ENSG00000254726 ENSG00000165507 ENSG00000131386 ENSG00000146250 ENSG00000131389 ENSG00000267669 ENSG00000164442 ENSG00000143494 ENSG00000016391 ENSG00000179862 ENSG00000261490 ENSG00000154864 ENSG00000157368 ENSG00000147119 ENSG00000072571 ENSG00000163491 ENSG00000099194 ENSG00000134121 ENSG00000156675 ENSG00000183508 ENSG00000049246 ENSG00000168621 ENSG00000171793 ENSG00000128165 ENSG00000028277 ENSG00000048540 ENSG00000174437 ENSG00000123689 ENSG00000107562 ENSG00000133142 ENSG00000163171 ENSG00000136999 ENSG00000146592 ENSG00000179820 ENSG00000172260 ENSG00000128606 ENSG00000100784 ENSG00000175471 ENSG00000161647 ENSG00000128510 ENSG00000139269 ENSG00000151726 ENSG00000137265 ENSG00000178695 ENSG00000168398 ENSG00000135362 ENSG00000162878 ENSG00000177614 ENSG00000235109 ENSG00000162998 ENSG00000198431 ENSG00000138316 ENSG00000196932 ENSG00000106617 ENSG00000137959 ENSG00000108830 ENSG00000148848 ENSG00000035664 ENSG00000131979 FDR 0.05, FC -2.5 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000168811 ENSG00000147883 ENSG00000270885 ENSG00000162493 ENSG00000177570 ENSG00000183876 ENSG00000146122 ENSG00000162772 ENSG00000117600 ENSG00000131242 ENSG00000172403 ENSG00000116675 ENSG00000160145 ENSG00000100302 ENSG00000164647 ENSG00000154930 ENSG00000134253 ENSG00000126950 ENSG00000137880 ENSG00000196569 ENSG00000130513 ENSG00000182010 ENSG00000103175 ENSG00000145244 ENSG00000089041 ENSG00000105516 ENSG00000167191 ENSG00000169738 ENSG00000168918 ENSG00000235513 ENSG00000169031 ENSG00000211448 ENSG00000070808 ENSG00000007237 ENSG00000154856 ENSG00000237697 ENSG00000134363 ENSG00000162643 ENSG00000163110 ENSG00000157214 ENSG00000278727 ENSG00000135472 ENSG00000142871 ENSG00000116194 ENSG00000106484 ENSG00000138669 ENSG00000213160 ENSG00000095637 ENSG00000225783 ENSG00000160097 ENSG00000280143 ENSG00000169715 ENSG00000276600 ENSG00000054938 ENSG00000100242 ENSG00000119138 ENSG00000013297 ENSG00000138135 ENSG00000197943 ENSG00000149218 ENSG00000126861 ENSG00000186198 ENSG00000280099 ENSG00000185950 ENSG00000106976 ENSG00000185745 ENSG00000128262 ENSG00000137672 ENSG00000172738 ENSG00000127824 ENSG00000100206 ENSG00000138829 ENSG00000129682 ENSG00000158806 ENSG00000246430 ENSG00000166741 ENSG00000134259 ENSG00000123612 ENSG00000184307 ENSG00000163661 ENSG00000122966 ENSG00000149256 ENSG00000259426 ENSG00000253368 ENSG00000112837 ENSG00000143786 ENSG00000081052 ENSG00000267480 ENSG00000102524 ENSG00000170989 ENSG00000070404 ENSG00000165072 ENSG00000132321 ENSG00000223949 ENSG00000137801 ENSG00000165899 ENSG00000133216 ENSG00000129467 ENSG00000154736 ENSG00000176928 ENSG00000100739 ENSG00000196155 ENSG00000119139 ENSG00000067798 ENSG00000143320 ENSG00000111728 ENSG00000127083 ENSG00000162614 ENSG00000183496 ENSG00000117461 ENSG00000108387 ENSG00000143869 ENSG00000227268 ENSG00000103647 ENSG00000137393 ENSG00000163251 ENSG00000092969 ENSG00000272168 ENSG00000174944 ENSG00000163017 ENSG00000223764 ENSG00000137872 ENSG00000170873 ENSG00000150907 ENSG00000088756 ENSG00000167992 ENSG00000139132 ENSG00000197381 ENSG00000166592 ENSG00000166793 ENSG00000185432 ENSG00000205364 ENSG00000107611 ENSG00000100292 ENSG00000134243 ENSG00000167549 ENSG00000213420 ENSG00000137266 ENSG00000261685 ENSG00000060718 ENSG00000110900 ENSG00000165891 ENSG00000158246 ENSG00000102554 ENSG00000258947 ENSG00000164619 ENSG00000122035 ENSG00000141401 ENSG00000101825 ENSG00000246763 ENSG00000119508 ENSG00000159167 ENSG00000102984 ENSG00000173114 ENSG00000140807 ENSG00000145390 ENSG00000064309 ENSG00000230417 ENSG00000125148 ENSG00000116285 FDR 0.05, FC -2.5 FDR 0.05, FC 2.5 FDR 0.05, FC 2.5 ENSG00000164761 ENSG00000177283 ENSG00000268913 ENSG00000149633 ENSG00000230018 ENSG00000103196 ENSG00000012048 ENSG00000261468 ENSG00000162630 ENSG00000126860 ENSG00000197301 ENSG00000247311 ENSG00000092621 ENSG00000154734 ENSG00000046653 ENSG00000154263 ENSG00000169271 ENSG00000167641 ENSG00000079462 ENSG00000124440 ENSG00000135821 ENSG00000182580 ENSG00000099998 ENSG00000136237 ENSG00000167771 ENSG00000120162 ENSG00000099337 ENSG00000205208 ENSG00000126803 ENSG00000120129 ENSG00000172986 ENSG00000068831 ENSG00000004799 ENSG00000272341 ENSG00000123685 ENSG00000221866 ENSG00000128045 ENSG00000157150 ENSG00000101342 ENSG00000102760 ENSG00000096060 ENSG00000198624 ENSG00000128917 ENSG00000179094 ENSG00000163083 ENSG00000179300 ENSG00000173838 ENSG00000136383 ENSG00000143127 ENSG00000189221 ENSG00000163884 ENSG00000174697 ENSG00000168309 ENSG00000112936 ENSG00000152583 ENSG00000165995 ENSG00000127954 ENSG00000157514 ENSG00000250978 ENSG00000233117 ENSG00000109906 ENSG00000157152 ENSG00000179593 ENSG00000187193 ENSG00000101347 ENSG00000152779 ENSG00000211445 ENSG00000170214","title":"Lab 4 Running the GO term analysis"},{"location":"workshops/Labs/#lab-5-id-mapping-and-conversion","text":"Learn about gene identifiers, g:profiler, Synergizer and BioMart Use the above gene list: 1. Convert Gene IDs to Entrez Gene, gene name: Use g:Profiler Explore more functions, what the site can do for you 2. Get gene name, GO annotation + evidence codes Use Ensembl BioMart 3. Do it again with your own gene list","title":"Lab 5 ID mapping and conversion"},{"location":"workshops/cryoem-oarc/","text":"Overview As a part of ongoing collaboration between Rutgers Office of Advanced Research Computing (OARC) and Rutgers Institute for Quantitative Biomedicine we are proud to announce that OARC clusters were chosen to be a primary computational platform to support the 6th Annual Interdisciplinary Quantitative Biology Winter Boot Camp for Single-Particle Cryo-Electron Microscopy January 7 - 11, 2019 https://iqb.rutgers.edu/bootcampwinter2019 We are proud of being at the frontier of science and helping to spread the knowledge of Rutgers scientists with the national and international scientific community. Setup Connect to the OARC cluster The boot camp participants are expected to bring their own laptops with a Chrome web browser installed. The preferred method to connect to the cluster is through remote desktop (fastX) in a web browser. Each participant will be granted an access to an individual compute node for the duration of the boot camp. The compute node will be accessible only on the Rutgers network or if you are connected via VPN . The access port is unique for each participant and should not be shared. Remote desktop via FastX : In your browser, go to https://cryoem-oarc.hpc.rutgers.edu:port Enter your Rutgers netID and Password and click on the Log in button. In the new window, click on Launch Session on the left, select XFCE desktop and then click on Launch button on the right. Ignore the warning It will open a new desktop window on a compute node At the bottom of the page find a terminal icon, a black rectangular that looks like an old fashion TV box. Click on it. In the terminal window start Scipion by typing scipion ##### Please be patient, it may take up to 30sec to start Scipion for the first time Reconnecting to the FastX remote desktop session : - Open the web browser and go to https://cryoem-oarc.hpc.rutgers.edu:port - Login into your account with your Rutgers netID and a password. - Click on the XFCE Desktop session in the My Session list. CRYO-EM Software This is a list of software installed on OARC clusters for the workshop Software Description URL Scipion An image processing framework for obtaining 3D models of macromolecular complexes using Electron Microscopy (3DEM) [link] Scipion packages Scipion integrated packages: Bsoft, cryoFF, CTFFIND, EMAN, ETHAN, Frealign, Gautomatch, gCTF, gEMpicker, IMAGIC-4D, Localrec, Motioncorr/dosefgpu, Motioncorr2, Relion, ResMap, SIMPLE, SPIDER, Xmipp, Unblur summovie [link] Chimera A program for interactive visualization and analysis of molecular structures and related data, including density maps [link] You can also check what Scipion packages are installed from a command line. Open a new terminal window and type scipion install --help Understanding your data space You have two main spaces on the OARC cluster. These are: your home directory (100Gb) - /home/netid/ your scratch directory (unlimited, no-backup) - /scratch/netid/ The difference between two is that scratch is not backed up and the read/write speed is higher for the scratch. For the purpose of our workshop all the data and computational output will be held in /scratch/netid/ScipionUserData . In the linux environment the names of files/directories are case sensative. Make sure that you properly type upper and lower case letters in the path when needed. When you start Scipion for the first time you may see some warning messages about security of python cashe directory. python /gpfs/home/projects/community/scipion/openmpi3/pyworkflow/apps/pw_manager.py /gpfs/home/projects/community/scipion/openmpi3/software/lib/python2.7/site-packages/setuptools-5.4.1-py2.7.egg/pkg_resources.py:1049: UserWarning: /home/db1102/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). You may ignore this message or if you want to get rid off its annoyance, in a new terminal window type the following command: chmod g-w ~/.python-eggs","title":"Overview"},{"location":"workshops/cryoem-oarc/#overview","text":"","title":"Overview"},{"location":"workshops/cryoem-oarc/#as-a-part-of-ongoing-collaboration-between-rutgers-office-of-advanced-research-computing-oarc-and-rutgers-institute-for-quantitative-biomedicine-we-are-proud-to-announce-that-oarc-clusters-were-chosen-to-be-a-primary-computational-platform-to-support-the-6th-annual-interdisciplinary-quantitative-biology-winter-boot-camp-for-single-particle-cryo-electron-microscopy-january-7-11-2019-httpsiqbrutgersedubootcampwinter2019","text":"","title":"As a part of ongoing collaboration between Rutgers Office of Advanced Research Computing (OARC) and Rutgers Institute for Quantitative Biomedicine we are proud to announce that OARC clusters were chosen to be a primary computational platform to support the 6th Annual Interdisciplinary Quantitative Biology Winter Boot Camp for  Single-Particle Cryo-Electron Microscopy January 7 - 11, 2019 https://iqb.rutgers.edu/bootcampwinter2019"},{"location":"workshops/cryoem-oarc/#we-are-proud-of-being-at-the-frontier-of-science-and-helping-to-spread-the-knowledge-of-rutgers-scientists-with-the-national-and-international-scientific-community","text":"","title":"We are proud of being at the frontier of science and helping to spread the knowledge of Rutgers scientists with the  national and international scientific community."},{"location":"workshops/cryoem-oarc/#setup","text":"","title":"Setup"},{"location":"workshops/cryoem-oarc/#connect-to-the-oarc-cluster","text":"The boot camp participants are expected to bring their own laptops with a Chrome web browser installed. The preferred method to connect to the cluster is through remote desktop (fastX) in a web browser. Each participant will be granted an access to an individual compute node for the duration of the boot camp. The compute node will be accessible only on the Rutgers network or if you are connected via VPN . The access port is unique for each participant and should not be shared. Remote desktop via FastX : In your browser, go to https://cryoem-oarc.hpc.rutgers.edu:port Enter your Rutgers netID and Password and click on the Log in button. In the new window, click on Launch Session on the left, select XFCE desktop and then click on Launch button on the right. Ignore the warning It will open a new desktop window on a compute node At the bottom of the page find a terminal icon, a black rectangular that looks like an old fashion TV box. Click on it. In the terminal window start Scipion by typing scipion ##### Please be patient, it may take up to 30sec to start Scipion for the first time Reconnecting to the FastX remote desktop session : - Open the web browser and go to https://cryoem-oarc.hpc.rutgers.edu:port - Login into your account with your Rutgers netID and a password. - Click on the XFCE Desktop session in the My Session list.","title":"Connect to the OARC cluster"},{"location":"workshops/cryoem-oarc/#cryo-em-software","text":"This is a list of software installed on OARC clusters for the workshop Software Description URL Scipion An image processing framework for obtaining 3D models of macromolecular complexes using Electron Microscopy (3DEM) [link] Scipion packages Scipion integrated packages: Bsoft, cryoFF, CTFFIND, EMAN, ETHAN, Frealign, Gautomatch, gCTF, gEMpicker, IMAGIC-4D, Localrec, Motioncorr/dosefgpu, Motioncorr2, Relion, ResMap, SIMPLE, SPIDER, Xmipp, Unblur summovie [link] Chimera A program for interactive visualization and analysis of molecular structures and related data, including density maps [link] You can also check what Scipion packages are installed from a command line. Open a new terminal window and type scipion install --help","title":"CRYO-EM Software"},{"location":"workshops/cryoem-oarc/#understanding-your-data-space","text":"You have two main spaces on the OARC cluster. These are: your home directory (100Gb) - /home/netid/ your scratch directory (unlimited, no-backup) - /scratch/netid/ The difference between two is that scratch is not backed up and the read/write speed is higher for the scratch. For the purpose of our workshop all the data and computational output will be held in /scratch/netid/ScipionUserData . In the linux environment the names of files/directories are case sensative. Make sure that you properly type upper and lower case letters in the path when needed. When you start Scipion for the first time you may see some warning messages about security of python cashe directory. python /gpfs/home/projects/community/scipion/openmpi3/pyworkflow/apps/pw_manager.py /gpfs/home/projects/community/scipion/openmpi3/software/lib/python2.7/site-packages/setuptools-5.4.1-py2.7.egg/pkg_resources.py:1049: UserWarning: /home/db1102/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). You may ignore this message or if you want to get rid off its annoyance, in a new terminal window type the following command: chmod g-w ~/.python-eggs","title":"Understanding your data space"},{"location":"workshops/genomics_1/","text":"Overview Genomic Software This is a list of software to install for the workshop Software Description URL seqtk very handy and fast for processing fastq/a files [link] sratoolkit downloading and processing data from GEO/SRA database [link] [download] htseq-count counting the reads mapped on to genomics feature [link] fastQC widely used for sequencing read QC [link] [download] RSeQC-2.6.4 An RNA-seq quality control package, multiple functions [link] trimmomatic fastq quality trim and adaptor removal [link] [download] This is a list of software already available on the cluster and the command you need to execute to load it in your environment: Software Description Access on the cluster samtools utilities for short DNA seq alignments [link] module load samtools/1.3.1 bedtools tools for a wide-range of genomics analysis [link] module load bedtools2/2.25.0 bowtie2 alignment software [link] module load bowtie2/2.2.9 tophat2 a fast splice junction mapper for RNA-Seq reads [link] module load mvapich2/2.1 boost/1.59.0 tophat2/2.1.0 R language for statistical analysis module load intel/17.0.4 R-Project/3.4.1 This is a list of other software you might find useful: Software Description URL GSEA Genome set enrichment analysis [link] IGV Interactive Genome Viewer [link] Cytoscape Network visualization softwar [link] Setup Connect to the cluster login node The preferred method to connect to the cluster is through a web browser and fastX client via FastX : in your browser, go to https://amarel.hpc.rutgers.edu:3443 or click here . (It will only work from campus or if you are connected via VPN link .) See FastX for a walkthrough that includes a tip on copying/pasting commands. Login page Click on Launch Session and Select XFCE desktop Open new terminal, copy text from your computer into clipboard window and click on Send to Remote. Put cursor into a terminal and click Edit- Paste. The text from a clipboard appears in the terminal. Hit Return to submit a job to a compute node. Notice that the jobid number has been assigned and resources have been allocated. The command prompt in the treminal will also change from a login node netid@amarel to a compute node netid@slepner or netid@hal. On rare occasions, especially if the user has a modified .bashrc file, FastX doesn't work. If it is the case, you can still use a terminal but the connection will be much slower for graphical outputs. - via a terminal : if you have a Mac or Linux, a terminal is part of your standard apps. If you have Windows, install an SSH client such as mobaXterm . Then from your terminal connect to the cluster by executing the following command: ssh -X your netid @amarel.hpc.rutgers.edu DO NOT RUN ANY COMPUTATIONAL JOBS ON THE LOGIN NODE Get resources on the compute node When you login to the cluster you are on the login node. Jobs are not allowed to be run on the loging node, intstead you need to request a resource on the compute node for your job. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. You can do so by running the following command in your terminal: srun -p main --x11 --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash -i or just run this script node_request.sh Notice that the name in your terminal will change from amarel to a node name like hal0025 or slepner086 . The following table explains the parts of this command: command part meaning srun slurm run, i.e. allocate resources and run via slurm scheduler -p main on the main partition, one of several queues on the cluster --x11 it allows the graphical output from a compute node, e.g. GUI of the program --reservation=genomics we reserved some compute nodes for this workshop to avoid waiting in the queue -N 1 ask for one compute node -c 2 ask for two cpu cores -n 1 tells slurm that the job will be run as 1 task ( for parallel MPI jobs it could be more than 1 -t 01:40:00 run this for a maximum time of 1 hour 40 minutes --pty /bin/bash -i run the terminal shell in an interactive mode Understanding your data space You have two main spaces on the Amarel cluster. These are: your home directory (100Gb) - /home/netid/ your scratch directory (500Gb)- /scratch/netid/ They differ in how they are backed up (scratch is not backed up) and by read/write speed. So we will install programs in /home/$USER/Genomics_Workshop , while the data and computational output will be held in /scratch/$USER/Genomics_Workshop . 1. Install programs and create a workspace for the workshop Each program has slightly different installation instructions. You do not need to install programs manually. Instead just run the following scirpt: /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/Labs/lab_PartI.sh It will install neccessary programs and creates folders for this workshop. For curious one, here is the content of the script ######################################################## ``` #!/bin/bash mkdir -p /home/$USER/Genomics_Workshop/ mkdir -p /scratch/$USER/Genomics_Workshop/scripts mkdir -p /scratch/$USER/Genomics_Workshop/download mkdir -p /scratch/$USER/Genomics_Workshop/untreated mkdir -p /scratch/$USER/Genomics_Workshop/dex_treated mkdir -p /scratch/$USER/Genomocs_Workshop/Reference echo \"Copying files... Please wait\" cp -r /projects/oarc/Genomics_Workshop/Programs/ /home/$USER/Genomics_Workshop/ echo '## Genomics_Workshop specific settings 07/16/2018' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH' ~/.bashrc source ~/.bashrc module load intel/17.0.2 python/2.7.12 pip install HTSeq --user wait pip install bx-python==0.7.3 --user wait pip install RSeQC --user ######################################################## br ## 2. Download data We will download human RNA-seq data with [GEO accession GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778). The samples are in NCBI's short read archive format (SRA). We will use **sratoolkit** programs to download data but first we need to configure a location where all data files will be stored. br `vdb-config` is a configuration subprogram for `sratoolkit`. We will use it to specify the directory where `sratoolkit` fetches data. You will need to type in the followingt path, but remember to replace `netID` with your own `Rutgers netid` `/scratch/your_netID/Genomics_Workshop/download`. Do not copy blindly! Remember that **sratoolkit** is not designed to handle complex downloads. All data will be placed in one folder. You will need to move downloaded files for further analysis/manipulation into different locations manually. vdb-config --interactive-mode textual ### dash-dash before interactive-mode Now in the program: Your choice 4 type new path in /scratch/your_netID/Genomics_Workshop/download Your choice Y Hit Enter and exit the program Then execute the following commands to get the data. Both `prefetch` and `fastq-dump` are part of sratools. br Downloading can be time consuming, it takes about 3 minutes per file. Thus we already downloaded files and placed them in your /scratch/$USER/Genomics_Workshop/download/sra folder. Skim through this section to understand how we did it and continue with 'fast-dump' below. br fetch the SRA data Untreated prefetch -v SRR1039508 prefetch -v SRR1039512 prefetch -v SRR1039516 DEX treated prefetch -v SRR1039509 prefetch -v SRR1039513 prefetch -v SRR1039517 Next step is to unpack sra files and convert them to more suitable *fastq* format with `fastq-dump` program br To unpack the original sequence files can be a bit tricky at first. We need to put them into different directories: br *508,512,516* into /scratch/$USER/Genomics_Workshop/download/untreated and br *509,513,517* into /scratch/$USER/Genomics_Workshop/download/dex_treated br Luckily `fastq-dump` can do processing and output results into specified folders at the same time. cd /scratch/$USER/Genomics_Workshop/download/sra fastq-dump --outdir /scratch/$USER/Genomics_Workshop/untreated --split-files SRR10395{08,12,16}.sra fastq-dump --outdir /scratch/$USER/Genomics_Workshop/dex_treated --split-files SRR10395{09,13,17}.sra It takes a while to convert sra files. To save time, files are already converted for you. Run the following command to copy files into your /scratch/ directories. sra-fastq.sh # Running bioinformatics jobs ## 3. FastQC - raw data QC FastQC performs a quality control checks on raw sequence data and produces various graphical outputs for visual analysis. cd /scratch/$USER/Genomics_Workshop/untreated module load java ## fastqc is written in java; we need to load java before using fastqc mkdir fastqc ## create a folder to store the QC output fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq FastQC produces html pages in `fastqc/SRR1039508_1(2)_fastqc.html`, with different kinds of views of data (and Phred scores). You can open this file in Firefox browser. firefox fastqc/SRR1039508_1_fastqc.html To learn more about FastQC, see this pdf file br /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/misc/FastQC_details.pdf br Close Firefox when you are done. ## 4. Trimmomatic - quality trim/adaptor removal For demonstration purpose, we will take a small subset data using `seqtk` program cd /scratch/$USER/Genomics_Workshop/untreated seqtk sample -s100 SRR1039508_1.fastq 10000 SRR1039508_1_10k.fastq seqtk sample -s100 SRR1039508_2.fastq 10000 SRR1039508_2_10k.fastq More details and examples how to use `seqtk` can be found in br /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/misc/Seqtk_Examples Now, run `trimmomatic` to trim the read quality, and remove adaptor br **NOTE:** trimmomatic command starting with `java -jar` is a one line command, move the slider to the right to see the whole line. module load java ### needed for trimmomatic java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 For your convenience we put this command into a bash script, thus you may just type run_trimmo.sh Once it started run, you should see the following: TrimmomaticPE: Started with arguments: -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 Multiple cores found: Using 2 threads Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT' ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%) TrimmomaticPE: Completed surccessfully View the output, the trim.log file, .e.g. length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained) Alternatively, you may also use `fastx_quality_stats` from *the FASTX\u2014toolkit* ( not covered in this workshop). ## 5. FastQC - Run on cleaned reads You may run FastQC again on the cleaned by trimmomatic reads and compare new results with results for raw data, step 3 above. module load java fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq ## 6. Download reference and reference indexing Indexing of Human genome may take hours. Thus we have the reference genome prepared and ready for use in br `/projects/oarc/Genomics_Workshop/Reference/ ` br For in class practice we demonstrate how to do genome indexing with E.coli genome cd /scratch/$USER/Genomics_Workshop/Reference wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz module load bowtie2 bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic For your convenience we put these commands into a bash script, thus you may just type run_bowtie2.sh ## 7. Mapping with tophat2, (STAR, HISAT2) Before we start mapping job, make sure that you WON'T start a job on a login node. tophat2 jobs are computationally intensive and may require hours to be completed. br - **NOTE: We do not expect you to complete mapping in the class. You may start it to get familiar with the procedure, but do not wait till the completion. You may however do it as a homework for a preparation for the next session ( Genomics Workshop part II).** br Also tophat2 is a multithreaded program, which means it can utilize more than one cpu-core, thus it is better to request more resources with srun command ( so far we used `srun -c 2` (two cpu-cores). For tophat2 jobs it is advisable to request more cpu-cores. For the convenience of this workshop we will use `-c 7`, however you may request more cpu-cores for your research. br Exit the current interactive session: type ```exit``` in the terminal wondow. You should see that the prompt in the terminal changed from a compute node to a login node. `netid@hal0011` to `netid@amarel` Start a new interactive session requesting more cpu-cores (-c 7) br srun -p main --x11 --reservation=genomics -N 1 -c 7 -n 1 -t 01:40:00 --pty /bin/bash -i For your convenience we put this command into a bash script, thus you may just type tophat2_node_request.sh Notice that the prompt changed again to a compute node, e.g. `netid@hal0011` Go to your folder with data. br Remember that you should match the `tophat2 -p` option to be consistent with the number of cores `srun -c` that you requested, thus we use `tophat2 -p 7`, because we requested `srun -c 7`. br **NOTE:** tophat2 command starting with `tophat2 -p 7` is a one line command, move the slider to the right to see the whole line. cd /scratch/$USER/Genomics_Workshop/untreated module load mvapich2/2.1 boost/1.59.0 tophat2/2.1.0 module load samtools bowtie2 mkdir tophat_out tophat2 -p 7 --library-type fr-unstranded -o tophat_out/untreated_SRR1039508_10k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq For your convenience we put these commands into a bash script, thus you may just type run_tophat2.sh You should see a similar output: [2018-03-30 11:48:57] Beginning TopHat run (v2.1.0) [2018-03-30 11:48:57] Checking for Bowtie Bowtie version: 2.2.9.0 [2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome).. [2018-03-30 11:48:58] Checking for Bowtie index files (genome).. [2018-03-30 11:48:58] Checking for reference FASTA file [2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen ce/hg20/Homo_sapiens.GRCh38.dna.toplevel [2018-03-30 11:49:09] Reading known junctions from GTF file [2018-03-30 11:49:27] Preparing reads left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded) right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded) [2018-03-30 11:49:29] Using pre-built transcriptome data.. [2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2 [2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2 [2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads [2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t oplevel with Bowtie2 [2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2) [2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2) [2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 [2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2) [2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Here is another example of mapping with tophat. br The transcriptome index was built by pointing to a gtf file first (the gtf file is prepared earlier). The following would be the command to generate the transcriptome index while running tophat alignment. br **NOTE:** tophat2 command starting with `tophat2 -p 7` is a one line command, move the slider to the right to see the whole line. tophat2 -p 7 --library-type fr-unstranded -o tophat_out/untreated_SRR1039508 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039508_1.fastq.pairedOut.fastq SRR1039508_2.fastq.pairedOut.fastq After the completion the output folder `tophat_out/untreated_SRR1039508/` shall contain the following files/folders: -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam -rw-rw-r-- 1 yc759 oarc 565 Jan 16 21:57 align_summary.txt -rw-rw-r-- 1 yc759 oarc 1921529 Jan 16 21:57 deletions.bed -rw-rw-r-- 1 yc759 oarc 2239884 Jan 16 21:57 insertions.bed -rw-rw-r-- 1 yc759 oarc 14181618 Jan 16 21:57 junctions.bed drwxrwsr-x 2 yc759 oarc 4096 Jan 16 21:57 logs -rw-rw-r-- 1 yc759 oarc 184 Jan 16 21:57 prep_reads.info -rw-rw-r-- 1 yc759 oarc 42846571 Jan 16 21:57 unmapped.bam ## 8. Read counts using htseq-count Tophat mapping is resource intense and time consuming. If you managed to produce the bam file yourself, it's great. If not, let's move forward and copy files that we prepared for you. cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 cp /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.bam accepted_hits.bam Next, we will use samtools to sort the bam file by name: because htseq-count accepts bam file sorted by **name** as default, but tophat generates bam sorted by **coordinates** as default. module load samtools intel/17.0.2 python/2.7.12 #loads the software samtools sort -n accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name - /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf untreated08.txt In the same way, generate the counts file `untreated12.txt`, `untreated16.txt`,`dex09.txt`, `dex13.txt`, `dex17.txt`. br To save time and for your convenience we prepared files already, thus you may just type run_htseq_count.sh ## 9. Perform Mapping QC using RSeQC Now, quality control using RSeQC - a few examples are given here, please go to the [rseqc website](http://rseqc.sourceforge.net/) for more functions. br - Example 1 br Execute: cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 module load python/2.7.12 module load intel/17.0.4 read_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed Provided a BAM/SAM file and reference gene model, `read_distribution.py` will calculate how mapped reads were distributed over genome feature (like CDS exon, 5\u2019UTR exon, 3\u2019 UTR exon, Intron, Intergenic regions). br Output: processing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done processing accepted_hits.bam ... Finished Total Reads 43474036 Total Tags 54438789 Total Assigned Tags 53991382 ===================================================================== Group Total_bases Tag_count Tags/Kb CDS_Exons 103371993 43264842 418.54 5'UTR_Exons 5217678 583447 111.82 3'UTR_Exons 29324747 8145122 277.76 Introns 1500197093 1805034 1.20 TSS_up_1kb 33306654 18893 0.57 TSS_up_5kb 148463534 41165 0.28 TSS_up_10kb 265823549 55644 0.21 TES_down_1kb 35215293 50954 1.45 TES_down_5kb 152556214 113325 0.74 TES_down_10kb 268614580 137293 0.51 ===================================================================== - Example 2 br Execute: bam_stat.py -i accepted_hits.bam It will summarize mapping statistics of a BAM or SAM file. br Output: Load BAM file ... Done #================================================== #All numbers are READ count #================================================== Total records: 52528699 QC failed: 0 Optical/PCR duplicate: 0 Non primary hits 9054663 Unmapped reads: 0 mapq mapq_cut (non-unique): 2684801 mapq = mapq_cut (unique): 40789235 Read-1: 20414530 Read-2: 20374705 Reads map to '+': 20393901 Reads map to '-': 20395334 Non-splice reads: 30860931 Splice reads: 9928304 Reads mapped in proper pairs: 32386536 Proper-paired reads map to different chrom:312 - Example 3 br In this example we calculate the RNA-seq reads coverage over gene body using `geneBody_coverage.py`. However, this script requires that bam files to be sorted and indexed. We will do it using `samtools`. Moreover for calculations and generation of graphical outputs R-package is needed. br Load `samtools` and `R-Project` modules and go to the tophat_out sample folder module load intel/17.0.4 R-Project/3.4.1 module load samtools samtools sort accepted_hits.bam -o accepted_hits.sorted.bam This may take some time, thus you may use instead one of already prepared files by making a soft link to it: ln -s /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam Execute: samtools index accepted_hits.sorted.bam geneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test Output: @ 2018-01-14 13:17:33: Read BED file (reference gene model) ... @ 2018-01-14 13:17:33: Total 3802 transcripts loaded @ 2018-01-14 13:17:33: Get BAM file(s) ... accepted_hits.sorted.bam @ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ... 3800 transcripts finished Sample Skewness accepted_hits.sorted -3.61577607436 @ 2018-01-14 13:28:59: Running R script ... null device 1 output files: br test.geneBodyCoverage.r br test.geneBodyCoverage.txt br log.txt br test.geneBodyCoverage.curves.pdf br - Example 4 br Checking whether the sequencing read is strand specific or not br Download the gene.bed files : `https://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/` **NOTE** Be careful, the genome version has to be consistent with the reference genome used in mapping. br Execute: infer_experiment.py -r /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam Output: Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done Loading SAM/BAM file ... Total 200000 usable reads were sampled This is PairEnd Data Fraction of reads failed to determine: 0.1406 Fraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302 Fraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292 ## 10. R practice There is one exeption when the computational job can be started on the cluster login node, and it is when you need to install some software packages. `R program` is alread installed on the cluster as a module, but we need to add some packages from [bioconductor](https://bioconductor.org) as well. br - a). Install needed packages exit from the compute node ``` exit ``` You should see that the prompt in the terminal changed from a compute node to a login node. `netid@hal0011` to `netid@amarel` Now load R module on a login node module load intel/17.0.4 module load R-Project/3.4.1 R In R environmnent install the following packages source(\"https://bioconductor.org/biocLite.R\") biocLite(\"ape\") biocLite(\"MKmisc\") biocLite(\"Heatplus\") biocLite(\"affycoretools\") biocLite(\"flashClust\") biocLite(\"affy\") ##yes (for personal library) Exit R: quit() Save workspace image? [y/n/c]: n - b). Download gene annotation in gtf format from [Ensembl](http://useast.ensembl.org/index.html) br /scratch/$USER/Genomics_Workshop/Reference/ wget ftp://ftp.ensembl.org/pub/release-78/gtf/homo_sapiens/Homo_sapiens.GRCh38.78.gtf.gz tar -zxvf Homo_sapiens.GRCh38.78.gtf.gz *NOTE*: the latest Ensembl Release is 93 (July 2018) but we will work with release 78 for this class To save time you may just copy gtf file: cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf /scratch/$USER/Genomics_Workshop/Reference/ ``` Calculate a gene length ###cp the Homo_sapiens.GRCh38.78.gtf to your own Reference folder cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf /scratch/$USER/Genomics_Workshop/Reference/. module load intel/17.0.4 module load R-Project/3.4.1 ##start R on computer node now srun --x11 -p main --reservation=ddclass -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash library(GenomicFeatures) gtfdb - makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\") exons.list.per.gene - exonsBy(gtfdb,by=\"gene\") exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength') write.csv(Hg20_geneLength, file=\"Hg20_geneLength\") ##export/save this gene length file into your working directory ### the gene length is defined to be the total length of all exons in the gene, including the 3'UTR### c). More self practice ##at R prompt options(stringsAsFactors = FALSE) #this will prevent R from reading in character data as factor data #### Arithmetic functions 2+2 3*3 3*8+2 log10(1000) log2(8) abs(-10) sqrt(81) #### Creating objects ls() #see what objects are in the workspace x - 4 x x = 3 #a single = is an assignment operator x x == 5 #a double == asks \"is the left side equivalent to the right side?\" x + 2 #objects can be used in equations y - \"anyname\" y class(x) class(y) ls() #### Vector and Matrix x1 - c(1,2,3,4,5) x1 class(x1) length(x1) x - cbind(x1, x1+1) #1 will be added to all the numbers in x1 x class(x) #what kind of object is x? dim(x) #the dimension of matrix x1[1:3] #use [] to get subsets of a vector x[1,] #use [,] to get subsets of a matrix (or dataframe) x[,1] x[,-1] x[c(1,2),] x[-c(1,3),] colnames(x) colnames(x) -c(\"A\",\"B\") rownames(x) -c(\"C\",\"D\",\"E\",\"F\",\"G\") x #### Data Frames z - data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F) class(z) names(z) dim(z) class(z$A) class(z$B) class(z$C) class(z$D) z$B z$C #### More ways to subset dataframes z$B z[[2]] z[,2] #these first 3 give equivalent results z[,1:2] z[,c(1,3)] z[c(1,3:5),] #### Lists mylist - list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\")) class(mylist) mylist names(mylist) class(mylist$first) class(mylist$second) #### Functions my.add - function(a, b) {a - b} class(my.add) my.add(4,99) my.add(99,4) my.add(b = 99, a = 4) library(limma) #load the limma package #### Make sure the working directory is set to your file on the computer; getwd() #see what the current working directory is setwd(\"????????????????\") #change the working directory #### Output a single object as a comma separated value file write.csv(z, file=\"test.csv\") #### Save all the objects you have created to your workspace save.image() #creates a default file named \".RData\" save.image(\"intro.Rdata\") #creates a named file #### Remove objects from your workspace ls() rm(x) #remove a single object by name ls() rm(z,x1) #remove multiple objects by name ls() load(\"intro.Rdata\") ls() rm(list=ls()) #remove all objects ls() #### Save a history of all the commands entered savehistory(\"introhistory.Rhistory\")","title":"Overview"},{"location":"workshops/genomics_1/#overview","text":"","title":"Overview"},{"location":"workshops/genomics_1/#genomic-software","text":"This is a list of software to install for the workshop Software Description URL seqtk very handy and fast for processing fastq/a files [link] sratoolkit downloading and processing data from GEO/SRA database [link] [download] htseq-count counting the reads mapped on to genomics feature [link] fastQC widely used for sequencing read QC [link] [download] RSeQC-2.6.4 An RNA-seq quality control package, multiple functions [link] trimmomatic fastq quality trim and adaptor removal [link] [download] This is a list of software already available on the cluster and the command you need to execute to load it in your environment: Software Description Access on the cluster samtools utilities for short DNA seq alignments [link] module load samtools/1.3.1 bedtools tools for a wide-range of genomics analysis [link] module load bedtools2/2.25.0 bowtie2 alignment software [link] module load bowtie2/2.2.9 tophat2 a fast splice junction mapper for RNA-Seq reads [link] module load mvapich2/2.1 boost/1.59.0 tophat2/2.1.0 R language for statistical analysis module load intel/17.0.4 R-Project/3.4.1 This is a list of other software you might find useful: Software Description URL GSEA Genome set enrichment analysis [link] IGV Interactive Genome Viewer [link] Cytoscape Network visualization softwar [link]","title":"Genomic Software"},{"location":"workshops/genomics_1/#setup","text":"","title":"Setup"},{"location":"workshops/genomics_1/#connect-to-the-cluster-login-node","text":"The preferred method to connect to the cluster is through a web browser and fastX client via FastX : in your browser, go to https://amarel.hpc.rutgers.edu:3443 or click here . (It will only work from campus or if you are connected via VPN link .) See FastX for a walkthrough that includes a tip on copying/pasting commands. Login page Click on Launch Session and Select XFCE desktop Open new terminal, copy text from your computer into clipboard window and click on Send to Remote. Put cursor into a terminal and click Edit- Paste. The text from a clipboard appears in the terminal. Hit Return to submit a job to a compute node. Notice that the jobid number has been assigned and resources have been allocated. The command prompt in the treminal will also change from a login node netid@amarel to a compute node netid@slepner or netid@hal. On rare occasions, especially if the user has a modified .bashrc file, FastX doesn't work. If it is the case, you can still use a terminal but the connection will be much slower for graphical outputs. - via a terminal : if you have a Mac or Linux, a terminal is part of your standard apps. If you have Windows, install an SSH client such as mobaXterm . Then from your terminal connect to the cluster by executing the following command: ssh -X your netid @amarel.hpc.rutgers.edu","title":"Connect to the cluster login node"},{"location":"workshops/genomics_1/#do-not-run-any-computational-jobs-on-the-login-node","text":"","title":"DO NOT RUN ANY COMPUTATIONAL JOBS ON THE LOGIN NODE"},{"location":"workshops/genomics_1/#get-resources-on-the-compute-node","text":"When you login to the cluster you are on the login node. Jobs are not allowed to be run on the loging node, intstead you need to request a resource on the compute node for your job. This means that you will not impede other users who are also using the login node, and will be placed on a machine which you share with only a few people. You can do so by running the following command in your terminal: srun -p main --x11 --reservation=genomics -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash -i or just run this script node_request.sh Notice that the name in your terminal will change from amarel to a node name like hal0025 or slepner086 . The following table explains the parts of this command: command part meaning srun slurm run, i.e. allocate resources and run via slurm scheduler -p main on the main partition, one of several queues on the cluster --x11 it allows the graphical output from a compute node, e.g. GUI of the program --reservation=genomics we reserved some compute nodes for this workshop to avoid waiting in the queue -N 1 ask for one compute node -c 2 ask for two cpu cores -n 1 tells slurm that the job will be run as 1 task ( for parallel MPI jobs it could be more than 1 -t 01:40:00 run this for a maximum time of 1 hour 40 minutes --pty /bin/bash -i run the terminal shell in an interactive mode","title":"Get resources on the compute node"},{"location":"workshops/genomics_1/#understanding-your-data-space","text":"You have two main spaces on the Amarel cluster. These are: your home directory (100Gb) - /home/netid/ your scratch directory (500Gb)- /scratch/netid/ They differ in how they are backed up (scratch is not backed up) and by read/write speed. So we will install programs in /home/$USER/Genomics_Workshop , while the data and computational output will be held in /scratch/$USER/Genomics_Workshop .","title":"Understanding your data space"},{"location":"workshops/genomics_1/#1-install-programs-and-create-a-workspace-for-the-workshop","text":"Each program has slightly different installation instructions. You do not need to install programs manually. Instead just run the following scirpt: /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/Labs/lab_PartI.sh It will install neccessary programs and creates folders for this workshop. For curious one, here is the content of the script ######################################################## ``` #!/bin/bash mkdir -p /home/$USER/Genomics_Workshop/ mkdir -p /scratch/$USER/Genomics_Workshop/scripts mkdir -p /scratch/$USER/Genomics_Workshop/download mkdir -p /scratch/$USER/Genomics_Workshop/untreated mkdir -p /scratch/$USER/Genomics_Workshop/dex_treated mkdir -p /scratch/$USER/Genomocs_Workshop/Reference echo \"Copying files... Please wait\" cp -r /projects/oarc/Genomics_Workshop/Programs/ /home/$USER/Genomics_Workshop/ echo '## Genomics_Workshop specific settings 07/16/2018' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/seqtk:$PATH' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/sratoolkit.2.8.2-centos_linux64/bin:$PATH' ~/.bashrc echo 'export PATH=$HOME/Genomics_Workshop/Programs/FastQC:$PATH' ~/.bashrc source ~/.bashrc module load intel/17.0.2 python/2.7.12 pip install HTSeq --user wait pip install bx-python==0.7.3 --user wait pip install RSeQC --user ######################################################## br ## 2. Download data We will download human RNA-seq data with [GEO accession GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778). The samples are in NCBI's short read archive format (SRA). We will use **sratoolkit** programs to download data but first we need to configure a location where all data files will be stored. br `vdb-config` is a configuration subprogram for `sratoolkit`. We will use it to specify the directory where `sratoolkit` fetches data. You will need to type in the followingt path, but remember to replace `netID` with your own `Rutgers netid` `/scratch/your_netID/Genomics_Workshop/download`. Do not copy blindly! Remember that **sratoolkit** is not designed to handle complex downloads. All data will be placed in one folder. You will need to move downloaded files for further analysis/manipulation into different locations manually. vdb-config --interactive-mode textual ### dash-dash before interactive-mode","title":"1. Install programs and create a workspace for the workshop"},{"location":"workshops/genomics_1/#now-in-the-program","text":"Your choice 4","title":"Now in the program:"},{"location":"workshops/genomics_1/#type-new-path-in","text":"/scratch/your_netID/Genomics_Workshop/download Your choice Y","title":"type new path in"},{"location":"workshops/genomics_1/#hit-enter-and-exit-the-program","text":"Then execute the following commands to get the data. Both `prefetch` and `fastq-dump` are part of sratools. br Downloading can be time consuming, it takes about 3 minutes per file. Thus we already downloaded files and placed them in your /scratch/$USER/Genomics_Workshop/download/sra folder. Skim through this section to understand how we did it and continue with 'fast-dump' below. br","title":"Hit Enter and exit the program"},{"location":"workshops/genomics_1/#fetch-the-sra-data","text":"","title":"fetch the SRA data"},{"location":"workshops/genomics_1/#untreated","text":"prefetch -v SRR1039508 prefetch -v SRR1039512 prefetch -v SRR1039516","title":"Untreated"},{"location":"workshops/genomics_1/#dex-treated","text":"prefetch -v SRR1039509 prefetch -v SRR1039513 prefetch -v SRR1039517 Next step is to unpack sra files and convert them to more suitable *fastq* format with `fastq-dump` program br To unpack the original sequence files can be a bit tricky at first. We need to put them into different directories: br *508,512,516* into /scratch/$USER/Genomics_Workshop/download/untreated and br *509,513,517* into /scratch/$USER/Genomics_Workshop/download/dex_treated br Luckily `fastq-dump` can do processing and output results into specified folders at the same time. cd /scratch/$USER/Genomics_Workshop/download/sra fastq-dump --outdir /scratch/$USER/Genomics_Workshop/untreated --split-files SRR10395{08,12,16}.sra fastq-dump --outdir /scratch/$USER/Genomics_Workshop/dex_treated --split-files SRR10395{09,13,17}.sra It takes a while to convert sra files. To save time, files are already converted for you. Run the following command to copy files into your /scratch/ directories. sra-fastq.sh # Running bioinformatics jobs ## 3. FastQC - raw data QC FastQC performs a quality control checks on raw sequence data and produces various graphical outputs for visual analysis. cd /scratch/$USER/Genomics_Workshop/untreated module load java ## fastqc is written in java; we need to load java before using fastqc mkdir fastqc ## create a folder to store the QC output fastqc -o fastqc SRR1039508_1.fastq SRR1039508_2.fastq FastQC produces html pages in `fastqc/SRR1039508_1(2)_fastqc.html`, with different kinds of views of data (and Phred scores). You can open this file in Firefox browser. firefox fastqc/SRR1039508_1_fastqc.html To learn more about FastQC, see this pdf file br /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/misc/FastQC_details.pdf br Close Firefox when you are done. ## 4. Trimmomatic - quality trim/adaptor removal For demonstration purpose, we will take a small subset data using `seqtk` program cd /scratch/$USER/Genomics_Workshop/untreated seqtk sample -s100 SRR1039508_1.fastq 10000 SRR1039508_1_10k.fastq seqtk sample -s100 SRR1039508_2.fastq 10000 SRR1039508_2_10k.fastq More details and examples how to use `seqtk` can be found in br /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/misc/Seqtk_Examples Now, run `trimmomatic` to trim the read quality, and remove adaptor br **NOTE:** trimmomatic command starting with `java -jar` is a one line command, move the slider to the right to see the whole line. module load java ### needed for trimmomatic java -jar /home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/$USER/Genomics_Workshop/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 For your convenience we put this command into a bash script, thus you may just type run_trimmo.sh Once it started run, you should see the following: TrimmomaticPE: Started with arguments: -phred33 -trimlog trim.log SRR1039508_1_10k.fastq SRR1039508_2_10k.fastq SRR1039508_1.paired.fastq SRR1039508_1.unpaired.fastq SRR1039508_2.paired.fastq SRR1039508_2.unpaired.fastq ILLUMINACLIP:/home/yc759/Programs/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 Multiple cores found: Using 2 threads Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT' ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences Input Read Pairs: 100000 Both Surviving: 96596 (96.60%) Forward Only Surviving: 1542 (1.54%) Reverse Only Surviving: 1467 (1.47%) Dropped: 395 (0.40%) TrimmomaticPE: Completed surccessfully View the output, the trim.log file, .e.g. length=63 55 1 56 7 (the original read length 63, now 55 after trim, 1 base from left end and 7 bases from the right end were trimmed off, 56 bases in middle remained) Alternatively, you may also use `fastx_quality_stats` from *the FASTX\u2014toolkit* ( not covered in this workshop). ## 5. FastQC - Run on cleaned reads You may run FastQC again on the cleaned by trimmomatic reads and compare new results with results for raw data, step 3 above. module load java fastqc -o fastqc SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq ## 6. Download reference and reference indexing Indexing of Human genome may take hours. Thus we have the reference genome prepared and ready for use in br `/projects/oarc/Genomics_Workshop/Reference/ ` br For in class practice we demonstrate how to do genome indexing with E.coli genome cd /scratch/$USER/Genomics_Workshop/Reference wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Escherichia_coli/latest_assembly_versions/GCA_000005845.2_ASM584v2/GCA_000005845.2_ASM584v2_genomic.fna.gz gunzip GCA_000005845.2_ASM584v2_genomic.fna.gz module load bowtie2 bowtie2-build GCA_000005845.2_ASM584v2_genomic.fna GCA_000005845.2_ASM584v2_genomic For your convenience we put these commands into a bash script, thus you may just type run_bowtie2.sh ## 7. Mapping with tophat2, (STAR, HISAT2) Before we start mapping job, make sure that you WON'T start a job on a login node. tophat2 jobs are computationally intensive and may require hours to be completed. br - **NOTE: We do not expect you to complete mapping in the class. You may start it to get familiar with the procedure, but do not wait till the completion. You may however do it as a homework for a preparation for the next session ( Genomics Workshop part II).** br Also tophat2 is a multithreaded program, which means it can utilize more than one cpu-core, thus it is better to request more resources with srun command ( so far we used `srun -c 2` (two cpu-cores). For tophat2 jobs it is advisable to request more cpu-cores. For the convenience of this workshop we will use `-c 7`, however you may request more cpu-cores for your research. br Exit the current interactive session: type ```exit``` in the terminal wondow. You should see that the prompt in the terminal changed from a compute node to a login node. `netid@hal0011` to `netid@amarel` Start a new interactive session requesting more cpu-cores (-c 7) br srun -p main --x11 --reservation=genomics -N 1 -c 7 -n 1 -t 01:40:00 --pty /bin/bash -i For your convenience we put this command into a bash script, thus you may just type tophat2_node_request.sh Notice that the prompt changed again to a compute node, e.g. `netid@hal0011` Go to your folder with data. br Remember that you should match the `tophat2 -p` option to be consistent with the number of cores `srun -c` that you requested, thus we use `tophat2 -p 7`, because we requested `srun -c 7`. br **NOTE:** tophat2 command starting with `tophat2 -p 7` is a one line command, move the slider to the right to see the whole line. cd /scratch/$USER/Genomics_Workshop/untreated module load mvapich2/2.1 boost/1.59.0 tophat2/2.1.0 module load samtools bowtie2 mkdir tophat_out tophat2 -p 7 --library-type fr-unstranded -o tophat_out/untreated_SRR1039508_10k --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039508_1.paired.fastq SRR1039508_2.paired.fastq For your convenience we put these commands into a bash script, thus you may just type run_tophat2.sh You should see a similar output:","title":"DEX treated"},{"location":"workshops/genomics_1/#2018-03-30-114857-beginning-tophat-run-v210","text":"[2018-03-30 11:48:57] Checking for Bowtie Bowtie version: 2.2.9.0 [2018-03-30 11:48:58] Checking for Bowtie index files (transcriptome).. [2018-03-30 11:48:58] Checking for Bowtie index files (genome).. [2018-03-30 11:48:58] Checking for reference FASTA file [2018-03-30 11:48:58] Generating SAM header for /projects/oarc/Genomics_Workshop/Referen ce/hg20/Homo_sapiens.GRCh38.dna.toplevel [2018-03-30 11:49:09] Reading known junctions from GTF file [2018-03-30 11:49:27] Preparing reads left reads: min. length=35, max. length=63, 96592 kept reads (4 discarded) right reads: min. length=35, max. length=63, 96594 kept reads (2 discarded) [2018-03-30 11:49:29] Using pre-built transcriptome data.. [2018-03-30 11:49:35] Mapping left_kept_reads to transcriptome GRCh38.78 with Bowtie2 [2018-03-30 11:49:49] Mapping right_kept_reads to transcriptome GRCh38.78 with Bowtie2 [2018-03-30 11:50:03] Resuming TopHat pipeline with unmapped reads [2018-03-30 11:50:03] Mapping left_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.t oplevel with Bowtie2 [2018-03-30 11:50:16] Mapping left_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2) [2018-03-30 11:50:18] Mapping left_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2) [2018-03-30 11:50:20] Mapping right_kept_reads.m2g_um to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 [2018-03-30 11:50:23] Mapping right_kept_reads.m2g_um_seg1 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (1/2) [2018-03-30 11:50:25] Mapping right_kept_reads.m2g_um_seg2 to genome Homo_sapiens.GRCh38.dna.toplevel with Bowtie2 (2/2) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. Here is another example of mapping with tophat. br The transcriptome index was built by pointing to a gtf file first (the gtf file is prepared earlier). The following would be the command to generate the transcriptome index while running tophat alignment. br **NOTE:** tophat2 command starting with `tophat2 -p 7` is a one line command, move the slider to the right to see the whole line. tophat2 -p 7 --library-type fr-unstranded -o tophat_out/untreated_SRR1039508 \u2013GTF /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf --transcriptome-index /projects/oarc/Genomics_Workshop/Reference/hg20_transciptome/GRCh38.78 /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.dna.toplevel SRR1039508_1.fastq.pairedOut.fastq SRR1039508_2.fastq.pairedOut.fastq After the completion the output folder `tophat_out/untreated_SRR1039508/` shall contain the following files/folders: -rw-rw-r-- 1 yc759 oarc 2174796848 Jan 16 21:57 accepted_hits.bam -rw-rw-r-- 1 yc759 oarc 565 Jan 16 21:57 align_summary.txt -rw-rw-r-- 1 yc759 oarc 1921529 Jan 16 21:57 deletions.bed -rw-rw-r-- 1 yc759 oarc 2239884 Jan 16 21:57 insertions.bed -rw-rw-r-- 1 yc759 oarc 14181618 Jan 16 21:57 junctions.bed drwxrwsr-x 2 yc759 oarc 4096 Jan 16 21:57 logs -rw-rw-r-- 1 yc759 oarc 184 Jan 16 21:57 prep_reads.info -rw-rw-r-- 1 yc759 oarc 42846571 Jan 16 21:57 unmapped.bam ## 8. Read counts using htseq-count Tophat mapping is resource intense and time consuming. If you managed to produce the bam file yourself, it's great. If not, let's move forward and copy files that we prepared for you. cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 cp /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.bam accepted_hits.bam Next, we will use samtools to sort the bam file by name: because htseq-count accepts bam file sorted by **name** as default, but tophat generates bam sorted by **coordinates** as default. module load samtools intel/17.0.2 python/2.7.12 #loads the software samtools sort -n accepted_hits.bam | samtools view | htseq-count -m intersection-nonempty -t exon -i gene_id -s no --additional-attr=gene_name - /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf untreated08.txt In the same way, generate the counts file `untreated12.txt`, `untreated16.txt`,`dex09.txt`, `dex13.txt`, `dex17.txt`. br To save time and for your convenience we prepared files already, thus you may just type run_htseq_count.sh ## 9. Perform Mapping QC using RSeQC Now, quality control using RSeQC - a few examples are given here, please go to the [rseqc website](http://rseqc.sourceforge.net/) for more functions. br - Example 1 br Execute: cd /scratch/$USER/Genomics_Workshop/untreated/tophat_out/untreated_SRR1039508 module load python/2.7.12 module load intel/17.0.4 read_distribution.py -i accepted_hits.bam -r /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed Provided a BAM/SAM file and reference gene model, `read_distribution.py` will calculate how mapped reads were distributed over genome feature (like CDS exon, 5\u2019UTR exon, 3\u2019 UTR exon, Intron, Intergenic regions). br Output: processing/projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done processing accepted_hits.bam ... Finished Total Reads 43474036 Total Tags 54438789 Total Assigned Tags 53991382 ===================================================================== Group Total_bases Tag_count Tags/Kb CDS_Exons 103371993 43264842 418.54 5'UTR_Exons 5217678 583447 111.82 3'UTR_Exons 29324747 8145122 277.76 Introns 1500197093 1805034 1.20 TSS_up_1kb 33306654 18893 0.57 TSS_up_5kb 148463534 41165 0.28 TSS_up_10kb 265823549 55644 0.21 TES_down_1kb 35215293 50954 1.45 TES_down_5kb 152556214 113325 0.74 TES_down_10kb 268614580 137293 0.51 ===================================================================== - Example 2 br Execute: bam_stat.py -i accepted_hits.bam It will summarize mapping statistics of a BAM or SAM file. br Output: Load BAM file ... Done #================================================== #All numbers are READ count #================================================== Total records: 52528699 QC failed: 0 Optical/PCR duplicate: 0 Non primary hits 9054663 Unmapped reads: 0 mapq mapq_cut (non-unique): 2684801 mapq = mapq_cut (unique): 40789235 Read-1: 20414530 Read-2: 20374705 Reads map to '+': 20393901 Reads map to '-': 20395334 Non-splice reads: 30860931 Splice reads: 9928304 Reads mapped in proper pairs: 32386536 Proper-paired reads map to different chrom:312 - Example 3 br In this example we calculate the RNA-seq reads coverage over gene body using `geneBody_coverage.py`. However, this script requires that bam files to be sorted and indexed. We will do it using `samtools`. Moreover for calculations and generation of graphical outputs R-package is needed. br Load `samtools` and `R-Project` modules and go to the tophat_out sample folder module load intel/17.0.4 R-Project/3.4.1 module load samtools samtools sort accepted_hits.bam -o accepted_hits.sorted.bam This may take some time, thus you may use instead one of already prepared files by making a soft link to it: ln -s /projects/oarc/Genomics_Workshop/RNA-Seq_analysis/SRA_data/untreated/tophat_out/untreated_SRR1039508/accepted_hits.sorted.bam accepted_hits.sorted.bam Execute: samtools index accepted_hits.sorted.bam geneBody_coverage.py -r /projects/oarc/Genomics_Workshop/Reference/hg38.housekeepingGenes.bed -i accepted_hits.sorted.bam -o test Output: @ 2018-01-14 13:17:33: Read BED file (reference gene model) ... @ 2018-01-14 13:17:33: Total 3802 transcripts loaded @ 2018-01-14 13:17:33: Get BAM file(s) ... accepted_hits.sorted.bam @ 2018-01-14 13:17:33: Processing accepted_hits.sorted.bam ... 3800 transcripts finished Sample Skewness accepted_hits.sorted -3.61577607436 @ 2018-01-14 13:28:59: Running R script ... null device 1 output files: br test.geneBodyCoverage.r br test.geneBodyCoverage.txt br log.txt br test.geneBodyCoverage.curves.pdf br - Example 4 br Checking whether the sequencing read is strand specific or not br Download the gene.bed files : `https://sourceforge.net/projects/rseqc/files/BED/Human_Homo_sapiens/` **NOTE** Be careful, the genome version has to be consistent with the reference genome used in mapping. br Execute: infer_experiment.py -r /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed -i accepted_hits.bam Output: Reading reference gene model /projects/oarc/Genomics_Workshop/Reference/Homo_sapiens.GRCh38.79.bed ... Done Loading SAM/BAM file ... Total 200000 usable reads were sampled This is PairEnd Data Fraction of reads failed to determine: 0.1406 Fraction of reads explained by \"1++,1--,2+-,2-+\": 0.4302 Fraction of reads explained by \"1+-,1-+,2++,2--\": 0.4292 ## 10. R practice There is one exeption when the computational job can be started on the cluster login node, and it is when you need to install some software packages. `R program` is alread installed on the cluster as a module, but we need to add some packages from [bioconductor](https://bioconductor.org) as well. br - a). Install needed packages exit from the compute node ``` exit ``` You should see that the prompt in the terminal changed from a compute node to a login node. `netid@hal0011` to `netid@amarel` Now load R module on a login node module load intel/17.0.4 module load R-Project/3.4.1 R In R environmnent install the following packages source(\"https://bioconductor.org/biocLite.R\") biocLite(\"ape\") biocLite(\"MKmisc\") biocLite(\"Heatplus\") biocLite(\"affycoretools\") biocLite(\"flashClust\") biocLite(\"affy\") ##yes (for personal library) Exit R: quit() Save workspace image? [y/n/c]: n - b). Download gene annotation in gtf format from [Ensembl](http://useast.ensembl.org/index.html) br /scratch/$USER/Genomics_Workshop/Reference/ wget ftp://ftp.ensembl.org/pub/release-78/gtf/homo_sapiens/Homo_sapiens.GRCh38.78.gtf.gz tar -zxvf Homo_sapiens.GRCh38.78.gtf.gz *NOTE*: the latest Ensembl Release is 93 (July 2018) but we will work with release 78 for this class To save time you may just copy gtf file: cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf /scratch/$USER/Genomics_Workshop/Reference/ ``` Calculate a gene length ###cp the Homo_sapiens.GRCh38.78.gtf to your own Reference folder cp /projects/oarc/Genomics_Workshop/Reference/hg20/Homo_sapiens.GRCh38.78.gtf /scratch/$USER/Genomics_Workshop/Reference/. module load intel/17.0.4 module load R-Project/3.4.1 ##start R on computer node now srun --x11 -p main --reservation=ddclass -N 1 -c 2 -n 1 -t 01:40:00 --pty /bin/bash library(GenomicFeatures) gtfdb - makeTxDbFromGFF(\"Homo_sapiens.GRCh38.78.gtf\",format=\"gtf\") exons.list.per.gene - exonsBy(gtfdb,by=\"gene\") exonic.gene.sizes - lapply(exons.list.per.gene,function(x){sum(width(reduce(x)))}) class(exonic.gene.sizes) Hg20_geneLength -do.call(rbind, exonic.gene.sizes) colnames(Hg20_geneLength) - paste('geneLength') write.csv(Hg20_geneLength, file=\"Hg20_geneLength\") ##export/save this gene length file into your working directory ### the gene length is defined to be the total length of all exons in the gene, including the 3'UTR### c). More self practice ##at R prompt options(stringsAsFactors = FALSE) #this will prevent R from reading in character data as factor data #### Arithmetic functions 2+2 3*3 3*8+2 log10(1000) log2(8) abs(-10) sqrt(81) #### Creating objects ls() #see what objects are in the workspace x - 4 x x = 3 #a single = is an assignment operator x x == 5 #a double == asks \"is the left side equivalent to the right side?\" x + 2 #objects can be used in equations y - \"anyname\" y class(x) class(y) ls() #### Vector and Matrix x1 - c(1,2,3,4,5) x1 class(x1) length(x1) x - cbind(x1, x1+1) #1 will be added to all the numbers in x1 x class(x) #what kind of object is x? dim(x) #the dimension of matrix x1[1:3] #use [] to get subsets of a vector x[1,] #use [,] to get subsets of a matrix (or dataframe) x[,1] x[,-1] x[c(1,2),] x[-c(1,3),] colnames(x) colnames(x) -c(\"A\",\"B\") rownames(x) -c(\"C\",\"D\",\"E\",\"F\",\"G\") x #### Data Frames z - data.frame(A=x[,1], B=rownames(x), C=factor(rownames(x)), D=x[,1]==3, stringsAsFactors=F) class(z) names(z) dim(z) class(z$A) class(z$B) class(z$C) class(z$D) z$B z$C #### More ways to subset dataframes z$B z[[2]] z[,2] #these first 3 give equivalent results z[,1:2] z[,c(1,3)] z[c(1,3:5),] #### Lists mylist - list(first=z,second=x,third=c(\"W\",\"X\",\"Y\",\"Z\")) class(mylist) mylist names(mylist) class(mylist$first) class(mylist$second) #### Functions my.add - function(a, b) {a - b} class(my.add) my.add(4,99) my.add(99,4) my.add(b = 99, a = 4) library(limma) #load the limma package #### Make sure the working directory is set to your file on the computer; getwd() #see what the current working directory is setwd(\"????????????????\") #change the working directory #### Output a single object as a comma separated value file write.csv(z, file=\"test.csv\") #### Save all the objects you have created to your workspace save.image() #creates a default file named \".RData\" save.image(\"intro.Rdata\") #creates a named file #### Remove objects from your workspace ls() rm(x) #remove a single object by name ls() rm(z,x1) #remove multiple objects by name ls() load(\"intro.Rdata\") ls() rm(list=ls()) #remove all objects ls() #### Save a history of all the commands entered savehistory(\"introhistory.Rhistory\")","title":"[2018-03-30 11:48:57] Beginning TopHat run (v2.1.0)"}]}